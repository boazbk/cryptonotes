<!DOCTYPE html>
<html  lang="en">

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Introduction to Theoretical Computer Science: Efficient computation</title>
  <meta name="description" content="Textbook on Theoretical Computer Science by Boaz Barak">

  <meta property="og:title" content="Introduction to Theoretical Computer Science: Efficient computation" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://introtcs.org/" />
  <meta property="og:image" content="icons/cover.png" />
  <meta property="og:description" content="Textbook on Theoretical Computer Science by Boaz Barak" />
  <meta name="github-repo" content="boazbk/tcs" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Introduction to Theoretical Computer Science" />
  <meta name="twitter:description" content="Textbook on Theoretical Computer Science by Boaz Barak" />
  <meta name="twitter:image" content="https://introtcs.org/icons/cover.png" />

<meta name="author" content="Boaz Barak">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <link rel="apple-touch-icon-precomposed" sizes="120x120" href="icons/apple-touch-icon-120x120.png">
  <link rel="shortcut icon" href="icons/favicon.ico" type="image/x-icon">

<!-- Boaz: resources -->

<!-- <script src="https://kit.fontawesome.com/ab08ce82a8.js"></script> -->

<link rel="stylesheet" src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.9.0/css/all.min.css">


<!-- KaTeX -->


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css"
  integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">

<!-- The loading of KaTeX is deferred to speed up page rendering -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js"
  integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>

<!-- To automatically render math in text elements, include the auto-render extension: -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js"
  integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
  onload='renderMathInElement(document.body, {  throwOnError: false, macros: { "\\N": "\\mathbb{N}", "\\R": "\\mathbb{R}", "\\Z": "\\mathbb{Z}","\\E": "\\mathbb{E}","\\val": "\\mathrm{val}", "\\label": "\\;\\;\\;\\;\\;\\;\\;\\;","\\floor": "\\lfloor #1 \\rfloor","\\ceil": "\\lceil #1 \\rceil", "\\ensuremath": "#1"}});'>
</script>




<!-- KaTeX -->
<!-- pseudocode -->
<link rel="stylesheet" href="css/pseudocode.css">
<!-- <script src="js/pseudocode.min.js"></script> -->


<!-- Gitbook resources -->

  <script src="js/jquery.min.js"></script>
  <link href="css/style.css" rel="stylesheet" />
  
  <link href="css/plugin-table.css" rel="stylesheet" />
  <link href="css/plugin-bookdown.css" rel="stylesheet" />
  <link href="css/plugin-highlight.css" rel="stylesheet" />
  <link href="css/plugin-search.css" rel="stylesheet" />
  <link href="css/plugin-fontsettings.css" rel="stylesheet" />
  <link href="css/moregitbook.css" rel="stylesheet" />

  <link href="css/resmisc.css" rel="stylesheet" />





<!-- Boaz: end resources -->



<!--bookdown:link_prev-->
<!--bookdown:link_next-->




<!-- bigfoot-->

<link href="css/bigfoot-default.css" rel="stylesheet" />
<script type="text/javascript" src="js/bigfoot.js"></script>

<script type="text/javascript">
    var bigfoot = jQuery.bigfoot(
        {
            deleteOnUnhover: false,
            preventPageScroll: false,
            hoverDelay: 250
        }
    );
</script>

<!-- end bigfoot -->


</head>

<body>



<!--bookdown:title:start-->
<!--bookdown:title:end-->


<!--bookdown:toc:start-->
  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">
<!--bookdown:toc2:start-->
<ul class="summary">
<li><a href="./">Introduction to Theoretical Computer Science</a></li>
<li class="divider"></li><li class="chapter" data-level="p" data-path="lec_00_0_preface.html"><a href="lec_00_0_preface.html"><i class="fa fa-check"></i><b>p</b> Preface</a><ul><li class="chapter" data-level="p.1" data-path="lec_00_0_preface.html"><a href="lec_00_0_preface.html#to-the-student"><i class="fa fa-check"></i><b>p.1</b> To the student</a><ul><li class="chapter" data-level="p.1.1" data-path="lec_00_0_preface.html"><a href="lec_00_0_preface.html#is-the-effort-worth-it"><i class="fa fa-check"></i><b>p.1.1</b> Is the effort worth it?</a></li></ul></li><li class="chapter" data-level="p.2" data-path="lec_00_0_preface.html"><a href="lec_00_0_preface.html#to-potential-instructors"><i class="fa fa-check"></i><b>p.2</b> To potential instructors</a></li><li class="chapter" data-level="p.3" data-path="lec_00_0_preface.html"><a href="lec_00_0_preface.html#acknowledgements"><i class="fa fa-check"></i><b>p.3</b> Acknowledgements</a></li></ul></li><li class="chapter" data-level="0" data-path="lec_01_introduction.html"><a href="lec_01_introduction.html"><i class="fa fa-check"></i><b>0</b> Introduction</a><ul><li class="chapter" data-level="0.1" data-path="lec_01_introduction.html"><a href="lec_01_introduction.html#integer-multiplication-an-example-of-an-algorithm"><i class="fa fa-check"></i><b>0.1</b> Integer multiplication: an example of an algorithm</a></li><li class="chapter" data-level="0.2" data-path="lec_01_introduction.html"><a href="lec_01_introduction.html#karatsubasec"><i class="fa fa-check"></i><b>0.2</b> Extended Example: A faster way to multiply (optional)</a></li><li class="chapter" data-level="0.3" data-path="lec_01_introduction.html"><a href="lec_01_introduction.html#algsbeyondarithmetic"><i class="fa fa-check"></i><b>0.3</b> Algorithms beyond arithmetic</a></li><li class="chapter" data-level="0.4" data-path="lec_01_introduction.html"><a href="lec_01_introduction.html#on-the-importance-of-negative-results."><i class="fa fa-check"></i><b>0.4</b> On the importance of negative results.</a></li><li class="chapter" data-level="0.5" data-path="lec_01_introduction.html"><a href="lec_01_introduction.html#roadmapsec"><i class="fa fa-check"></i><b>0.5</b> Roadmap to the rest of this book</a><ul><li class="chapter" data-level="0.5.1" data-path="lec_01_introduction.html"><a href="lec_01_introduction.html#dependencies-between-chapters"><i class="fa fa-check"></i><b>0.5.1</b> Dependencies between chapters</a></li></ul></li><li class="chapter" data-level="0.6" data-path="lec_01_introduction.html"><a href="lec_01_introduction.html#exercises"><i class="fa fa-check"></i><b>0.6</b> Exercises</a></li><li class="chapter" data-level="0.7" data-path="lec_01_introduction.html"><a href="lec_01_introduction.html#bnotesintrosec"><i class="fa fa-check"></i><b>0.7</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="1" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html"><i class="fa fa-check"></i><b>1</b> Mathematical Background</a><ul><li class="chapter" data-level="1.1" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#manualbackground"><i class="fa fa-check"></i><b>1.1</b> This chapter: a reader’s manual</a></li><li class="chapter" data-level="1.2" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#secmathoverview"><i class="fa fa-check"></i><b>1.2</b> A quick overview of mathematical prerequisites</a></li><li class="chapter" data-level="1.3" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#reading-mathematical-texts"><i class="fa fa-check"></i><b>1.3</b> Reading mathematical texts</a><ul><li class="chapter" data-level="1.3.1" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#definitions"><i class="fa fa-check"></i><b>1.3.1</b> Definitions</a></li><li class="chapter" data-level="1.3.2" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#assertions-theorems-lemmas-claims"><i class="fa fa-check"></i><b>1.3.2</b> Assertions: Theorems, lemmas, claims</a></li><li class="chapter" data-level="1.3.3" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#proofs"><i class="fa fa-check"></i><b>1.3.3</b> Proofs</a></li></ul></li><li class="chapter" data-level="1.4" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#basic-discrete-math-objects"><i class="fa fa-check"></i><b>1.4</b> Basic discrete math objects</a><ul><li class="chapter" data-level="1.4.1" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#sets"><i class="fa fa-check"></i><b>1.4.1</b> Sets</a></li><li class="chapter" data-level="1.4.2" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#specialsets"><i class="fa fa-check"></i><b>1.4.2</b> Special sets</a></li><li class="chapter" data-level="1.4.3" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#functionsec"><i class="fa fa-check"></i><b>1.4.3</b> Functions</a></li><li class="chapter" data-level="1.4.4" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#graphsec"><i class="fa fa-check"></i><b>1.4.4</b> Graphs</a></li><li class="chapter" data-level="1.4.5" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#secquantifiers"><i class="fa fa-check"></i><b>1.4.5</b> Logic operators and quantifiers</a></li><li class="chapter" data-level="1.4.6" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#secquantifierssums"><i class="fa fa-check"></i><b>1.4.6</b> Quantifiers for summations and products</a></li><li class="chapter" data-level="1.4.7" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#boundvarsec"><i class="fa fa-check"></i><b>1.4.7</b> Parsing formulas: bound and free variables</a></li><li class="chapter" data-level="1.4.8" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#secbigohnotation"><i class="fa fa-check"></i><b>1.4.8</b> Asymptotics and Big-O notation</a></li><li class="chapter" data-level="1.4.9" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#some-rules-of-thumb-for-big-o-notation"><i class="fa fa-check"></i><b>1.4.9</b> Some rules of thumb for Big-O notation</a></li></ul></li><li class="chapter" data-level="1.5" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#proofsbackgroundsec"><i class="fa fa-check"></i><b>1.5</b> Proofs</a><ul><li class="chapter" data-level="1.5.1" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#proofs-and-programs"><i class="fa fa-check"></i><b>1.5.1</b> Proofs and programs</a></li><li class="chapter" data-level="1.5.2" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#proof-writing-style"><i class="fa fa-check"></i><b>1.5.2</b> Proof writing style</a></li><li class="chapter" data-level="1.5.3" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#patterns-in-proofs"><i class="fa fa-check"></i><b>1.5.3</b> Patterns in proofs</a></li></ul></li><li class="chapter" data-level="1.6" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#topsortsec"><i class="fa fa-check"></i><b>1.6</b> Extended example: Topological Sorting</a><ul><li class="chapter" data-level="1.6.1" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#inductionsec"><i class="fa fa-check"></i><b>1.6.1</b> Mathematical induction</a></li><li class="chapter" data-level="1.6.2" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#proving-the-result-by-induction"><i class="fa fa-check"></i><b>1.6.2</b> Proving the result by induction</a></li><li class="chapter" data-level="1.6.3" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#minimality-and-uniqueness"><i class="fa fa-check"></i><b>1.6.3</b> Minimality and uniqueness</a></li></ul></li><li class="chapter" data-level="1.7" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#notationsec"><i class="fa fa-check"></i><b>1.7</b> This book: notation and conventions</a><ul><li class="chapter" data-level="1.7.1" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#conventionsec"><i class="fa fa-check"></i><b>1.7.1</b> Variable name conventions</a></li><li class="chapter" data-level="1.7.2" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#some-idioms"><i class="fa fa-check"></i><b>1.7.2</b> Some idioms</a></li></ul></li><li class="chapter" data-level="1.8" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#exercises"><i class="fa fa-check"></i><b>1.8</b> Exercises</a></li><li class="chapter" data-level="1.9" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#notesmathchap"><i class="fa fa-check"></i><b>1.9</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="2" data-path="lec_02_representation.html"><a href="lec_02_representation.html"><i class="fa fa-check"></i><b>2</b> Computation and Representation</a><ul><li class="chapter" data-level="2.1" data-path="lec_02_representation.html"><a href="lec_02_representation.html#defining-representations"><i class="fa fa-check"></i><b>2.1</b> Defining representations</a><ul><li class="chapter" data-level="2.1.1" data-path="lec_02_representation.html"><a href="lec_02_representation.html#representing-natural-numbers"><i class="fa fa-check"></i><b>2.1.1</b> Representing natural numbers</a></li><li class="chapter" data-level="2.1.2" data-path="lec_02_representation.html"><a href="lec_02_representation.html#meaning-of-representations-discussion"><i class="fa fa-check"></i><b>2.1.2</b> Meaning of representations (discussion)</a></li></ul></li><li class="chapter" data-level="2.2" data-path="lec_02_representation.html"><a href="lec_02_representation.html#representations-beyond-natural-numbers"><i class="fa fa-check"></i><b>2.2</b> Representations beyond natural numbers</a><ul><li class="chapter" data-level="2.2.1" data-path="lec_02_representation.html"><a href="lec_02_representation.html#repnegativeintegerssec"><i class="fa fa-check"></i><b>2.2.1</b> Representing (potentially negative) integers</a></li><li class="chapter" data-level="2.2.2" data-path="lec_02_representation.html"><a href="lec_02_representation.html#twoscomplement"><i class="fa fa-check"></i><b>2.2.2</b> Two’s complement representation (optional)</a></li><li class="chapter" data-level="2.2.3" data-path="lec_02_representation.html"><a href="lec_02_representation.html#rational-numbers-and-representing-pairs-of-strings"><i class="fa fa-check"></i><b>2.2.3</b> Rational numbers, and representing pairs of strings</a></li></ul></li><li class="chapter" data-level="2.3" data-path="lec_02_representation.html"><a href="lec_02_representation.html#representing-real-numbers"><i class="fa fa-check"></i><b>2.3</b> Representing real numbers</a><ul><li class="chapter" data-level="2.3.1" data-path="lec_02_representation.html"><a href="lec_02_representation.html#cantorsec"><i class="fa fa-check"></i><b>2.3.1</b> Can we represent reals exactly?</a></li></ul></li><li class="chapter" data-level="2.4" data-path="lec_02_representation.html"><a href="lec_02_representation.html#representing-objects-beyond-numbers"><i class="fa fa-check"></i><b>2.4</b> Representing objects beyond numbers</a><ul><li class="chapter" data-level="2.4.1" data-path="lec_02_representation.html"><a href="lec_02_representation.html#finite-representations"><i class="fa fa-check"></i><b>2.4.1</b> Finite representations</a></li><li class="chapter" data-level="2.4.2" data-path="lec_02_representation.html"><a href="lec_02_representation.html#prefixfreesec"><i class="fa fa-check"></i><b>2.4.2</b> Prefix-free encoding</a></li><li class="chapter" data-level="2.4.3" data-path="lec_02_representation.html"><a href="lec_02_representation.html#making-representations-prefix-free"><i class="fa fa-check"></i><b>2.4.3</b> Making representations prefix-free</a></li><li class="chapter" data-level="2.4.4" data-path="lec_02_representation.html"><a href="lec_02_representation.html#proof-by-python-optional"><i class="fa fa-check"></i><b>2.4.4</b> Proof by Python (optional)</a></li><li class="chapter" data-level="2.4.5" data-path="lec_02_representation.html"><a href="lec_02_representation.html#representing-letters-and-text"><i class="fa fa-check"></i><b>2.4.5</b> Representing letters and text</a></li><li class="chapter" data-level="2.4.6" data-path="lec_02_representation.html"><a href="lec_02_representation.html#representing-vectors-matrices-images"><i class="fa fa-check"></i><b>2.4.6</b> Representing vectors, matrices, images</a></li><li class="chapter" data-level="2.4.7" data-path="lec_02_representation.html"><a href="lec_02_representation.html#representing-graphs"><i class="fa fa-check"></i><b>2.4.7</b> Representing graphs</a></li><li class="chapter" data-level="2.4.8" data-path="lec_02_representation.html"><a href="lec_02_representation.html#representing-lists-and-nested-lists"><i class="fa fa-check"></i><b>2.4.8</b> Representing lists and nested lists</a></li><li class="chapter" data-level="2.4.9" data-path="lec_02_representation.html"><a href="lec_02_representation.html#notation"><i class="fa fa-check"></i><b>2.4.9</b> Notation</a></li></ul></li><li class="chapter" data-level="2.5" data-path="lec_02_representation.html"><a href="lec_02_representation.html#defining-computational-tasks-as-mathematical-functions"><i class="fa fa-check"></i><b>2.5</b> Defining computational tasks as mathematical functions</a><ul><li class="chapter" data-level="2.5.1" data-path="lec_02_representation.html"><a href="lec_02_representation.html#secimplvsspec"><i class="fa fa-check"></i><b>2.5.1</b> Distinguish functions from programs!</a></li></ul></li><li class="chapter" data-level="2.6" data-path="lec_02_representation.html"><a href="lec_02_representation.html#exercises"><i class="fa fa-check"></i><b>2.6</b> Exercises</a></li><li class="chapter" data-level="2.7" data-path="lec_02_representation.html"><a href="lec_02_representation.html#bibnotesrepres"><i class="fa fa-check"></i><b>2.7</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="3" data-path="lec_03_computation.html"><a href="lec_03_computation.html"><i class="fa fa-check"></i><b>3</b> Defining computation</a><ul><li class="chapter" data-level="3.1" data-path="lec_03_computation.html"><a href="lec_03_computation.html#defining-computation"><i class="fa fa-check"></i><b>3.1</b> Defining computation</a></li><li class="chapter" data-level="3.2" data-path="lec_03_computation.html"><a href="lec_03_computation.html#computing-using-and-or-and-not."><i class="fa fa-check"></i><b>3.2</b> Computing using AND, OR, and NOT.</a><ul><li class="chapter" data-level="3.2.1" data-path="lec_03_computation.html"><a href="lec_03_computation.html#some-properties-of-and-and-or"><i class="fa fa-check"></i><b>3.2.1</b> Some properties of AND and OR</a></li><li class="chapter" data-level="3.2.2" data-path="lec_03_computation.html"><a href="lec_03_computation.html#xoraonexample"><i class="fa fa-check"></i><b>3.2.2</b> Extended example: Computing \ensuremath{\mathit{XOR}} from \ensuremath{\mathit{AND}}, \ensuremath{\mathit{OR}}, and \ensuremath{\mathit{NOT}}</a></li><li class="chapter" data-level="3.2.3" data-path="lec_03_computation.html"><a href="lec_03_computation.html#informally-defining-basic-operations-and-algorithms"><i class="fa fa-check"></i><b>3.2.3</b> Informally defining basic operations and algorithms</a></li></ul></li><li class="chapter" data-level="3.3" data-path="lec_03_computation.html"><a href="lec_03_computation.html#booleancircuitfig"><i class="fa fa-check"></i><b>3.3</b> Boolean Circuits</a><ul><li class="chapter" data-level="3.3.1" data-path="lec_03_computation.html"><a href="lec_03_computation.html#boolean-circuits-a-formal-definition"><i class="fa fa-check"></i><b>3.3.1</b> Boolean circuits: a formal definition</a></li><li class="chapter" data-level="3.3.2" data-path="lec_03_computation.html"><a href="lec_03_computation.html#equivalence-of-circuits-and-straight-line-programs"><i class="fa fa-check"></i><b>3.3.2</b> Equivalence of circuits and straight-line programs</a></li></ul></li><li class="chapter" data-level="3.4" data-path="lec_03_computation.html"><a href="lec_03_computation.html#physicalimplementationsec"><i class="fa fa-check"></i><b>3.4</b> Physical implementations of computing devices (digression)</a><ul><li class="chapter" data-level="3.4.1" data-path="lec_03_computation.html"><a href="lec_03_computation.html#transistors"><i class="fa fa-check"></i><b>3.4.1</b> Transistors</a></li><li class="chapter" data-level="3.4.2" data-path="lec_03_computation.html"><a href="lec_03_computation.html#logical-gates-from-transistors"><i class="fa fa-check"></i><b>3.4.2</b> Logical gates from transistors</a></li><li class="chapter" data-level="3.4.3" data-path="lec_03_computation.html"><a href="lec_03_computation.html#biological-computing"><i class="fa fa-check"></i><b>3.4.3</b> Biological computing</a></li><li class="chapter" data-level="3.4.4" data-path="lec_03_computation.html"><a href="lec_03_computation.html#cellular-automata-and-the-game-of-life"><i class="fa fa-check"></i><b>3.4.4</b> Cellular automata and the game of life</a></li><li class="chapter" data-level="3.4.5" data-path="lec_03_computation.html"><a href="lec_03_computation.html#neural-networks"><i class="fa fa-check"></i><b>3.4.5</b> Neural networks</a></li><li class="chapter" data-level="3.4.6" data-path="lec_03_computation.html"><a href="lec_03_computation.html#a-computer-made-from-marbles-and-pipes"><i class="fa fa-check"></i><b>3.4.6</b> A computer made from marbles and pipes</a></li></ul></li><li class="chapter" data-level="3.5" data-path="lec_03_computation.html"><a href="lec_03_computation.html#nandsec"><i class="fa fa-check"></i><b>3.5</b> The NAND function</a><ul><li class="chapter" data-level="3.5.1" data-path="lec_03_computation.html"><a href="lec_03_computation.html#nand-circuits"><i class="fa fa-check"></i><b>3.5.1</b> NAND Circuits</a></li><li class="chapter" data-level="3.5.2" data-path="lec_03_computation.html"><a href="lec_03_computation.html#more-examples-of-nand-circuits-optional"><i class="fa fa-check"></i><b>3.5.2</b> More examples of NAND circuits (optional)</a></li><li class="chapter" data-level="3.5.3" data-path="lec_03_computation.html"><a href="lec_03_computation.html#nandcircsec"><i class="fa fa-check"></i><b>3.5.3</b> The NAND-CIRC Programming language</a></li></ul></li><li class="chapter" data-level="3.6" data-path="lec_03_computation.html"><a href="lec_03_computation.html#equivalence-of-all-these-models"><i class="fa fa-check"></i><b>3.6</b> Equivalence of all these models</a><ul><li class="chapter" data-level="3.6.1" data-path="lec_03_computation.html"><a href="lec_03_computation.html#othergatessec"><i class="fa fa-check"></i><b>3.6.1</b> Circuits with other gate sets</a></li><li class="chapter" data-level="3.6.2" data-path="lec_03_computation.html"><a href="lec_03_computation.html#specvsimplrem"><i class="fa fa-check"></i><b>3.6.2</b> Specification vs. implementation (again)</a></li></ul></li><li class="chapter" data-level="3.7" data-path="lec_03_computation.html"><a href="lec_03_computation.html#exercises"><i class="fa fa-check"></i><b>3.7</b> Exercises</a></li><li class="chapter" data-level="3.8" data-path="lec_03_computation.html"><a href="lec_03_computation.html#biographical-notes"><i class="fa fa-check"></i><b>3.8</b> Biographical notes</a></li></ul></li><li class="chapter" data-level="4" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html"><i class="fa fa-check"></i><b>4</b> Syntactic sugar, and computing every function</a><ul><li class="chapter" data-level="4.1" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#secsyntacticsugar"><i class="fa fa-check"></i><b>4.1</b> Some examples of syntactic sugar</a><ul><li class="chapter" data-level="4.1.1" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#user-defined-procedures"><i class="fa fa-check"></i><b>4.1.1</b> User-defined procedures</a></li><li class="chapter" data-level="4.1.2" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#functionsynsugarthmpython"><i class="fa fa-check"></i><b>4.1.2</b> Proof by Python (optional)</a></li><li class="chapter" data-level="4.1.3" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#ifstatementsec"><i class="fa fa-check"></i><b>4.1.3</b> Conditional statements</a></li></ul></li><li class="chapter" data-level="4.2" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#addexample"><i class="fa fa-check"></i><b>4.2</b> Extended example: Addition and Multiplication (optional)</a></li><li class="chapter" data-level="4.3" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#seclookupfunc"><i class="fa fa-check"></i><b>4.3</b> The LOOKUP function</a><ul><li class="chapter" data-level="4.3.1" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#constructing-a-nand-circ-program-for-lookup"><i class="fa fa-check"></i><b>4.3.1</b> Constructing a NAND-CIRC program for \ensuremath{\mathit{LOOKUP}}</a></li></ul></li><li class="chapter" data-level="4.4" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#seccomputeallfunctions"><i class="fa fa-check"></i><b>4.4</b> Computing every function</a><ul><li class="chapter" data-level="4.4.1" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#proof-of-nands-universality"><i class="fa fa-check"></i><b>4.4.1</b> Proof of NAND’s Universality</a></li><li class="chapter" data-level="4.4.2" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#tight-upper-bound"><i class="fa fa-check"></i><b>4.4.2</b> Improving by a factor of n (optional)</a></li></ul></li><li class="chapter" data-level="4.5" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#seccomputalternative"><i class="fa fa-check"></i><b>4.5</b> Computing every function: An alternative proof</a></li><li class="chapter" data-level="4.6" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#secdefinesizeclasses"><i class="fa fa-check"></i><b>4.6</b> The class \ensuremath{\mathit{SIZE}}(T)</a></li><li class="chapter" data-level="4.7" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#exercises"><i class="fa fa-check"></i><b>4.7</b> Exercises</a></li><li class="chapter" data-level="4.8" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#computeeveryfunctionbibnotes"><i class="fa fa-check"></i><b>4.8</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="5" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html"><i class="fa fa-check"></i><b>5</b> Code as data, data as code</a><ul><li class="chapter" data-level="5.1" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#representprogramsec"><i class="fa fa-check"></i><b>5.1</b> Representing programs as strings</a></li><li class="chapter" data-level="5.2" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#countingcircuitsec"><i class="fa fa-check"></i><b>5.2</b> Counting programs, and lower bounds on the size of NAND-CIRC programs</a><ul><li class="chapter" data-level="5.2.1" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#size-hierarchy-theorem-optional"><i class="fa fa-check"></i><b>5.2.1</b> Size hierarchy theorem (optional)</a></li></ul></li><li class="chapter" data-level="5.3" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#listoftuplesrepsec"><i class="fa fa-check"></i><b>5.3</b> The tuples representation</a><ul><li class="chapter" data-level="5.3.1" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#stringrepresentationrpgoramsec"><i class="fa fa-check"></i><b>5.3.1</b> From tuples to strings</a></li></ul></li><li class="chapter" data-level="5.4" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#a-nand-circ-interpreter-in-nand-circ"><i class="fa fa-check"></i><b>5.4</b> A NAND-CIRC interpreter in NAND-CIRC</a><ul><li class="chapter" data-level="5.4.1" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#efficient-universal-programs"><i class="fa fa-check"></i><b>5.4.1</b> Efficient universal programs</a></li><li class="chapter" data-level="5.4.2" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#a-nand-circ-interpeter-in-pseudocode"><i class="fa fa-check"></i><b>5.4.2</b> A NAND-CIRC interpeter in pseudocode</a></li><li class="chapter" data-level="5.4.3" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#nandevalpythonsec"><i class="fa fa-check"></i><b>5.4.3</b> A NAND interpreter in Python</a></li><li class="chapter" data-level="5.4.4" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#constructing-the-nand-circ-interpreter-in-nand-circ"><i class="fa fa-check"></i><b>5.4.4</b> Constructing the NAND-CIRC interpreter in NAND-CIRC</a></li></ul></li><li class="chapter" data-level="5.5" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#a-python-interpreter-in-nand-circ-discussion"><i class="fa fa-check"></i><b>5.5</b> A Python interpreter in NAND-CIRC (discussion)</a></li><li class="chapter" data-level="5.6" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#PECTTsec"><i class="fa fa-check"></i><b>5.6</b> The physical extended Church-Turing thesis (discussion)</a><ul><li class="chapter" data-level="5.6.1" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#attempts-at-refuting-the-pectt"><i class="fa fa-check"></i><b>5.6.1</b> Attempts at refuting the PECTT</a></li></ul></li><li class="chapter" data-level="5.7" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#recap-of-part-i-finite-computation"><i class="fa fa-check"></i><b>5.7</b> Recap of Part I: Finite Computation</a></li><li class="chapter" data-level="5.8" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#exercises"><i class="fa fa-check"></i><b>5.8</b> Exercises</a></li><li class="chapter" data-level="5.9" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#bibnotescodeasdata"><i class="fa fa-check"></i><b>5.9</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="6" data-path="lec_06_loops.html"><a href="lec_06_loops.html"><i class="fa fa-check"></i><b>6</b> Loops and infinity</a><ul><li class="chapter" data-level="6.1" data-path="lec_06_loops.html"><a href="lec_06_loops.html#turing-machines"><i class="fa fa-check"></i><b>6.1</b> Turing Machines</a><ul><li class="chapter" data-level="6.1.1" data-path="lec_06_loops.html"><a href="lec_06_loops.html#turingmachinepalindrome"><i class="fa fa-check"></i><b>6.1.1</b> Extended example: A Turing machine for palindromes</a></li><li class="chapter" data-level="6.1.2" data-path="lec_06_loops.html"><a href="lec_06_loops.html#turing-machines-a-formal-definition"><i class="fa fa-check"></i><b>6.1.2</b> Turing machines: a formal definition</a></li><li class="chapter" data-level="6.1.3" data-path="lec_06_loops.html"><a href="lec_06_loops.html#computable-functions"><i class="fa fa-check"></i><b>6.1.3</b> Computable functions</a></li><li class="chapter" data-level="6.1.4" data-path="lec_06_loops.html"><a href="lec_06_loops.html#infinite-loops-and-partial-functions"><i class="fa fa-check"></i><b>6.1.4</b> Infinite loops and partial functions</a></li></ul></li><li class="chapter" data-level="6.2" data-path="lec_06_loops.html"><a href="lec_06_loops.html#turing-machines-as-programming-languages"><i class="fa fa-check"></i><b>6.2</b> Turing machines as programming languages</a><ul><li class="chapter" data-level="6.2.1" data-path="lec_06_loops.html"><a href="lec_06_loops.html#the-nand-tm-programming-language"><i class="fa fa-check"></i><b>6.2.1</b> The NAND-TM Programming language</a></li><li class="chapter" data-level="6.2.2" data-path="lec_06_loops.html"><a href="lec_06_loops.html#sneak-peak-nand-tm-vs-turing-machines"><i class="fa fa-check"></i><b>6.2.2</b> Sneak peak: NAND-TM vs Turing machines</a></li><li class="chapter" data-level="6.2.3" data-path="lec_06_loops.html"><a href="lec_06_loops.html#examples"><i class="fa fa-check"></i><b>6.2.3</b> Examples</a></li></ul></li><li class="chapter" data-level="6.3" data-path="lec_06_loops.html"><a href="lec_06_loops.html#equivalence-of-turing-machines-and-nand-tm-programs"><i class="fa fa-check"></i><b>6.3</b> Equivalence of Turing machines and NAND-TM programs</a><ul><li class="chapter" data-level="6.3.1" data-path="lec_06_loops.html"><a href="lec_06_loops.html#specification-vs-implementation-again"><i class="fa fa-check"></i><b>6.3.1</b> Specification vs implementation (again)</a></li></ul></li><li class="chapter" data-level="6.4" data-path="lec_06_loops.html"><a href="lec_06_loops.html#nand-tm-syntactic-sugar"><i class="fa fa-check"></i><b>6.4</b> NAND-TM syntactic sugar</a><ul><li class="chapter" data-level="6.4.1" data-path="lec_06_loops.html"><a href="lec_06_loops.html#nandtminnerloopssec"><i class="fa fa-check"></i><b>6.4.1</b> GOTO and inner loops</a></li></ul></li><li class="chapter" data-level="6.5" data-path="lec_06_loops.html"><a href="lec_06_loops.html#uniformity-and-nand-vs-nand-tm-discussion"><i class="fa fa-check"></i><b>6.5</b> Uniformity, and NAND vs NAND-TM (discussion)</a></li><li class="chapter" data-level="6.6" data-path="lec_06_loops.html"><a href="lec_06_loops.html#exercises"><i class="fa fa-check"></i><b>6.6</b> Exercises</a></li><li class="chapter" data-level="6.7" data-path="lec_06_loops.html"><a href="lec_06_loops.html#chaploopnotes"><i class="fa fa-check"></i><b>6.7</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="7" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html"><i class="fa fa-check"></i><b>7</b> Equivalent models of computation</a><ul><li class="chapter" data-level="7.1" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#ram-machines-and-nand-ram"><i class="fa fa-check"></i><b>7.1</b> RAM machines and NAND-RAM</a></li><li class="chapter" data-level="7.2" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#nandtmgorydetailssec"><i class="fa fa-check"></i><b>7.2</b> The gory details (optional)</a><ul><li class="chapter" data-level="7.2.1" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#indexed-access-in-nand-tm"><i class="fa fa-check"></i><b>7.2.1</b> Indexed access in NAND-TM</a></li><li class="chapter" data-level="7.2.2" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#two-dimensional-arrays-in-nand-tm"><i class="fa fa-check"></i><b>7.2.2</b> Two dimensional arrays in NAND-TM</a></li><li class="chapter" data-level="7.2.3" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#all-the-rest"><i class="fa fa-check"></i><b>7.2.3</b> All the rest</a></li></ul></li><li class="chapter" data-level="7.3" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#turing-equivalence-discussion"><i class="fa fa-check"></i><b>7.3</b> Turing equivalence (discussion)</a><ul><li class="chapter" data-level="7.3.1" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#the-best-of-both-worlds-paradigm"><i class="fa fa-check"></i><b>7.3.1</b> The Best of both worlds paradigm</a></li><li class="chapter" data-level="7.3.2" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#lets-talk-about-abstractions."><i class="fa fa-check"></i><b>7.3.2</b> Let’s talk about abstractions.</a></li><li class="chapter" data-level="7.3.3" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#turingcompletesec"><i class="fa fa-check"></i><b>7.3.3</b> Turing completeness and equivalence, a formal definition (optional)</a></li></ul></li><li class="chapter" data-level="7.4" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#cellularautomatasec"><i class="fa fa-check"></i><b>7.4</b> Cellular automata</a><ul><li class="chapter" data-level="7.4.1" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#one-dimensional-cellular-automata-are-turing-complete"><i class="fa fa-check"></i><b>7.4.1</b> One dimensional cellular automata are Turing complete</a></li><li class="chapter" data-level="7.4.2" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#turingmachinesconfigsec"><i class="fa fa-check"></i><b>7.4.2</b> Configurations of Turing machines and the next-step function</a></li></ul></li><li class="chapter" data-level="7.5" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#lambdacalculussec"><i class="fa fa-check"></i><b>7.5</b> Lambda calculus and functional programming languages</a><ul><li class="chapter" data-level="7.5.1" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#applying-functions-to-functions"><i class="fa fa-check"></i><b>7.5.1</b> Applying functions to functions</a></li><li class="chapter" data-level="7.5.2" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#curryingsec"><i class="fa fa-check"></i><b>7.5.2</b> Obtaining multi-argument functions via Currying</a></li><li class="chapter" data-level="7.5.3" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#formal-description-of-the-λ-calculus."><i class="fa fa-check"></i><b>7.5.3</b> Formal description of the λ calculus.</a></li><li class="chapter" data-level="7.5.4" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#infiniteloopslambda"><i class="fa fa-check"></i><b>7.5.4</b> Infinite loops in the λ calculus</a></li></ul></li><li class="chapter" data-level="7.6" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#the-enhanced-λ-calculus"><i class="fa fa-check"></i><b>7.6</b> The Enhanced λ calculus</a><ul><li class="chapter" data-level="7.6.1" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#computing-a-function-in-the-enhanced-λ-calculus"><i class="fa fa-check"></i><b>7.6.1</b> Computing a function in the enhanced λ calculus</a></li><li class="chapter" data-level="7.6.2" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#enhanced-λ-calculus-is-turing-complete"><i class="fa fa-check"></i><b>7.6.2</b> Enhanced λ calculus is Turing-complete</a></li></ul></li><li class="chapter" data-level="7.7" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#lambdacacluluspuresec"><i class="fa fa-check"></i><b>7.7</b> From enhanced to pure λ calculus</a><ul><li class="chapter" data-level="7.7.1" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#list-processing"><i class="fa fa-check"></i><b>7.7.1</b> List processing</a></li><li class="chapter" data-level="7.7.2" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#ycombinatorsec"><i class="fa fa-check"></i><b>7.7.2</b> The Y combinator, or recursion without recursion</a></li></ul></li><li class="chapter" data-level="7.8" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#churchturingdiscussionsec"><i class="fa fa-check"></i><b>7.8</b> The Church-Turing Thesis (discussion)</a><ul><li class="chapter" data-level="7.8.1" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#different-models-of-computation"><i class="fa fa-check"></i><b>7.8.1</b> Different models of computation</a></li></ul></li><li class="chapter" data-level="7.9" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#exercises"><i class="fa fa-check"></i><b>7.9</b> Exercises</a></li><li class="chapter" data-level="7.10" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#othermodelsbibnotes"><i class="fa fa-check"></i><b>7.10</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="8" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html"><i class="fa fa-check"></i><b>8</b> Universality and uncomputability</a><ul><li class="chapter" data-level="8.1" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#universality-or-a-meta-circular-evaluator"><i class="fa fa-check"></i><b>8.1</b> Universality or a meta-circular evaluator</a><ul><li class="chapter" data-level="8.1.1" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#representtmsec"><i class="fa fa-check"></i><b>8.1.1</b> Proving the existence of a universal Turing Machine</a></li><li class="chapter" data-level="8.1.2" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#implications-of-universality-discussion"><i class="fa fa-check"></i><b>8.1.2</b> Implications of universality (discussion)</a></li></ul></li><li class="chapter" data-level="8.2" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#is-every-function-computable"><i class="fa fa-check"></i><b>8.2</b> Is every function computable?</a></li><li class="chapter" data-level="8.3" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#haltingsec"><i class="fa fa-check"></i><b>8.3</b> The Halting problem</a><ul><li class="chapter" data-level="8.3.1" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#is-the-halting-problem-really-hard-discussion"><i class="fa fa-check"></i><b>8.3.1</b> Is the Halting problem really hard? (discussion)</a></li><li class="chapter" data-level="8.3.2" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#haltalternativesec"><i class="fa fa-check"></i><b>8.3.2</b> A direct proof of the uncomputability of \ensuremath{\mathit{HALT}} (optional)</a></li></ul></li><li class="chapter" data-level="8.4" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#reductionsuncompsec"><i class="fa fa-check"></i><b>8.4</b> Reductions</a><ul><li class="chapter" data-level="8.4.1" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#example-halting-on-the-zero-problem"><i class="fa fa-check"></i><b>8.4.1</b> Example: Halting on the zero problem</a></li></ul></li><li class="chapter" data-level="8.5" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#rices-theorem-and-the-impossibility-of-general-software-verification"><i class="fa fa-check"></i><b>8.5</b> Rice’s Theorem and the impossibility of general software verification</a><ul><li class="chapter" data-level="8.5.1" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#ricethmsec"><i class="fa fa-check"></i><b>8.5.1</b> Rice’s Theorem</a></li><li class="chapter" data-level="8.5.2" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#halting-and-rices-theorem-for-other-turing-complete-models"><i class="fa fa-check"></i><b>8.5.2</b> Halting and Rice’s Theorem for other Turing-complete models</a></li><li class="chapter" data-level="8.5.3" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#is-software-verification-doomed-discussion"><i class="fa fa-check"></i><b>8.5.3</b> Is software verification doomed? (discussion)</a></li></ul></li><li class="chapter" data-level="8.6" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#exercises"><i class="fa fa-check"></i><b>8.6</b> Exercises</a></li><li class="chapter" data-level="8.7" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#uncomputablebibnotes"><i class="fa fa-check"></i><b>8.7</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="9" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html"><i class="fa fa-check"></i><b>9</b> Restricted computational models</a><ul><li class="chapter" data-level="9.1" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#turing-completeness-as-a-bug"><i class="fa fa-check"></i><b>9.1</b> Turing completeness as a bug</a></li><li class="chapter" data-level="9.2" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#regular-expressions"><i class="fa fa-check"></i><b>9.2</b> Regular expressions</a></li><li class="chapter" data-level="9.3" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#deterministic-finite-automata-and-efficient-matching-of-regular-expressions-optional"><i class="fa fa-check"></i><b>9.3</b> Deterministic finite automata, and efficient matching of regular expressions (optional)</a><ul><li class="chapter" data-level="9.3.1" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#matching-regular-expressions-using-constant-memory"><i class="fa fa-check"></i><b>9.3.1</b> Matching regular expressions using constant memory</a></li><li class="chapter" data-level="9.3.2" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#secdfa"><i class="fa fa-check"></i><b>9.3.2</b> Deterministic Finite Automata</a></li><li class="chapter" data-level="9.3.3" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#regular-functions-are-closed-under-complement"><i class="fa fa-check"></i><b>9.3.3</b> Regular functions are closed under complement</a></li></ul></li><li class="chapter" data-level="9.4" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#limitations-of-regular-expressions"><i class="fa fa-check"></i><b>9.4</b> Limitations of regular expressions</a></li><li class="chapter" data-level="9.5" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#other-semantic-properties-of-regular-expressions"><i class="fa fa-check"></i><b>9.5</b> Other semantic properties of regular expressions</a></li><li class="chapter" data-level="9.6" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#seccfg"><i class="fa fa-check"></i><b>9.6</b> Context free grammars</a><ul><li class="chapter" data-level="9.6.1" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#context-free-grammars-as-a-computational-model"><i class="fa fa-check"></i><b>9.6.1</b> Context-free grammars as a computational model</a></li><li class="chapter" data-level="9.6.2" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#the-power-of-context-free-grammars"><i class="fa fa-check"></i><b>9.6.2</b> The power of context free grammars</a></li><li class="chapter" data-level="9.6.3" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#limitations-of-context-free-grammars-optional"><i class="fa fa-check"></i><b>9.6.3</b> Limitations of context-free grammars (optional)</a></li></ul></li><li class="chapter" data-level="9.7" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#semantic-properties-of-context-free-languages"><i class="fa fa-check"></i><b>9.7</b> Semantic properties of context free languages</a><ul><li class="chapter" data-level="9.7.1" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#uncomputability-of-context-free-grammar-equivalence-optional"><i class="fa fa-check"></i><b>9.7.1</b> Uncomputability of context-free grammar equivalence (optional)</a></li></ul></li><li class="chapter" data-level="9.8" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#summary-of-semantic-properties-for-regular-expressions-and-context-free-grammars"><i class="fa fa-check"></i><b>9.8</b> Summary of semantic properties for regular expressions and context-free grammars</a></li><li class="chapter" data-level="9.9" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#exercises"><i class="fa fa-check"></i><b>9.9</b> Exercises</a></li><li class="chapter" data-level="9.10" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#bibliographical-notes"><i class="fa fa-check"></i><b>9.10</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="10" data-path="lec_09_godel.html"><a href="lec_09_godel.html"><i class="fa fa-check"></i><b>10</b> Is every theorem provable?</a><ul><li class="chapter" data-level="10.1" data-path="lec_09_godel.html"><a href="lec_09_godel.html#godelproofdef"><i class="fa fa-check"></i><b>10.1</b> Hilbert’s Program and Gödel’s Incompleteness Theorem</a><ul><li class="chapter" data-level="10.1.1" data-path="lec_09_godel.html"><a href="lec_09_godel.html#godelproofsystemssec"><i class="fa fa-check"></i><b>10.1.1</b> Defining Proof Systems</a></li></ul></li><li class="chapter" data-level="10.2" data-path="lec_09_godel.html"><a href="lec_09_godel.html#gödels-incompleteness-theorem-computational-variant"><i class="fa fa-check"></i><b>10.2</b> Gödel’s Incompleteness Theorem: Computational variant</a></li><li class="chapter" data-level="10.3" data-path="lec_09_godel.html"><a href="lec_09_godel.html#quantified-integer-statements"><i class="fa fa-check"></i><b>10.3</b> Quantified integer statements</a></li><li class="chapter" data-level="10.4" data-path="lec_09_godel.html"><a href="lec_09_godel.html#diophantine-equations-and-the-mrdp-theorem"><i class="fa fa-check"></i><b>10.4</b> Diophantine equations and the MRDP Theorem</a></li><li class="chapter" data-level="10.5" data-path="lec_09_godel.html"><a href="lec_09_godel.html#hardness-of-quantified-integer-statements"><i class="fa fa-check"></i><b>10.5</b> Hardness of quantified integer statements</a><ul><li class="chapter" data-level="10.5.1" data-path="lec_09_godel.html"><a href="lec_09_godel.html#step-1-quantified-mixed-statements-and-computation-histories"><i class="fa fa-check"></i><b>10.5.1</b> Step 1: Quantified mixed statements and computation histories</a></li><li class="chapter" data-level="10.5.2" data-path="lec_09_godel.html"><a href="lec_09_godel.html#step-2-reducing-mixed-statements-to-integer-statements"><i class="fa fa-check"></i><b>10.5.2</b> Step 2: Reducing mixed statements to integer statements</a></li></ul></li><li class="chapter" data-level="10.6" data-path="lec_09_godel.html"><a href="lec_09_godel.html#exercises"><i class="fa fa-check"></i><b>10.6</b> Exercises</a></li><li class="chapter" data-level="10.7" data-path="lec_09_godel.html"><a href="lec_09_godel.html#bibliographical-notes"><i class="fa fa-check"></i><b>10.7</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="11" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html"><i class="fa fa-check"></i><b>11</b> Efficient computation</a><ul><li class="chapter" data-level="11.1" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#problems-on-graphs"><i class="fa fa-check"></i><b>11.1</b> Problems on graphs</a><ul><li class="chapter" data-level="11.1.1" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#finding-the-shortest-path-in-a-graph"><i class="fa fa-check"></i><b>11.1.1</b> Finding the shortest path in a graph</a></li><li class="chapter" data-level="11.1.2" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#finding-the-longest-path-in-a-graph"><i class="fa fa-check"></i><b>11.1.2</b> Finding the longest path in a graph</a></li><li class="chapter" data-level="11.1.3" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#mincutsec"><i class="fa fa-check"></i><b>11.1.3</b> Finding the minimum cut in a graph</a></li><li class="chapter" data-level="11.1.4" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#linerprogsec"><i class="fa fa-check"></i><b>11.1.4</b> Min-Cut Max-Flow and Linear programming</a></li><li class="chapter" data-level="11.1.5" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#finding-the-maximum-cut-in-a-graph"><i class="fa fa-check"></i><b>11.1.5</b> Finding the maximum cut in a graph</a></li><li class="chapter" data-level="11.1.6" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#a-note-on-convexity"><i class="fa fa-check"></i><b>11.1.6</b> A note on convexity</a></li></ul></li><li class="chapter" data-level="11.2" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#beyond-graphs"><i class="fa fa-check"></i><b>11.2</b> Beyond graphs</a><ul><li class="chapter" data-level="11.2.1" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#sat"><i class="fa fa-check"></i><b>11.2.1</b> SAT</a></li><li class="chapter" data-level="11.2.2" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#solving-linear-equations"><i class="fa fa-check"></i><b>11.2.2</b> Solving linear equations</a></li><li class="chapter" data-level="11.2.3" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#solving-quadratic-equations"><i class="fa fa-check"></i><b>11.2.3</b> Solving quadratic equations</a></li></ul></li><li class="chapter" data-level="11.3" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#more-advanced-examples"><i class="fa fa-check"></i><b>11.3</b> More advanced examples</a><ul><li class="chapter" data-level="11.3.1" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#determinant-of-a-matrix"><i class="fa fa-check"></i><b>11.3.1</b> Determinant of a matrix</a></li><li class="chapter" data-level="11.3.2" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#permanent-of-a-matrix"><i class="fa fa-check"></i><b>11.3.2</b> Permanent of a matrix</a></li><li class="chapter" data-level="11.3.3" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#finding-a-zero-sum-equilibrium"><i class="fa fa-check"></i><b>11.3.3</b> Finding a zero-sum equilibrium</a></li><li class="chapter" data-level="11.3.4" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#finding-a-nash-equilibrium"><i class="fa fa-check"></i><b>11.3.4</b> Finding a Nash equilibrium</a></li><li class="chapter" data-level="11.3.5" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#primality-testing"><i class="fa fa-check"></i><b>11.3.5</b> Primality testing</a></li><li class="chapter" data-level="11.3.6" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#integer-factoring"><i class="fa fa-check"></i><b>11.3.6</b> Integer factoring</a></li></ul></li><li class="chapter" data-level="11.4" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#our-current-knowledge"><i class="fa fa-check"></i><b>11.4</b> Our current knowledge</a></li><li class="chapter" data-level="11.5" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#exercises"><i class="fa fa-check"></i><b>11.5</b> Exercises</a></li><li class="chapter" data-level="11.6" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#effalgnotes"><i class="fa fa-check"></i><b>11.6</b> Bibliographical notes</a></li><li class="chapter" data-level="11.7" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#further-explorations"><i class="fa fa-check"></i><b>11.7</b> Further explorations</a></li></ul></li><li class="chapter" data-level="12" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html"><i class="fa fa-check"></i><b>12</b> Modeling running time</a><ul><li class="chapter" data-level="12.1" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#formally-defining-running-time"><i class="fa fa-check"></i><b>12.1</b> Formally defining running time</a><ul><li class="chapter" data-level="12.1.1" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#polynomial-and-exponential-time"><i class="fa fa-check"></i><b>12.1.1</b> Polynomial and Exponential Time</a></li></ul></li><li class="chapter" data-level="12.2" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#modeling-running-time-using-ram-machines-nand-ram"><i class="fa fa-check"></i><b>12.2</b> Modeling running time using RAM Machines / NAND-RAM</a></li><li class="chapter" data-level="12.3" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#ECTTsec"><i class="fa fa-check"></i><b>12.3</b> Extended Church-Turing Thesis (discussion)</a></li><li class="chapter" data-level="12.4" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#efficient-universal-machine-a-nand-ram-interpreter-in-nand-ram"><i class="fa fa-check"></i><b>12.4</b> Efficient universal machine: a NAND-RAM interpreter in NAND-RAM</a><ul><li class="chapter" data-level="12.4.1" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#timed-universal-turing-machine"><i class="fa fa-check"></i><b>12.4.1</b> Timed Universal Turing Machine</a></li></ul></li><li class="chapter" data-level="12.5" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#the-time-hierarchy-theorem"><i class="fa fa-check"></i><b>12.5</b> The time hierarchy theorem</a></li><li class="chapter" data-level="12.6" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#nonuniformcompsec"><i class="fa fa-check"></i><b>12.6</b> Non uniform computation</a><ul><li class="chapter" data-level="12.6.1" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#obliviousnandtm"><i class="fa fa-check"></i><b>12.6.1</b> Oblivious NAND-TM programs</a></li><li class="chapter" data-level="12.6.2" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#unrollloopsec"><i class="fa fa-check"></i><b>12.6.2</b> Unrolling the loop: algorithmic transformation of Turing Machines to circuits</a></li><li class="chapter" data-level="12.6.3" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#can-uniform-algorithms-simulate-non-uniform-ones"><i class="fa fa-check"></i><b>12.6.3</b> Can uniform algorithms simulate non uniform ones?</a></li><li class="chapter" data-level="12.6.4" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#uniform-vs.-nonuniform-computation-a-recap"><i class="fa fa-check"></i><b>12.6.4</b> Uniform vs. Nonuniform computation: A recap</a></li></ul></li><li class="chapter" data-level="12.7" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#exercises"><i class="fa fa-check"></i><b>12.7</b> Exercises</a></li><li class="chapter" data-level="12.8" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#bibnotesrunningtime"><i class="fa fa-check"></i><b>12.8</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="13" data-path="lec_12_NP.html"><a href="lec_12_NP.html"><i class="fa fa-check"></i><b>13</b> Polynomial-time reductions</a><ul><li class="chapter" data-level="13.1" data-path="lec_12_NP.html"><a href="lec_12_NP.html#formaldefdecisionexamplessec"><i class="fa fa-check"></i><b>13.1</b> Formal definitions of problems</a></li><li class="chapter" data-level="13.2" data-path="lec_12_NP.html"><a href="lec_12_NP.html#polytimeredsec"><i class="fa fa-check"></i><b>13.2</b> Polynomial-time reductions</a></li><li class="chapter" data-level="13.3" data-path="lec_12_NP.html"><a href="lec_12_NP.html#reducing-3sat-to-zero-one-equations"><i class="fa fa-check"></i><b>13.3</b> Reducing 3SAT to zero one equations</a><ul><li class="chapter" data-level="13.3.1" data-path="lec_12_NP.html"><a href="lec_12_NP.html#quadratic-equations"><i class="fa fa-check"></i><b>13.3.1</b> Quadratic equations</a></li></ul></li><li class="chapter" data-level="13.4" data-path="lec_12_NP.html"><a href="lec_12_NP.html#the-independent-set-problem"><i class="fa fa-check"></i><b>13.4</b> The independent set problem</a></li><li class="chapter" data-level="13.5" data-path="lec_12_NP.html"><a href="lec_12_NP.html#reducing-independent-set-to-maximum-cut"><i class="fa fa-check"></i><b>13.5</b> Reducing Independent Set to Maximum Cut</a></li><li class="chapter" data-level="13.6" data-path="lec_12_NP.html"><a href="lec_12_NP.html#reducing-3sat-to-longest-path"><i class="fa fa-check"></i><b>13.6</b> Reducing 3SAT to Longest Path</a><ul><li class="chapter" data-level="13.6.1" data-path="lec_12_NP.html"><a href="lec_12_NP.html#summary-of-relations"><i class="fa fa-check"></i><b>13.6.1</b> Summary of relations</a></li></ul></li><li class="chapter" data-level="13.7" data-path="lec_12_NP.html"><a href="lec_12_NP.html#exercises"><i class="fa fa-check"></i><b>13.7</b> Exercises</a></li><li class="chapter" data-level="13.8" data-path="lec_12_NP.html"><a href="lec_12_NP.html#reductionsbibnotes"><i class="fa fa-check"></i><b>13.8</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="14" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html"><i class="fa fa-check"></i><b>14</b> NP, NP completeness, and the Cook-Levin Theorem</a><ul><li class="chapter" data-level="14.1" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#the-class-mathbfnp"><i class="fa fa-check"></i><b>14.1</b> The class \mathbf{NP}</a><ul><li class="chapter" data-level="14.1.1" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#examples-of-functions-in-mathbfnp"><i class="fa fa-check"></i><b>14.1.1</b> Examples of functions in \mathbf{NP}</a></li><li class="chapter" data-level="14.1.2" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#basic-facts-about-mathbfnp"><i class="fa fa-check"></i><b>14.1.2</b> Basic facts about \mathbf{NP}</a></li></ul></li><li class="chapter" data-level="14.2" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#from-mathbfnp-to-3sat-the-cook-levin-theorem"><i class="fa fa-check"></i><b>14.2</b> From \mathbf{NP} to 3SAT: The Cook-Levin Theorem</a><ul><li class="chapter" data-level="14.2.1" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#what-does-this-mean"><i class="fa fa-check"></i><b>14.2.1</b> What does this mean?</a></li><li class="chapter" data-level="14.2.2" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#the-cook-levin-theorem-proof-outline"><i class="fa fa-check"></i><b>14.2.2</b> The Cook-Levin Theorem: Proof outline</a></li></ul></li><li class="chapter" data-level="14.3" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#the-nandsat-problem-and-why-it-is-mathbfnp-hard."><i class="fa fa-check"></i><b>14.3</b> The \ensuremath{\mathit{NANDSAT}} Problem, and why it is \mathbf{NP} hard.</a></li><li class="chapter" data-level="14.4" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#the-3nand-problem"><i class="fa fa-check"></i><b>14.4</b> The 3\ensuremath{\mathit{NAND}} problem</a></li><li class="chapter" data-level="14.5" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#from-3nand-to-3sat"><i class="fa fa-check"></i><b>14.5</b> From 3\ensuremath{\mathit{NAND}} to 3\ensuremath{\mathit{SAT}}</a></li><li class="chapter" data-level="14.6" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#wrapping-up"><i class="fa fa-check"></i><b>14.6</b> Wrapping up</a></li><li class="chapter" data-level="14.7" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#exercises"><i class="fa fa-check"></i><b>14.7</b> Exercises</a></li><li class="chapter" data-level="14.8" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#bibliographical-notes"><i class="fa fa-check"></i><b>14.8</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="15" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html"><i class="fa fa-check"></i><b>15</b> What if P equals NP?</a><ul><li class="chapter" data-level="15.1" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#search-to-decision-reduction"><i class="fa fa-check"></i><b>15.1</b> Search-to-decision reduction</a></li><li class="chapter" data-level="15.2" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#optimizationsection"><i class="fa fa-check"></i><b>15.2</b> Optimization</a><ul><li class="chapter" data-level="15.2.1" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#example-supervised-learning"><i class="fa fa-check"></i><b>15.2.1</b> Example: Supervised learning</a></li><li class="chapter" data-level="15.2.2" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#example-breaking-cryptosystems"><i class="fa fa-check"></i><b>15.2.2</b> Example: Breaking cryptosystems</a></li></ul></li><li class="chapter" data-level="15.3" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#finding-mathematical-proofs"><i class="fa fa-check"></i><b>15.3</b> Finding mathematical proofs</a></li><li class="chapter" data-level="15.4" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#quantifier-elimination-advanced"><i class="fa fa-check"></i><b>15.4</b> Quantifier elimination (advanced)</a><ul><li class="chapter" data-level="15.4.1" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#selfimprovingsat"><i class="fa fa-check"></i><b>15.4.1</b> Application: self improving algorithm for 3\ensuremath{\mathit{SAT}}</a></li></ul></li><li class="chapter" data-level="15.5" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#approximating-counting-problems-and-posterior-sampling-advanced-optional"><i class="fa fa-check"></i><b>15.5</b> Approximating counting problems and posterior sampling (advanced, optional)</a></li><li class="chapter" data-level="15.6" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#what-does-all-of-this-imply"><i class="fa fa-check"></i><b>15.6</b> What does all of this imply?</a></li><li class="chapter" data-level="15.7" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#can-mathbfp-neq-mathbfnp-be-neither-true-nor-false"><i class="fa fa-check"></i><b>15.7</b> Can \mathbf{P} \neq \mathbf{NP} be neither true nor false?</a></li><li class="chapter" data-level="15.8" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#is-mathbfpmathbfnp-in-practice"><i class="fa fa-check"></i><b>15.8</b> Is \mathbf{P}=\mathbf{NP} in practice?</a></li><li class="chapter" data-level="15.9" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#what-if-mathbfp-neq-mathbfnp"><i class="fa fa-check"></i><b>15.9</b> What if \mathbf{P} \neq \mathbf{NP}?</a></li><li class="chapter" data-level="15.10" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#exercises"><i class="fa fa-check"></i><b>15.10</b> Exercises</a></li><li class="chapter" data-level="15.11" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#bibliographical-notes"><i class="fa fa-check"></i><b>15.11</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="16" data-path="lec_14a_space_complexity.html"><a href="lec_14a_space_complexity.html"><i class="fa fa-check"></i><b>16</b> Space bounded computation</a><ul><li class="chapter" data-level="16.1" data-path="lec_14a_space_complexity.html"><a href="lec_14a_space_complexity.html#lecture-summary"><i class="fa fa-check"></i><b>16.1</b> Lecture summary</a></li><li class="chapter" data-level="16.2" data-path="lec_14a_space_complexity.html"><a href="lec_14a_space_complexity.html#exercises"><i class="fa fa-check"></i><b>16.2</b> Exercises</a></li><li class="chapter" data-level="16.3" data-path="lec_14a_space_complexity.html"><a href="lec_14a_space_complexity.html#bibliographical-notes"><i class="fa fa-check"></i><b>16.3</b> Bibliographical notes</a></li><li class="chapter" data-level="16.4" data-path="lec_14a_space_complexity.html"><a href="lec_14a_space_complexity.html#further-explorations"><i class="fa fa-check"></i><b>16.4</b> Further explorations</a></li><li class="chapter" data-level="16.5" data-path="lec_14a_space_complexity.html"><a href="lec_14a_space_complexity.html#acknowledgements"><i class="fa fa-check"></i><b>16.5</b> Acknowledgements</a></li></ul></li><li class="chapter" data-level="17" data-path="lec_15_probability.html"><a href="lec_15_probability.html"><i class="fa fa-check"></i><b>17</b> Probability Theory 101</a><ul><li class="chapter" data-level="17.1" data-path="lec_15_probability.html"><a href="lec_15_probability.html#random-coins"><i class="fa fa-check"></i><b>17.1</b> Random coins</a><ul><li class="chapter" data-level="17.1.1" data-path="lec_15_probability.html"><a href="lec_15_probability.html#random-variables"><i class="fa fa-check"></i><b>17.1.1</b> Random variables</a></li><li class="chapter" data-level="17.1.2" data-path="lec_15_probability.html"><a href="lec_15_probability.html#distributions-over-strings"><i class="fa fa-check"></i><b>17.1.2</b> Distributions over strings</a></li><li class="chapter" data-level="17.1.3" data-path="lec_15_probability.html"><a href="lec_15_probability.html#more-general-sample-spaces."><i class="fa fa-check"></i><b>17.1.3</b> More general sample spaces.</a></li></ul></li><li class="chapter" data-level="17.2" data-path="lec_15_probability.html"><a href="lec_15_probability.html#correlations-and-independence"><i class="fa fa-check"></i><b>17.2</b> Correlations and independence</a><ul><li class="chapter" data-level="17.2.1" data-path="lec_15_probability.html"><a href="lec_15_probability.html#independent-random-variables"><i class="fa fa-check"></i><b>17.2.1</b> Independent random variables</a></li><li class="chapter" data-level="17.2.2" data-path="lec_15_probability.html"><a href="lec_15_probability.html#collections-of-independent-random-variables."><i class="fa fa-check"></i><b>17.2.2</b> Collections of independent random variables.</a></li></ul></li><li class="chapter" data-level="17.3" data-path="lec_15_probability.html"><a href="lec_15_probability.html#concentration-and-tail-bounds"><i class="fa fa-check"></i><b>17.3</b> Concentration and tail bounds</a><ul><li class="chapter" data-level="17.3.1" data-path="lec_15_probability.html"><a href="lec_15_probability.html#chebyshevs-inequality"><i class="fa fa-check"></i><b>17.3.1</b> Chebyshev’s Inequality</a></li><li class="chapter" data-level="17.3.2" data-path="lec_15_probability.html"><a href="lec_15_probability.html#the-chernoff-bound"><i class="fa fa-check"></i><b>17.3.2</b> The Chernoff bound</a></li></ul></li><li class="chapter" data-level="17.4" data-path="lec_15_probability.html"><a href="lec_15_probability.html#exercises"><i class="fa fa-check"></i><b>17.4</b> Exercises</a></li><li class="chapter" data-level="17.5" data-path="lec_15_probability.html"><a href="lec_15_probability.html#bibliographical-notes"><i class="fa fa-check"></i><b>17.5</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="18" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html"><i class="fa fa-check"></i><b>18</b> Probabilistic computation</a><ul><li class="chapter" data-level="18.1" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html#finding-approximately-good-maximum-cuts."><i class="fa fa-check"></i><b>18.1</b> Finding approximately good maximum cuts.</a><ul><li class="chapter" data-level="18.1.1" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html#amplifying-the-success-of-randomized-algorithms"><i class="fa fa-check"></i><b>18.1.1</b> Amplifying the success of randomized algorithms</a></li><li class="chapter" data-level="18.1.2" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html#success-amplification"><i class="fa fa-check"></i><b>18.1.2</b> Success amplification</a></li><li class="chapter" data-level="18.1.3" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html#two-sided-amplification"><i class="fa fa-check"></i><b>18.1.3</b> Two-sided amplification</a></li><li class="chapter" data-level="18.1.4" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html#what-does-this-mean"><i class="fa fa-check"></i><b>18.1.4</b> What does this mean?</a></li><li class="chapter" data-level="18.1.5" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html#solving-sat-through-randomization"><i class="fa fa-check"></i><b>18.1.5</b> Solving SAT through randomization</a></li><li class="chapter" data-level="18.1.6" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html#bipartite-matching."><i class="fa fa-check"></i><b>18.1.6</b> Bipartite matching.</a></li></ul></li><li class="chapter" data-level="18.2" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html#exercises"><i class="fa fa-check"></i><b>18.2</b> Exercises</a></li><li class="chapter" data-level="18.3" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html#bibliographical-notes"><i class="fa fa-check"></i><b>18.3</b> Bibliographical notes</a></li><li class="chapter" data-level="18.4" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html#acknowledgements"><i class="fa fa-check"></i><b>18.4</b> Acknowledgements</a></li></ul></li><li class="chapter" data-level="19" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html"><i class="fa fa-check"></i><b>19</b> Modeling randomized computation</a><ul><li class="chapter" data-level="19.1" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#modeling-randomized-computation"><i class="fa fa-check"></i><b>19.1</b> Modeling randomized computation</a><ul><li class="chapter" data-level="19.1.1" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#an-alternative-view-random-coins-as-an-extra-input"><i class="fa fa-check"></i><b>19.1.1</b> An alternative view: random coins as an extra input</a></li><li class="chapter" data-level="19.1.2" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#successamptwosided"><i class="fa fa-check"></i><b>19.1.2</b> Success amplification of two-sided error algorithms</a></li></ul></li><li class="chapter" data-level="19.2" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#mathbfbpp-and-mathbfnp-completeness"><i class="fa fa-check"></i><b>19.2</b> \mathbf{BPP} and \mathbf{NP} completeness</a></li><li class="chapter" data-level="19.3" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#the-power-of-randomization"><i class="fa fa-check"></i><b>19.3</b> The power of randomization</a><ul><li class="chapter" data-level="19.3.1" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#solving-mathbfbpp-in-exponential-time"><i class="fa fa-check"></i><b>19.3.1</b> Solving \mathbf{BPP} in exponential time</a></li><li class="chapter" data-level="19.3.2" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#simulating-randomized-algorithms-by-circuits"><i class="fa fa-check"></i><b>19.3.2</b> Simulating randomized algorithms by circuits</a></li></ul></li><li class="chapter" data-level="19.4" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#derandomization"><i class="fa fa-check"></i><b>19.4</b> Derandomization</a><ul><li class="chapter" data-level="19.4.1" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#pseudorandom-generators"><i class="fa fa-check"></i><b>19.4.1</b> Pseudorandom generators</a></li><li class="chapter" data-level="19.4.2" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#optimalprgconj"><i class="fa fa-check"></i><b>19.4.2</b> From existence to constructivity</a></li><li class="chapter" data-level="19.4.3" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#usefulness-of-pseudorandom-generators"><i class="fa fa-check"></i><b>19.4.3</b> Usefulness of pseudorandom generators</a></li></ul></li><li class="chapter" data-level="19.5" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#mathbfpmathbfnp-and-mathbfbpp-vs-mathbfp"><i class="fa fa-check"></i><b>19.5</b> \mathbf{P}=\mathbf{NP} and \mathbf{BPP} vs \mathbf{P}</a></li><li class="chapter" data-level="19.6" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#non-constructive-existence-of-pseudorandom-generators-advanced-optional"><i class="fa fa-check"></i><b>19.6</b> Non-constructive existence of pseudorandom generators (advanced, optional)</a></li><li class="chapter" data-level="19.7" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#exercises"><i class="fa fa-check"></i><b>19.7</b> Exercises</a></li><li class="chapter" data-level="19.8" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#modelrandbibnotes"><i class="fa fa-check"></i><b>19.8</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="20" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html"><i class="fa fa-check"></i><b>20</b> Cryptography</a><ul><li class="chapter" data-level="20.1" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#classical-cryptosystems"><i class="fa fa-check"></i><b>20.1</b> Classical cryptosystems</a></li><li class="chapter" data-level="20.2" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#defining-encryption"><i class="fa fa-check"></i><b>20.2</b> Defining encryption</a></li><li class="chapter" data-level="20.3" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#defining-security-of-encryption"><i class="fa fa-check"></i><b>20.3</b> Defining security of encryption</a></li><li class="chapter" data-level="20.4" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#perfect-secrecy"><i class="fa fa-check"></i><b>20.4</b> Perfect secrecy</a><ul><li class="chapter" data-level="20.4.1" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#example-perfect-secrecy-in-the-battlefield"><i class="fa fa-check"></i><b>20.4.1</b> Example: Perfect secrecy in the battlefield</a></li><li class="chapter" data-level="20.4.2" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#constructing-perfectly-secret-encryption"><i class="fa fa-check"></i><b>20.4.2</b> Constructing perfectly secret encryption</a></li></ul></li><li class="chapter" data-level="20.5" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#necessity-of-long-keys"><i class="fa fa-check"></i><b>20.5</b> Necessity of long keys</a></li><li class="chapter" data-level="20.6" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#computational-secrecy"><i class="fa fa-check"></i><b>20.6</b> Computational secrecy</a><ul><li class="chapter" data-level="20.6.1" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#stream-ciphers-or-the-derandomized-one-time-pad"><i class="fa fa-check"></i><b>20.6.1</b> Stream ciphers or the derandomized one-time pad</a></li></ul></li><li class="chapter" data-level="20.7" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#computational-secrecy-and-mathbfnp"><i class="fa fa-check"></i><b>20.7</b> Computational secrecy and \mathbf{NP}</a></li><li class="chapter" data-level="20.8" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#public-key-cryptography"><i class="fa fa-check"></i><b>20.8</b> Public key cryptography</a><ul><li class="chapter" data-level="20.8.1" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#defining-public-key-encryption"><i class="fa fa-check"></i><b>20.8.1</b> Defining public key encryption</a></li><li class="chapter" data-level="20.8.2" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#diffie-hellman-key-exchange"><i class="fa fa-check"></i><b>20.8.2</b> Diffie-Hellman key exchange</a></li></ul></li><li class="chapter" data-level="20.9" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#other-security-notions"><i class="fa fa-check"></i><b>20.9</b> Other security notions</a></li><li class="chapter" data-level="20.10" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#magic"><i class="fa fa-check"></i><b>20.10</b> Magic</a><ul><li class="chapter" data-level="20.10.1" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#zero-knowledge-proofs"><i class="fa fa-check"></i><b>20.10.1</b> Zero knowledge proofs</a></li><li class="chapter" data-level="20.10.2" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#fully-homomorphic-encryption"><i class="fa fa-check"></i><b>20.10.2</b> Fully homomorphic encryption</a></li><li class="chapter" data-level="20.10.3" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#multiparty-secure-computation"><i class="fa fa-check"></i><b>20.10.3</b> Multiparty secure computation</a></li></ul></li><li class="chapter" data-level="20.11" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#exercises"><i class="fa fa-check"></i><b>20.11</b> Exercises</a></li><li class="chapter" data-level="20.12" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#bibliographical-notes"><i class="fa fa-check"></i><b>20.12</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="21" data-path="lec_24_proofs.html"><a href="lec_24_proofs.html"><i class="fa fa-check"></i><b>21</b> Proofs and algorithms</a><ul><li class="chapter" data-level="21.1" data-path="lec_24_proofs.html"><a href="lec_24_proofs.html#exercises"><i class="fa fa-check"></i><b>21.1</b> Exercises</a></li><li class="chapter" data-level="21.2" data-path="lec_24_proofs.html"><a href="lec_24_proofs.html#bibliographical-notes"><i class="fa fa-check"></i><b>21.2</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="22" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html"><i class="fa fa-check"></i><b>22</b> Quantum computing</a><ul><li class="chapter" data-level="22.1" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#the-double-slit-experiment"><i class="fa fa-check"></i><b>22.1</b> The double slit experiment</a></li><li class="chapter" data-level="22.2" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantum-amplitudes"><i class="fa fa-check"></i><b>22.2</b> Quantum amplitudes</a><ul><li class="chapter" data-level="22.2.1" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#linear-algebra-quick-review"><i class="fa fa-check"></i><b>22.2.1</b> Linear algebra quick review</a></li></ul></li><li class="chapter" data-level="22.3" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#bellineqsec"><i class="fa fa-check"></i><b>22.3</b> Bell’s Inequality</a></li><li class="chapter" data-level="22.4" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantum-weirdness"><i class="fa fa-check"></i><b>22.4</b> Quantum weirdness</a></li><li class="chapter" data-level="22.5" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantum-computing-and-computation---an-executive-summary."><i class="fa fa-check"></i><b>22.5</b> Quantum computing and computation - an executive summary.</a></li><li class="chapter" data-level="22.6" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantum-systems"><i class="fa fa-check"></i><b>22.6</b> Quantum systems</a><ul><li class="chapter" data-level="22.6.1" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantum-amplitudes-1"><i class="fa fa-check"></i><b>22.6.1</b> Quantum amplitudes</a></li><li class="chapter" data-level="22.6.2" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantum-systems-an-executive-summary"><i class="fa fa-check"></i><b>22.6.2</b> Quantum systems: an executive summary</a></li></ul></li><li class="chapter" data-level="22.7" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#analysis-of-bells-inequality-optional"><i class="fa fa-check"></i><b>22.7</b> Analysis of Bell’s Inequality (optional)</a></li><li class="chapter" data-level="22.8" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantum-computation"><i class="fa fa-check"></i><b>22.8</b> Quantum computation</a><ul><li class="chapter" data-level="22.8.1" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantum-circuits"><i class="fa fa-check"></i><b>22.8.1</b> Quantum circuits</a></li><li class="chapter" data-level="22.8.2" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#qnand-circ-programs-optional"><i class="fa fa-check"></i><b>22.8.2</b> QNAND-CIRC programs (optional)</a></li><li class="chapter" data-level="22.8.3" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#uniform-computation"><i class="fa fa-check"></i><b>22.8.3</b> Uniform computation</a></li></ul></li><li class="chapter" data-level="22.9" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#physically-realizing-quantum-computation"><i class="fa fa-check"></i><b>22.9</b> Physically realizing quantum computation</a></li><li class="chapter" data-level="22.10" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#shors-algorithm-hearing-the-shape-of-prime-factors"><i class="fa fa-check"></i><b>22.10</b> Shor’s Algorithm: Hearing the shape of prime factors</a><ul><li class="chapter" data-level="22.10.1" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#period-finding"><i class="fa fa-check"></i><b>22.10.1</b> Period finding</a></li><li class="chapter" data-level="22.10.2" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#shors-algorithm-a-birds-eye-view"><i class="fa fa-check"></i><b>22.10.2</b> Shor’s Algorithm: A bird’s eye view</a></li></ul></li><li class="chapter" data-level="22.11" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantum-fourier-transform-advanced-optional"><i class="fa fa-check"></i><b>22.11</b> Quantum Fourier Transform (advanced, optional)</a><ul><li class="chapter" data-level="22.11.1" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantum-fourier-transform-over-the-boolean-cube-simons-algorithm"><i class="fa fa-check"></i><b>22.11.1</b> Quantum Fourier Transform over the Boolean Cube: Simon’s Algorithm</a></li><li class="chapter" data-level="22.11.2" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#from-fourier-to-period-finding-simons-algorithm-advanced-optional"><i class="fa fa-check"></i><b>22.11.2</b> From Fourier to Period finding: Simon’s Algorithm (advanced, optional)</a></li><li class="chapter" data-level="22.11.3" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#from-simon-to-shor-advanced-optional"><i class="fa fa-check"></i><b>22.11.3</b> From Simon to Shor (advanced, optional)</a></li></ul></li><li class="chapter" data-level="22.12" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#exercises"><i class="fa fa-check"></i><b>22.12</b> Exercises</a></li><li class="chapter" data-level="22.13" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantumbibnotessec"><i class="fa fa-check"></i><b>22.13</b> Bibliographical notes</a></li></ul></li><li class="divider"></li>
</ul>
<!--bookdown:toc2:end-->
      </nav>
    </div>

    <div class="book-header" role="navigation">
      <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Efficient computation</a>
      </h1>
    </div>

    <div class="book-body">
      <div class="body-inner">


        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<!--bookdown:toc:end-->
<!--bookdown:body:start-->

<div  class="section level2">

<!-- link to pdf version -->


<!-- start of header referring to comments -->
<div><p></p><p style="color:#871640;"><i class="fas fa-wrench"></i> See any bugs/typos/confusing explanations? <a href="https://github.com/boazbk/tcs/issues/new">Open a GitHub issue</a>. You can also <a href="#commentform">comment below</a> <i class="fas fa-wrench"></i></p></div>



<div><p style="color:#871640;">&#x2605; See also the <a id="pdflink" href='https://files.boazbarak.org/introtcs/lec_10_efficient_alg.pdf'><b>PDF version of this chapter</b></a> (better formatting/references) &#x2605;</p></div>

<!-- end of header referring to comments -->

<!--- start of actual content -->

<h1 id="chapefficient" data-number="11">Efficient computation</h1>
<div id="section" class="objectives" name="Objectives">
<ul>
<li>Describe at a high level some interesting computational problems.<br />
</li>
<li>The difference between polynomial and exponential time.<br />
</li>
<li>Examples of techniques for obtaining efficient algorithms<br />
</li>
<li>Examples of how seemingly small differences in problems can potentially make huge differences in their computational complexity.</li>
</ul>
</div>
<blockquote>
<p><em>“The problem of distinguishing prime numbers from composite and of resolving the latter into their prime factors is … one of the most important and useful in arithmetic … Nevertheless we must confess that all methods … are either restricted to very special cases or are so laborious … they try the patience of even the practiced calculator … and do not apply at all to larger numbers.”</em>, Carl Friedrich Gauss, 1798</p>
</blockquote>
<blockquote>
<p><em>“For practical purposes, the difference between algebraic and exponential order is often more crucial than the difference between finite and non-finite.”</em>, Jack Edmunds, “Paths, Trees, and Flowers”, 1963</p>
</blockquote>
<blockquote>
<div class="quote" name="Quote 11">
<p><em>“What is the most efficient way to sort a million 32-bit integers?”</em>, Eric Schmidt to Barack Obama, 2008</p>
<p><em>“I think the bubble sort would be the wrong way to go.”</em>, Barack Obama.</p>
</div>
</blockquote>
<p>So far we have been concerned with which functions are <em>computable</em> and which ones are not. In this chapter we look at the finer question of the <em>time</em> that it takes to compute functions, as a <em>function of their input length</em>. Time complexity is extremely important to both the theory and practice of computing, but in introductory courses, coding interviews, and software development, terms such as “<span><span class="math inline">\(O(n)\)</span></span> running time” are often used in an informal way. People don’t have a precise definition of what a linear-time algorithm is, but rather assume that “they’ll know it when they see it”. In this book we will define running time precisely, using the mathematical models of computation we developed in the previous chapters. This will allow us to ask (and sometimes answer) questions such as:</p>
<ul>
<li><p>“Is there a function that can be computed in <span><span class="math inline">\(O(n^2)\)</span></span> time but not in <span><span class="math inline">\(O(n)\)</span></span> time?”</p></li>
<li><p>“Are there natural problems for which the <em>best</em> algorithm (and not just the <em>best known</em>) requires <span><span class="math inline">\(2^{\Omega(n)}\)</span></span> time?”</p></li>
</ul>
<div id="runtimefunc" class="bigidea" name="Bigidea 15">
<p>The running time of an algorithm is not a <em>number</em>, it is a <em>function</em> of the length of the input.</p>
</div>
<p>We will see the precise definition of running time (using Turing machines and RAM machines / NAND-RAM) in <a href='lec_11_running_time.html#chapmodelruntime'>Chapter 12</a>. In this chapter, we informally survey examples of computational problems. For some of these problems we know efficient (i.e., <span><span class="math inline">\(O(n^c)\)</span></span>-time for a small constant <span><span class="math inline">\(c\)</span></span>) algorithms, and for others the best known algorithms are <em>exponential</em>. We present these examples to get a feel as to the kinds of problems that lie on each side of this divide and also see how sometimes seemingly minor changes in problem formulation can make the (known) complexity of a problem “jump” from polynomial to exponential. We do not formally define the notion of running time in this chapter, but use the same “I know it when I see it” notion of an <span><span class="math inline">\(O(n)\)</span></span> or <span><span class="math inline">\(O(n^2)\)</span></span> time algorithms as the one you’ve seen in introduction to computer science courses.</p>
<p>While the difference between <span><span class="math inline">\(O(n)\)</span></span> and <span><span class="math inline">\(O(n^2)\)</span></span> time can be crucial in practice, we focus on the difference between <em>polynomial</em> and <em>exponential</em> running time. One advantage is that, as we will see, questions about polynomial versus exponential time are often <em>insensitive</em> to the choice of the particular computational model, just as the question of whether a function <span><span class="math inline">\(F\)</span></span> is computable is insensitive to whether you use Turing machines, <span><span class="math inline">\(\lambda\)</span></span>-calculus, or Javascript as your model of computation. One of the interesting phenomena of computing is that there is often a kind of a “threshold phenomenon” or “zero-one law” for running time. Many natural problems can either be solved in <em>polynomial</em> running time with a <em>not-too-large exponent</em> (e.g., something like <span><span class="math inline">\(O(n^2)\)</span></span> or <span><span class="math inline">\(O(n^3)\)</span></span>), or require <em>exponential</em> (e.g., at least <span><span class="math inline">\(2^{\Omega(n)}\)</span></span> or <span><span class="math inline">\(2^{\Omega(\sqrt{n})}\)</span></span>) time to solve. The reasons for this phenomenon are still not fully understood, but some light on this is shed by the concept of <em>NP completeness</em>, which we will see in <a href='lec_13_Cook_Levin.html#cooklevinchap'>Chapter 14</a>.</p>
<p>This chapter is merely a tiny sample of the landscape of computational problems and efficient algorithms. If you want to explore the field of algorithms and data structures more deeply (which I very much hope you do!), the bibliographical notes contain references to some excellent texts, some of which are available freely on the web.</p>
<div id="relationpartsrem" class="remark" title="Relations between parts of this book" name="Remark 11.1 (Relations between parts of this book) ">
<p><strong>Part I</strong> of this book contained a <em>quantitative study</em> of computation of <em>finite functions</em>. We asked what are the resources (in terms of gates of Boolean circuits or lines in straight-line programs) required to compute various finite functions.</p>
<p><strong>Part II</strong> of the book contained a <em>qualitative study</em> of computation of <em>infinite functions</em> (i.e., functions of <em>unbounded input length</em>). In that part we asked the <em>qualitative question</em> of whether or not a function is computable at all, regardless of the number of operations.</p>
<p><strong>Part III</strong> of the book, beginning with this chapter, merges the two approaches and contains a <em>quantitative study</em> of computation of <em>infinite functions</em>. In this part we ask how do resources for computing a function <em>scale</em> with the length of the input. In <a href='lec_11_running_time.html#chapmodelruntime'>Chapter 12</a> we define the notion of running time, and the class <span><span class="math inline">\(\mathbf{P}\)</span></span> of functions that can be computed using a number of steps that scales <em>polynomially</em> with the input length. In <a href='lec_11_running_time.html#nonuniformcompsec'>Section 12.6</a> we will relate this class to the models of Boolean circuits and straightline programs that we studied in Part I.</p>
</div>
<h2 id="problems-on-graphs" data-number="11.1">Problems on graphs</h2>
<p>In this chapter we discuss several examples of important computational problems. Many of the problems will involve <em>graphs</em>. We have already encountered graphs before (see <a href='lec_00_1_math_background.html#graphsec'>Subsection 1.4.4</a>) but now quickly recall the basic notation. A graph <span><span class="math inline">\(G\)</span></span> consists of a set of <em>vertices</em> <span><span class="math inline">\(V\)</span></span> and <em>edges</em> <span><span class="math inline">\(E\)</span></span> where each edge is a pair of vertices. We typically denote by <span><span class="math inline">\(n\)</span></span> the number of vertices (and in fact often consider graphs where the set of vertices <span><span class="math inline">\(V\)</span></span> equals the set <span><span class="math inline">\([n]\)</span></span> of the integers between <span><span class="math inline">\(0\)</span></span> and <span><span class="math inline">\(n-1\)</span></span>). In a <em>directed</em> graph, an edge is an ordered pair <span><span class="math inline">\((u,v)\)</span></span>, which we sometimes denote as <span><span class="math inline">\(\overrightarrow{u\;v}\)</span></span>. In an <em>undirected</em> graph, an edge is an unordered pair (or simply a set) <span><span class="math inline">\(\{ u,v \}\)</span></span> which we sometimes denote as <span><span class="math inline">\(\overline{u\; v}\)</span></span> or <span><span class="math inline">\(u \sim v\)</span></span>. An equivalent viewpoint is that an undirected graph corresponds to a directed graph satisfying the property that whenever the edge <span><span class="math inline">\(\overrightarrow{u\; v}\)</span></span> is present then so is the edge <span><span class="math inline">\(\overrightarrow{v\; u}\)</span></span>. In this chapter we restrict our attention to graphs that are undirected and simple (i.e., containing no parallel edges or self-loops). Graphs can be represented either in the <em>adjacency list</em> or <em>adjacency matrix</em> representation. We can transform between these two representations using <span><span class="math inline">\(O(n^2)\)</span></span> operations, and hence for our purposes we will mostly consider them as equivalent.</p>
<p>Graphs are so ubiquitous in computer science and other sciences because they can be used to model a great many of the data that we encounter. These are not just the “obvious” data such as the road network (which can be thought of as a graph of whose vertices are locations with edges corresponding to road segments), or the web (which can be thought of as a graph whose vertices are web pages with edges corresponding to links), or social networks (which can be thought of as a graph whose vertices are people and the edges correspond to friend relation). Graphs can also denote correlations in data (e.g., graph of observations of features with edges corresponding to features that tend to appear together), causal relations (e.g., gene regulatory networks, where a gene is connected to gene products it derives), or the state space of a system (e.g., graph of configurations of a physical system, with edges corresponding to states that can be reached from one another in one step).</p>
<figure>
<img src="../figure/graphs.png" alt="11.1: Some examples of graphs found on the Internet." id="graphsfromwebfig" class="margin" /><figcaption>11.1: Some examples of graphs found on the Internet.</figcaption>
</figure>
<h3 id="finding-the-shortest-path-in-a-graph" data-number="11.1.1">Finding the shortest path in a graph</h3>
<p>The <em>shortest path problem</em> is the task of, given a graph <span><span class="math inline">\(G=(V,E)\)</span></span> and two vertices <span><span class="math inline">\(s,t \in V\)</span></span>, to find the length of the shortest path between <span><span class="math inline">\(s\)</span></span> and <span><span class="math inline">\(t\)</span></span> (if such a path exists). That is, we want to find the smallest number <span><span class="math inline">\(k\)</span></span> such that there are vertices <span><span class="math inline">\(v_0,v_1,\ldots,v_k\)</span></span> with <span><span class="math inline">\(v_0=s\)</span></span>, <span><span class="math inline">\(v_k=t\)</span></span> and for every <span><span class="math inline">\(i\in\{0,\ldots,k-1\}\)</span></span> an edge between <span><span class="math inline">\(v_i\)</span></span> and <span><span class="math inline">\(v_{i+1}\)</span></span>. Formally, we define <span><span class="math inline">\(\ensuremath{\mathit{MINPATH}}:\{0,1\}^* \rightarrow \{0,1\}^*\)</span></span> to be the function that on input a triple <span><span class="math inline">\((G,s,t)\)</span></span> (represented as a string) outputs the number <span><span class="math inline">\(k\)</span></span> which is the length of the shortest path in <span><span class="math inline">\(G\)</span></span> between <span><span class="math inline">\(s\)</span></span> and <span><span class="math inline">\(t\)</span></span> or a string representing <code>no path</code> if no such path exists. (In practice people often want to also find the actual path and not just its length; it turns out that the algorithms to compute the length of the path often yield the actual path itself as a byproduct, and so everything we say about the task of computing the length also applies to the task of finding the path.)</p>
<p>If each vertex has at least two neighbors then there can be an <em>exponential</em> number of paths from <span><span class="math inline">\(s\)</span></span> to <span><span class="math inline">\(t\)</span></span>, but fortunately we do not have to enumerate them all to find the shortest path. We can find the shortest path using a <a href="https://en.wikipedia.org/wiki/Breadth-first_search">breadth first search (BFS)</a>, enumerating <span><span class="math inline">\(s\)</span></span>’s neighbors, and then neighbors’ neighbors, etc.. in order. If we maintain the neighbors in a list we can perform a BFS in <span><span class="math inline">\(O(n^2)\)</span></span> time, while using a <em>queue</em> we can do this in <span><span class="math inline">\(O(m)\)</span></span> time.<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup> <a href="https://goo.gl/PJyc4D">Dijkstra’s algorithm</a> is a well-known generalization of BFS to <em>weighted</em> graphs. More formally, the algorithm for computing the function <span><span class="math inline">\(\ensuremath{\mathit{MINPATH}}\)</span></span> is described in <a href='#bfsshortpathalg'>Algorithm 11.2</a>.</p>
<div  class="pseudocodeoutput">
<div class="ps-root">
<div class="ps-algorithm with-caption" id = bfsshortpathalg>
<p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;">
<span class="ps-keyword">Algorithm 2 </span>Shortest path via BFS</p>
<div class="ps-algorithmic"><p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"><span class="ps-keyword">Input:</span>  Graph \(G=(V,E)\) and vertices \(s,t\in V\). Assume \(V=[n]\).<p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"><span class="ps-keyword">Output:</span>  Length \(k\) of shortest path from \(s\) to \(t\) or  \(\infty\) if no such path exists.<p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"> Let \(D\) be length-\(n\) array.<p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"> Set \(D[s]=0\) and \(D[i]=\infty\) for all \(i\in [n] \setminus \{s \}\).<p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"> Initialize queue \(Q\) to contain \(s\).<p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"><span class="ps-keyword">while</span>{\(S\) non empty} 
                <div class="ps-block" style="margin-left:1.2em;">
                <p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"> Pop \(v\) from \(Q\)<p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"><span class="ps-keyword">if</span>{\(v=t\)} 
                <div class="ps-block" style="margin-left:1.2em;">
                <p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"><span class="ps-keyword">return</span> \(D[v]\)
</div><p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"><span class="ps-keyword">endif</span><p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"><span class="ps-keyword">for</span>{\(u\) neighbor of \(v\) with \(D[u]=\infty\)} 
                <div class="ps-block" style="margin-left:1.2em;">
                <p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;">    Set \(D[u] \leftarrow D[v]+1\)<p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;">    Add \(u\) to \(Q\).
</div><p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"><span class="ps-keyword">endfor</span>
</div><p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"><span class="ps-keyword">endwhile</span><p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"><span class="ps-keyword">return</span> \(\infty\)<p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"></div>
</div>
</div>
</div>
<p>Since we only add to the queue vertices <span><span class="math inline">\(w\)</span></span> with <span><span class="math inline">\(D[w]=\infty\)</span></span> (and then immediately set <span><span class="math inline">\(D[w]\)</span></span> to an actual number), we never push to the queue a vertex more than once, and hence the algorithm makes at most <span><span class="math inline">\(n\)</span></span> “push” and “pop” operations. For each vertex <span><span class="math inline">\(v\)</span></span>, the number of times we run the inner loop is equal to the <em>degree</em> of <span><span class="math inline">\(v\)</span></span> and hence the total running time is proportional to the sum of all degrees which equals twice the number <span><span class="math inline">\(m\)</span></span> of edges. <a href='#bfsshortpathalg'>Algorithm 11.2</a> returns the correct answer since the vertices are added to the queue in the order of their distance from <span><span class="math inline">\(s\)</span></span>, and hence we will reach <span><span class="math inline">\(t\)</span></span> after we have explored all the vertices that are closer to <span><span class="math inline">\(s\)</span></span> than <span><span class="math inline">\(t\)</span></span>.</p>
<div id="datastructuresrem" class="remark" title="On data structures" name="Remark 11.3 (On data structures) ">
<p>If you’ve ever taken an algorithms course, you have probably encountered many <em>data structures</em> such as <strong>lists</strong>, <strong>arrays</strong>, <strong>queues</strong>, <strong>stacks</strong>, <strong>heaps</strong>, <strong>search trees</strong>, <strong>hash tables</strong> and many more. Data structures are extremely important in computer science, and each one of those offers different tradeoffs between overhead in storage, operations supported, cost in time for each operation, and more. For example, if we store <span><span class="math inline">\(n\)</span></span> items in a list, we will need a linear (i.e., <span><span class="math inline">\(O(n)\)</span></span> time) scan to retrieve an element, while we achieve the same operation in <span><span class="math inline">\(O(1)\)</span></span> time if we used a hash table. However, when we only care about polynomial-time algorithms, such factors of <span><span class="math inline">\(O(n)\)</span></span> in the running time will not make much difference. Similarly, if we don’t care about the difference between <span><span class="math inline">\(O(n)\)</span></span> and <span><span class="math inline">\(O(n^2)\)</span></span>, then it doesn’t matter if we represent graphs as adjacency lists or adjacency matrices. Hence we will often describe our algorithms at a very high level, without specifying the particular data structures that are used to implement them. However, it will always be clear that there exists <em>some</em> data structure that is sufficient for our purposes.</p>
</div>
<h3 id="finding-the-longest-path-in-a-graph" data-number="11.1.2">Finding the longest path in a graph</h3>
<p>The <em>longest path problem</em> is the task of finding the length of the <em>longest</em> simple (i.e., non intersecting) path between a given pair of vertices <span><span class="math inline">\(s\)</span></span> and <span><span class="math inline">\(t\)</span></span> in a given graph <span><span class="math inline">\(G\)</span></span>. If the graph is a road network, then the longest path might seem less motivated than the shortest path (unless you are the kind of person that always prefers the “scenic route”). But of graphs can and are used to model a variety of phenomena, and in many such cases finding the longest path (and some of its variants) can be very useful. In particular, finding the longest path is a generalization of the famous <a href="https://en.wikipedia.org/wiki/Hamiltonian_path_problem">Hamiltonian path problem</a> which asks for a <em>maximally long</em> simple path (i.e., path that visits all <span><span class="math inline">\(n\)</span></span> vertices once) between <span><span class="math inline">\(s\)</span></span> and <span><span class="math inline">\(t\)</span></span>, as well as the notorious <a href="https://en.wikipedia.org/wiki/Travelling_salesman_problem">traveling salesman problem (TSP)</a> of finding (in a weighted graph) a path visiting all vertices of cost at most <span><span class="math inline">\(w\)</span></span>. TSP is a classical optimization problem, with applications ranging from planning and logistics to DNA sequencing and astronomy.</p>
<p>Surprisingly, while we can find the shortest path in <span><span class="math inline">\(O(m)\)</span></span> time, there is no known algorithm for the <em>longest path problem</em> that significantly improves on the trivial “exhaustive search” or “brute force” algorithm that enumerates all the exponentially many possibilities for such paths. Specifically, the best known algorithms for the longest path problem take <span><span class="math inline">\(O(c^n)\)</span></span> time for some constant <span><span class="math inline">\(c&gt;1\)</span></span>. (At the moment the best record is <span><span class="math inline">\(c \sim 1.65\)</span></span> or so; even obtaining an <span><span class="math inline">\(O(2^n)\)</span></span> time bound is not that simple, see <a href='#longest-path-ex'>Exercise 11.1</a>.)</p>
<figure>
<img src="../figure/knights_tour.jpg" alt="11.2: A knight’s tour can be thought of as a maximally long path on the graph corresponding to a chessboard where we put an edge between any two squares that can be reached by one step via a legal knight move." id="knighttourpath" class="margin" /><figcaption>11.2: A <em>knight’s tour</em> can be thought of as a maximally long path on the graph corresponding to a chessboard where we put an edge between any two squares that can be reached by one step via a legal knight move.</figcaption>
</figure>
<h3 id="mincutsec" data-number="11.1.3">Finding the minimum cut in a graph</h3>
<p>Given a graph <span><span class="math inline">\(G=(V,E)\)</span></span>, a <em>cut</em> of <span><span class="math inline">\(G\)</span></span> is a subset <span><span class="math inline">\(S \subseteq V\)</span></span> such that <span><span class="math inline">\(S\)</span></span> is neither empty nor is it all of <span><span class="math inline">\(V\)</span></span>. The edges cut by <span><span class="math inline">\(S\)</span></span> are those edges where one of their endpoints is in <span><span class="math inline">\(S\)</span></span> and the other is in <span><span class="math inline">\(\overline{S} = V \setminus S\)</span></span>. We denote this set of edges by <span><span class="math inline">\(E(S,\overline{S})\)</span></span>. If <span><span class="math inline">\(s,t \in V\)</span></span> are a pair of vertices then an <em><span><span class="math inline">\(s,t\)</span></span> cut</em> is a cut such that <span><span class="math inline">\(s\in S\)</span></span> and <span><span class="math inline">\(t\in \overline{S}\)</span></span> (see <a href='#cutingraphfig'>Figure 11.3</a>). The <em>minimum <span><span class="math inline">\(s,t\)</span></span> cut problem</em> is the task of finding, given <span><span class="math inline">\(s\)</span></span> and <span><span class="math inline">\(t\)</span></span>, the minimum number <span><span class="math inline">\(k\)</span></span> such that there is an <span><span class="math inline">\(s,t\)</span></span> cut cutting <span><span class="math inline">\(k\)</span></span> edges (the problem is also sometimes phrased as finding the set that achieves this minimum; it turns out that algorithms to compute the number often yield the set as well). Formally, we define <span><span class="math inline">\(\ensuremath{\mathit{MINCUT}}:\{0,1\}^* \rightarrow \{0,1\}^*\)</span></span> to be the function that on input a string representing a triple <span><span class="math inline">\((G=(V,E),s,t)\)</span></span> of a graph and two vertices, outputs the minimum number <span><span class="math inline">\(k\)</span></span> such that there exists a set <span><span class="math inline">\(S \subseteq V\)</span></span> with <span><span class="math inline">\(s\in S\)</span></span>, <span><span class="math inline">\(t\not\in S\)</span></span> and <span><span class="math inline">\(|E(S,\overline{S})|=k\)</span></span>.</p>
<figure>
<img src="../figure/cutingraph.png" alt="11.3: A cut in a graph G=(V,E) is simply a subset S of its vertices. The edges that are cut by S are all those whose one endpoint is in S and the other one is in \overline{S} = V \setminus S. The cut edges are colored red in this figure." id="cutingraphfig" class="margin" /><figcaption>11.3: A <em>cut</em> in a graph <span><span class="math inline">\(G=(V,E)\)</span></span> is simply a subset <span><span class="math inline">\(S\)</span></span> of its vertices. The edges that are <em>cut</em> by <span><span class="math inline">\(S\)</span></span> are all those whose one endpoint is in <span><span class="math inline">\(S\)</span></span> and the other one is in <span><span class="math inline">\(\overline{S} = V \setminus S\)</span></span>. The cut edges are colored red in this figure.</figcaption>
</figure>
<p>Computing minimum <span><span class="math inline">\(s,t\)</span></span> cuts is useful for in many applications since minimum cuts often correspond to <em>bottlenecks</em>. For example, in a communication or railroad network the minimum cut between <span><span class="math inline">\(s\)</span></span> and <span><span class="math inline">\(t\)</span></span> corresponds to the smallest number of edges that, if dropped, will disconnect <span><span class="math inline">\(s\)</span></span> from <span><span class="math inline">\(t\)</span></span>. (This was actually the original motivation for this problem; see <a href='#effalgnotes'>Section 11.6</a>.) Similar applications arise in scheduling and planning. In the setting of <a href="https://en.wikipedia.org/wiki/Image_segmentation">image segmentation</a>, one can define a graph whose vertices are pixels and whose edges correspond to neighboring pixels of distinct colors. If we want to separate the foreground from the background then we can pick (or guess) a foreground pixel <span><span class="math inline">\(s\)</span></span> and background pixel <span><span class="math inline">\(t\)</span></span> and ask for a minimum cut between them.</p>
<p>The naive algorithm for computing <span><span class="math inline">\(\ensuremath{\mathit{MINCUT}}\)</span></span> will check all <span><span class="math inline">\(2^n\)</span></span> possible subsets of an <span><span class="math inline">\(n\)</span></span>-vertex graph, but it turns out we can do much better than that. As we’ve seen in this book time and again, there is more than one algorithm to compute the same function,and some of those algorithms might be more efficient than others. Luckily the minimum cut problem is one of those cases. In particular, as we will see in the next section, there are algorithms that compute <span><span class="math inline">\(\ensuremath{\mathit{MINCUT}}\)</span></span> in time which is <em>polynomial</em> in the number of vertices.</p>
<h3 id="linerprogsec" data-number="11.1.4">Min-Cut Max-Flow and Linear programming</h3>
<p>We can obtain a polynomial-time algorithm for computing <span><span class="math inline">\(\ensuremath{\mathit{MINCUT}}\)</span></span> using the <a href="https://en.wikipedia.org/wiki/Max-flow_min-cut_theorem">Max-Flow Min-Cut Theorem</a>. This theorem says that the minimum cut between <span><span class="math inline">\(s\)</span></span> and <span><span class="math inline">\(t\)</span></span> equals the maximum amount of <em>flow</em> we can send from <span><span class="math inline">\(s\)</span></span> to <span><span class="math inline">\(t\)</span></span>, if every edge has unit capacity. Specifically, imagine that every edge of the graph corresponded to a pipe that could carry one unit of fluid per one unit of time (say 1 liter of water per second). The <em>maximum <span><span class="math inline">\(s,t\)</span></span> flow</em> is the maximum units of water that we could transfer from <span><span class="math inline">\(s\)</span></span> to <span><span class="math inline">\(t\)</span></span> over these pipes. If there is an <span><span class="math inline">\(s,t\)</span></span> cut of <span><span class="math inline">\(k\)</span></span> edges, then the maximum flow is at most <span><span class="math inline">\(k\)</span></span>. The reason is that such a cut <span><span class="math inline">\(S\)</span></span> acts as a “bottleneck” since at most <span><span class="math inline">\(k\)</span></span> units can flow from <span><span class="math inline">\(S\)</span></span> to its complement at any given unit of time. This means that the maximum <span><span class="math inline">\(s,t\)</span></span> flow is always <em>at most</em> the value of the minimum <span><span class="math inline">\(s,t\)</span></span> cut. The surprising and non-trivial content of the Max-Flow Min-Cut Theorem is that the maximum flow is also <em>at least</em> the value of the minimum cut, and hence computing the cut is the same as computing the flow.</p>
<p>The Max-Flow Min-Cut Theorem reduces the task of computing a minimum cut to the task of computing a <em>maximum flow</em>. However, this still does not show how to compute such a flow. The <a href="https://en.wikipedia.org/wiki/Ford%E2%80%93Fulkerson_algorithm">Ford-Fulkerson Algorithm</a> is a direct way to compute a flow using incremental improvements. But computing flows in polynomial time is also a special case of a much more general tool known as <a href="https://en.wikipedia.org/wiki/Linear_programming">linear programming</a>.</p>
<p>A <em>flow</em> on a graph <span><span class="math inline">\(G\)</span></span> of <span><span class="math inline">\(m\)</span></span> edges can be modeled as a vector <span><span class="math inline">\(x\in \R^m\)</span></span> where for every edge <span><span class="math inline">\(e\)</span></span>, <span><span class="math inline">\(x_e\)</span></span> corresponds to the amount of water per time-unit that flows on <span><span class="math inline">\(e\)</span></span>. We think of an edge <span><span class="math inline">\(e\)</span></span> an an ordered pair <span><span class="math inline">\((u,v)\)</span></span> (we can choose the order arbitrarily) and let <span><span class="math inline">\(x_e\)</span></span> be the amount of flow that goes from <span><span class="math inline">\(u\)</span></span> to <span><span class="math inline">\(v\)</span></span>. (If the flow is in the other direction then we make <span><span class="math inline">\(x_e\)</span></span> negative.) Since every edge has capacity one, we know that <span><span class="math inline">\(-1 \leq x_e \leq 1\)</span></span> for every edge <span><span class="math inline">\(e\)</span></span>. A valid flow has the property that the amount of water leaving the source <span><span class="math inline">\(s\)</span></span> is the same as the amount entering the sink <span><span class="math inline">\(t\)</span></span>, and that for every other vertex <span><span class="math inline">\(v\)</span></span>, the amount of water entering and leaving <span><span class="math inline">\(v\)</span></span> is the same.</p>
<p>Mathematically, we can write these conditions as follows:</p>
<p><span>
<div class='myequationbox'><span class="math display">\[
\begin{aligned}
\sum_{e \ni s} x_e  + \sum_{e\ni t} x_e &amp;=0  &amp;&amp; \\
\sum_{e\ni v} x_e &amp;=0 \; &amp;&amp;\forall_{v \in V \setminus \{s,t\}} \\
-1 \leq x_e \leq 1 &amp;  \; &amp;&amp;\forall_{e\in E}
\end{aligned}
\;\;(11.1)
\]</span><a id='eqlinprogmincut'></a></div></span> where for every vertex <span><span class="math inline">\(v\)</span></span>, summing over <span><span class="math inline">\(e \ni v\)</span></span> means summing over all the edges that touch <span><span class="math inline">\(v\)</span></span>.</p>
<p>The maximum flow problem can be thought of as the task of maximizing <span><span class="math inline">\(\sum_{e \ni s} x_e\)</span></span> over all the vectors <span><span class="math inline">\(x\in\R^m\)</span></span> that satisfy the above conditions <a href='#eqlinprogmincut'>Equation 11.1</a>. Maximizing a linear function <span><span class="math inline">\(\ell(x)\)</span></span> over the set of <span><span class="math inline">\(x\in \R^m\)</span></span> that satisfy certain linear equalities and inequalities is known as <em>linear programming</em>. Luckily, there are <a href="https://en.wikipedia.org/wiki/Linear_programming#Algorithms">polynomial-time algorithms</a> for solving linear programming, and hence we can solve the maximum flow (and so, equivalently, minimum cut) problem in polynomial time. In fact, there are much better algorithms for maximum-flow/minimum-cut, even for weighted directed graphs, with currently the record standing at <span><span class="math inline">\(O(\min\{ m^{10/7}, m\sqrt{n}\})\)</span></span> time.</p>
<div id="globalmincut" class="solvedexercise" title="Global minimum cut" name="Solvedexercise 11.1 (Global minimum cut) ">
<p>Given a graph <span><span class="math inline">\(G=(V,E)\)</span></span>, define the <em>global</em> minimum cut of <span><span class="math inline">\(G\)</span></span> to be the minimum over all <span><span class="math inline">\(S \subseteq V\)</span></span> with <span><span class="math inline">\(S \neq \emptyset\)</span></span> and <span><span class="math inline">\(S \neq V\)</span></span> of the number of edges cut by <span><span class="math inline">\(S\)</span></span>. Prove that there is a polynomial-time algorithm to compute the global minimum cut of a graph.</p>
</div>
<div class="solution" data-ref="globalmincut" name="Solution 11.1.4">
<p>By the above we know that there is a polynomial-time algorithm <span><span class="math inline">\(A\)</span></span> that on input <span><span class="math inline">\((G,s,t)\)</span></span> finds the minimum <span><span class="math inline">\(s,t\)</span></span> cut in the graph <span><span class="math inline">\(G\)</span></span>. Using <span><span class="math inline">\(A\)</span></span>, we can obtain an algorithm <span><span class="math inline">\(B\)</span></span> that on input a graph <span><span class="math inline">\(G\)</span></span> computes the global minimum cut as follows:</p>
<ol type="1">
<li><p>For every distinct pair <span><span class="math inline">\(s,t \in V\)</span></span>, Algorithms <span><span class="math inline">\(B\)</span></span> sets <span><span class="math inline">\(k_{s,t}\leftarrow A(G,s,t)\)</span></span>.</p></li>
<li><p><span><span class="math inline">\(B\)</span></span> returns the minimum of <span><span class="math inline">\(k_{s,t}\)</span></span> over all distinct pairs <span><span class="math inline">\(s,t\)</span></span></p></li>
</ol>
<p>The running time of <span><span class="math inline">\(B\)</span></span> will be <span><span class="math inline">\(O(n^2)\)</span></span> times the running time of <span><span class="math inline">\(A\)</span></span> and hence polynomial time. Moreover, if the the global minimum cut is <span><span class="math inline">\(S\)</span></span>, then when <span><span class="math inline">\(B\)</span></span> reaches an iteration with <span><span class="math inline">\(s\in S\)</span></span> and <span><span class="math inline">\(t\not\in S\)</span></span> it will obtain the value of this cut, and hence the value output by <span><span class="math inline">\(B\)</span></span> will be the value of the global minimum cut.</p>
<p>The above is our first example of a <em>reduction</em> in the context of polynomial-time algorithms. Namely, we reduced the task of computing the global minimum cut to the task of computing minimum <span><span class="math inline">\(s,t\)</span></span> cuts.</p>
</div>
<h3 id="finding-the-maximum-cut-in-a-graph" data-number="11.1.5">Finding the maximum cut in a graph</h3>
<p>The <em>maximum cut</em> problem is the task of finding, given an input graph <span><span class="math inline">\(G=(V,E)\)</span></span>, the subset <span><span class="math inline">\(S\subseteq V\)</span></span> that <em>maximizes</em> the number of edges cut by <span><span class="math inline">\(S\)</span></span>. (We can also define an <span><span class="math inline">\(s,t\)</span></span>-cut variant of the maximum cut like we did for minimum cut; the two variants have similar complexity but the global maximum cut is more common in the literature.) Like its cousin the minimum cut problem, the maximum cut problem is also very well motivated. For example, maximum cut arises in VLSI design, and also has some surprising relation to analyzing the <a href="https://en.wikipedia.org/wiki/Ising_model">Ising model</a> in statistical physics.</p>
<p>Surprisingly, while (as we’ve seen) there is a polynomial-time algorithm for the minimum cut problem, there is no known algorithm solving <em>maximum cut</em> much faster than the trivial “brute force” algorithm that tries all <span><span class="math inline">\(2^n\)</span></span> possibilities for the set <span><span class="math inline">\(S\)</span></span>.</p>
<h3 id="a-note-on-convexity" data-number="11.1.6">A note on convexity</h3>
<figure>
<img src="../figure/convexvsnot.png" alt="11.4: In a convex function f (left figure), for every x and y and p\in [0,1] it holds that f(px+(1-p)y) \leq p\cdot f(x)+(1-p)\cdot f(y). In particular this means that every local minimum of f is also a global minimum. In contrast in a non convex function there can be many local minima." id="convexdeffig" class="margin" /><figcaption>11.4: In a <em>convex</em> function <span><span class="math inline">\(f\)</span></span> (left figure), for every <span><span class="math inline">\(x\)</span></span> and <span><span class="math inline">\(y\)</span></span> and <span><span class="math inline">\(p\in [0,1]\)</span></span> it holds that <span><span class="math inline">\(f(px+(1-p)y) \leq p\cdot f(x)+(1-p)\cdot f(y)\)</span></span>. In particular this means that every <em>local minimum</em> of <span><span class="math inline">\(f\)</span></span> is also a <em>global minimum</em>. In contrast in a <em>non convex</em> function there can be many local minima.</figcaption>
</figure>
<figure>
<img src="../figure/convexandnon.jpg" alt="11.5: In the high dimensional case, if f is a convex function (left figure) the global minimum is the only local minimum, and we can find it by a local-search algorithm which can be thought of as dropping a marble and letting it “slide down” until it reaches the global minimum. In contrast, a non-convex function (right figure) might have an exponential number of local minima in which any local-search algorithm could get stuck." id="convexfunctionfig" class="margin" /><figcaption>11.5: In the high dimensional case, if <span><span class="math inline">\(f\)</span></span> is a <em>convex</em> function (left figure) the global minimum is the only local minimum, and we can find it by a local-search algorithm which can be thought of as dropping a marble and letting it “slide down” until it reaches the global minimum. In contrast, a non-convex function (right figure) might have an exponential number of local minima in which any local-search algorithm could get stuck.</figcaption>
</figure>
<p>There is an underlying reason for the sometimes radical difference between the difficulty of maximizing and minimizing a function over a domain. If <span><span class="math inline">\(D \subseteq \R^n\)</span></span>, then a function <span><span class="math inline">\(f:D \rightarrow R\)</span></span> is <em>convex</em> if for every <span><span class="math inline">\(x,y \in D\)</span></span> and <span><span class="math inline">\(p\in [0,1]\)</span></span> <span><span class="math inline">\(f(px+(1-p)y) \leq pf(x) + (1-p)f(y)\)</span></span>. That is, <span><span class="math inline">\(f\)</span></span> applied to the <span><span class="math inline">\(p\)</span></span>-weighted midpoint between <span><span class="math inline">\(x\)</span></span> and <span><span class="math inline">\(y\)</span></span> is smaller than the <span><span class="math inline">\(p\)</span></span>-weighted average value of <span><span class="math inline">\(f\)</span></span>. If <span><span class="math inline">\(D\)</span></span> itself is convex (which means that if <span><span class="math inline">\(x,y\)</span></span> are in <span><span class="math inline">\(D\)</span></span> then so is the line segment between them), then this means that if <span><span class="math inline">\(x\)</span></span> is a <em>local minimum</em> of <span><span class="math inline">\(f\)</span></span> then it is also a <em>global minimum</em>. The reason is that if <span><span class="math inline">\(f(y)&lt;f(x)\)</span></span> then every point <span><span class="math inline">\(z=px+(1-p)y\)</span></span> on the line segment between <span><span class="math inline">\(x\)</span></span> and <span><span class="math inline">\(y\)</span></span> will satisfy <span><span class="math inline">\(f(z) \leq p f(x) + (1-p)f(y) &lt; f(x)\)</span></span> and hence in particular <span><span class="math inline">\(x\)</span></span> cannot be a local minimum. Intuitively, local minima of functions are much easier to find than global ones: after all, any “local search” algorithm that keeps finding a nearby point on which the value is lower, will eventually arrive at a local minima. One example of such a local search algorithm is <a href="https://en.wikipedia.org/wiki/Gradient_descent">gradient descent</a> which takes a sequence of small steps, each one in the direction that would reduce the value by the most amount based on the current derivative.</p>
<p>Indeed, under certain technical conditions, we can often efficiently find the minimum of convex functions over a convex domain, and this is the reason why problems such as minimum cut and shortest path are easy to solve. On the other hand, <em>maximizing</em> a convex function over a convex domain (or equivalently, minimizing a <em>concave</em> function) can often be a hard computational task. A <em>linear</em> function is both convex and concave, which is the reason that both the maximization and minimization problems for linear functions can be done efficiently.</p>
<p>The minimum cut problem is not a priori a convex minimization task, because the set of potential cuts is <em>discrete</em> and not continuous. However, it turns out that we can embed it in a continuous and convex set via the (linear) maximum flow problem. The “max flow min cut” theorem ensuring that this embedding is “tight” in the sense that the minimum “fractional cut” that we obtain through the maximum-flow linear program will be the same as the true minimum cut. Unfortunately, we don’t know of such a tight embedding in the setting of the <em>maximum</em> cut problem.</p>
<p>Convexity arises time and again in the context of efficient computation. For example, one of the basic tasks in machine learning is <em>empirical risk minimization</em>. This is the task of finding a classifier for a given set of <em>training examples</em>. That is, the input is a list of labeled examples <span><span class="math inline">\((x_{m-1},y_{m-1}),\ldots,(x_{m-1},y_{m-1})\)</span></span>, where each <span><span class="math inline">\(x_i \in \{0,1\}^n\)</span></span> and <span><span class="math inline">\(y_i \in \{0,1\}\)</span></span>, and the goal is to find a <em>classifier</em> <span><span class="math inline">\(h:\{0,1\}^n \rightarrow \{0,1\}\)</span></span> (or sometimes <span><span class="math inline">\(h:\{0,1\}^n \rightarrow \R\)</span></span>) that minimizes the number of <em>errors</em>. More generally, we want to find <span><span class="math inline">\(h\)</span></span> that minimizes <span>
<div class='myequationbox'><span class="math display">\[
\sum_{i=0}^{m-1}L(y_i,h(x_i))
\]</span></div></span> where <span><span class="math inline">\(L\)</span></span> is some <em>loss function</em> measuring how far is the predicted label <span><span class="math inline">\(h(x_i)\)</span></span> from the true label <span><span class="math inline">\(y_i\)</span></span>. When <span><span class="math inline">\(L\)</span></span> is the <em>square loss</em> function <span><span class="math inline">\(L(y,y&#39;)=(y-y&#39;)^2\)</span></span> and <span><span class="math inline">\(h\)</span></span> is a <em>linear function</em>, empirical risk minimization corresponds to the well-known convex minimization task of <em>linear regression</em>. In other cases, when the task is <em>non convex</em>, there can be many global or local minima. That said, even if we don’t find the global (or even a local) minima, this continuous embedding can still help us. In particular, when running a local improvement algorithm such as Gradient Descent, we might still find a function <span><span class="math inline">\(h\)</span></span> that is “useful” in the sense of having a small error on future examples from the same distribution.</p>
<h2 id="beyond-graphs" data-number="11.2">Beyond graphs</h2>
<p>Not all computational problems arise from graphs. We now list some other examples of computational problems that are of great interest.</p>
<h3 id="sat" data-number="11.2.1">SAT</h3>
<p>A <em>propositional formula</em> <span><span class="math inline">\(\varphi\)</span></span> involves <span><span class="math inline">\(n\)</span></span> variables <span><span class="math inline">\(x_1,\ldots,x_n\)</span></span> and the logical operators AND (<span><span class="math inline">\(\wedge\)</span></span>), OR (<span><span class="math inline">\(\vee\)</span></span>), and NOT (<span><span class="math inline">\(\neg\)</span></span>, also denoted as <span><span class="math inline">\(\overline{\cdot}\)</span></span>). We say that such a formula is in <em>conjunctive normal form</em> (CNF for short) if it is an AND of ORs of variables or their negations (we call a term of the form <span><span class="math inline">\(x_i\)</span></span> or <span><span class="math inline">\(\overline{x}_i\)</span></span> a <em>literal</em>). For example, this is a CNF formula <span>
<div class='myequationbox'><span class="math display">\[
(x_7 \vee \overline{x}_{22} \vee x_{15} ) \wedge (x_{37} \vee x_{22}) \wedge (x_{55} \vee \overline{x}_7)
\]</span></div></span></p>
<p>The <em>satisfiability problem</em> is the task of determining, given a CNF formula <span><span class="math inline">\(\varphi\)</span></span>, whether or not there exists a <em>satisfying assignment</em> for <span><span class="math inline">\(\varphi\)</span></span>. A satisfying assignment for <span><span class="math inline">\(\varphi\)</span></span> is a string <span><span class="math inline">\(x\in \{0,1\}^n\)</span></span> such that if <span><span class="math inline">\(\varphi\)</span></span> evaluates to <em>True</em> if we assign its variables the values of <span><span class="math inline">\(x\)</span></span>. The SAT problem might seem as an abstract question of interest only in logic but in fact SAT is of huge interest in industrial optimization, with applications including manufacturing planning, circuit synthesis, software verification, air-traffic control, scheduling sports tournaments, and more.</p>
<p><strong>2SAT.</strong> We say that a formula is a <span><span class="math inline">\(k\)</span></span>-CNF it is an AND of ORs where each OR involves exactly <span><span class="math inline">\(k\)</span></span> literals. The <span><span class="math inline">\(k\)</span></span>-SAT problem is the restriction of the satisfiability problem for the case that the input formula is a <span><span class="math inline">\(k\)</span></span>-CNF. In particular, the <em>2SAT problem</em> is to find out, given a <span><span class="math inline">\(2\)</span></span>-CNF formula <span><span class="math inline">\(\varphi\)</span></span>, whether there is an assignment <span><span class="math inline">\(x\in \{0,1\}^n\)</span></span> that <em>satisfies</em> <span><span class="math inline">\(\varphi\)</span></span>, in the sense that it makes it evaluate to <span><span class="math inline">\(1\)</span></span> or “True”. The trivial, brute-force, algorithm for 2SAT will enumerate all the <span><span class="math inline">\(2^n\)</span></span> assignments <span><span class="math inline">\(x\in \{0,1\}^n\)</span></span> but fortunately we can do much better. The key is that we can think of every constraint of the form <span><span class="math inline">\(\ell_i \vee \ell_j\)</span></span> (where <span><span class="math inline">\(\ell_i,\ell_j\)</span></span> are <em>literals</em>, corresponding to variables or their negations) as an <em>implication</em> <span><span class="math inline">\(\overline{\ell}_i \Rightarrow \ell_j\)</span></span>, since it corresponds to the constraints that if the literal <span><span class="math inline">\(\ell&#39;_i = \overline{\ell}_i\)</span></span> is true then it must be the case that <span><span class="math inline">\(\ell_j\)</span></span> is true as well. Hence we can think of <span><span class="math inline">\(\varphi\)</span></span> as a directed graph between the <span><span class="math inline">\(2n\)</span></span> literals, with an edge from <span><span class="math inline">\(\ell_i\)</span></span> to <span><span class="math inline">\(\ell_j\)</span></span> corresponding to an implication from the former to the latter. It can be shown that <span><span class="math inline">\(\varphi\)</span></span> is unsatisfiable if and only if there is a variable <span><span class="math inline">\(x_i\)</span></span> such that there is a directed path from <span><span class="math inline">\(x_i\)</span></span> to <span><span class="math inline">\(\overline{x}_i\)</span></span> as well as a directed path from <span><span class="math inline">\(\overline{x}_i\)</span></span> to <span><span class="math inline">\(x_i\)</span></span> (see <a href='#twosat_ex'>Exercise 11.2</a>). This reduces 2SAT to the (efficiently solvable) problem of determining connectivity in directed graphs.</p>
<p><strong>3SAT.</strong> The 3SAT problem is the task of determining satisfiability for 3CNFs. One might think that changing from two to three would not make that much of a difference for complexity. One would be wrong. Despite much effort, we do not know of a significantly better than brute force algorithm for 3SAT (the best known algorithms take roughly <span><span class="math inline">\(1.3^n\)</span></span> steps).</p>
<p>Interestingly, a similar issue arises time and again in computation, where the difference between two and three often corresponds to the difference between tractable and intractable. We do not fully understand the reasons for this phenomenon, though the notions of <span><span class="math inline">\(\mathbf{NP}\)</span></span> completeness we will see later does offer a partial explanation. It may be related to the fact that optimizing a polynomial often amounts to equations on its derivative. The derivative of a quadratic polynomial is linear, while the derivative of a cubic is quadratic, and, as we will see, the difference between solving linear and quadratic equations can be quite profound.</p>
<h3 id="solving-linear-equations" data-number="11.2.2">Solving linear equations</h3>
<p>One of the most useful problems that people have been solving time and again is solving <span><span class="math inline">\(n\)</span></span> linear equations in <span><span class="math inline">\(n\)</span></span> variables. That is, solve equations of the form</p>
<p><span>
<div class='myequationbox'><span class="math display">\[
\begin{aligned}
a_{0,0}x_0 &amp;+ a_{0,1}x_1 &amp;&amp;+ \cdots &amp;&amp;+ a_{0,{n-1}}x_{n-1} &amp;&amp;= b_0 \\
a_{1,0}x_0 &amp;+ a_{1,1}x_1 &amp;&amp;+ \cdots &amp;&amp;+ a_{1,{n-1}}x_{n-1} &amp;&amp;= b_1 \\
\vdots     &amp;+ \vdots     &amp;&amp;+  \vdots &amp;&amp;+ \vdots              &amp;&amp;= \vdots \\
a_{n-1,0}x_0 &amp;+ a_{n-1,1}x_1 &amp;&amp;+ \cdots &amp;&amp;+ a_{n-1,{n-1}}x_{n-1} &amp;&amp;= b_{n-1}
\end{aligned}
\]</span></div></span></p>
<p>where <span><span class="math inline">\(\{ a_{i,j} \}_{i,j \in [n]}\)</span></span> and <span><span class="math inline">\(\{ b_i \}_{i\in [n]}\)</span></span> are real (or rational) numbers. More compactly, we can write this as the equations <span><span class="math inline">\(Ax = b\)</span></span> where <span><span class="math inline">\(A\)</span></span> is an <span><span class="math inline">\(n\times n\)</span></span> matrix, and we think of <span><span class="math inline">\(x,b\)</span></span> are column vectors in <span><span class="math inline">\(\R^n\)</span></span>.</p>
<p>The standard <a href="https://en.wikipedia.org/wiki/Gaussian_elimination">Gaussian elimination</a> algorithm can be used to solve such equations in polynomial time (i.e., determine if they have a solution, and if so, to find it). As we discussed above, if we are willing to allow some loss in precision, we even have algorithms that handle linear <em>inequalities</em>, also known as linear programming. In contrast, if we insist on <em>integer</em> solutions, the task of solving for linear equalities or inequalities is known as <a href="https://en.wikipedia.org/wiki/Integer_programming">integer programming</a>, and the best known algorithms are exponential time in the worst case.</p>
<div id="numbersbits" class="remark" title="Bit complexity of numbers" name="Remark 11.4 (Bit complexity of numbers) ">
<p>Whenever we discuss problems whose inputs correspond to numbers, the input length corresponds to how many bits are needed to describe the number (or, as is equivalent up to a constant factor, the number of digits in base 10, 16 or any other constant). The difference between the length of the input and the magnitude of the number itself can be of course quite profound. For example, most people would agree that there is a huge difference between having a billion (i.e. <span><span class="math inline">\(10^9\)</span></span>) dollars and having nine dollars. Similarly there is a huge difference between an algorithm that takes <span><span class="math inline">\(n\)</span></span> steps on an <span><span class="math inline">\(n\)</span></span>-bit number and an algorithm that takes <span><span class="math inline">\(2^n\)</span></span> steps.</p>
<p>One example is the problem (discussed below) of finding the prime factors of a given integer <span><span class="math inline">\(N\)</span></span>. The natural algorithm is to search for such a factor by trying all numbers from <span><span class="math inline">\(1\)</span></span> to <span><span class="math inline">\(N\)</span></span>, but that would take <span><span class="math inline">\(N\)</span></span> steps which is <em>exponential</em> in the input length, which is the number of bits needed to describe <span><span class="math inline">\(N\)</span></span>. (The running time of this algorithm can be easily improved to roughly <span><span class="math inline">\(\sqrt{N}\)</span></span>, but this is still exponential (i.e., <span><span class="math inline">\(2^{n/2}\)</span></span>) in the number <span><span class="math inline">\(n\)</span></span> of bits to describe <span><span class="math inline">\(N\)</span></span>.) It is an important and long open question whether there is such an algorithm that runs in time polynomial in the input length (i.e., polynomial in <span><span class="math inline">\(\log N\)</span></span>).</p>
</div>
<h3 id="solving-quadratic-equations" data-number="11.2.3">Solving quadratic equations</h3>
<p>Suppose that we want to solve not just <em>linear</em> but also equations involving <em>quadratic</em> terms of the form <span><span class="math inline">\(a_{i,j,k}x_jx_k\)</span></span>. That is, suppose that we are given a set of quadratic polynomials <span><span class="math inline">\(p_1,\ldots,p_m\)</span></span> and consider the equations <span><span class="math inline">\(\{ p_i(x) = 0 \}\)</span></span>. To avoid issues with bit representations, we will always assume that the equations contain the constraints <span><span class="math inline">\(\{ x_i^2 - x_i = 0 \}_{i\in [n]}\)</span></span>. Since only <span><span class="math inline">\(0\)</span></span> and <span><span class="math inline">\(1\)</span></span> satisfy the equation <span><span class="math inline">\(a^2-a\)</span></span>, this assumption means that we can restrict attention to solutions in <span><span class="math inline">\(\{0,1\}^n\)</span></span>. Solving quadratic equations in several variables is a classical and extremely well motivated problem. This is the generalization of the classical case of single-variable quadratic equations that generations of high school students grapple with. It also generalizes the <a href="https://www.opt.math.tugraz.at/~cela/papers/qap_bericht.pdf">quadratic assignment problem</a>, introduced in the 1950’s as a way to optimize assignment of economic activities. Once again, we do not know a much better algorithm for this problem than the one that enumerates over all the <span><span class="math inline">\(2^n\)</span></span> possibilities.</p>
<h2 id="more-advanced-examples" data-number="11.3">More advanced examples</h2>
<p>We now list a few more examples of interesting problems that are a little more advanced but are of significant interest in areas such as physics, economics, number theory, and cryptography.</p>
<h3 id="determinant-of-a-matrix" data-number="11.3.1">Determinant of a matrix</h3>
<p>The <a href="https://en.wikipedia.org/wiki/Determinant">determinant</a> of a <span><span class="math inline">\(n\times n\)</span></span> matrix <span><span class="math inline">\(A\)</span></span>, denoted by <span><span class="math inline">\(\mathrm{det}(A)\)</span></span>, is an extremely important quantity in linear algebra. For example, it is known that <span><span class="math inline">\(\mathrm{det}(A) \neq 0\)</span></span> if and only if <span><span class="math inline">\(A\)</span></span> is <em>nonsingular</em>, which means that it has an inverse <span><span class="math inline">\(A^{-1}\)</span></span>, and hence we can always uniquely solve equations of the form <span><span class="math inline">\(Ax = b\)</span></span> where <span><span class="math inline">\(x\)</span></span> and <span><span class="math inline">\(b\)</span></span> are <span><span class="math inline">\(n\)</span></span>-dimensional vectors. More generally, the determinant can be thought of as a quantitative measure as to what extent <span><span class="math inline">\(A\)</span></span> is far from being singular. If the rows of <span><span class="math inline">\(A\)</span></span> are “almost” linearly dependent (for example, if the third row is very close to being a linear combination of the first two rows) then the determinant will be small, while if they are far from it (for example, if they are are <em>orthogonal</em> to one another, then the determinant will be large). In particular, for every matrix <span><span class="math inline">\(A\)</span></span>, the absolute value of the determinant of <span><span class="math inline">\(A\)</span></span> is at most the product of the norms (i.e., square root of sum of squares of entries) of the rows, with equality if and only if the rows are orthogonal to one another.</p>
<p>The determinant can be defined in several ways. One way to define the determinant of an <span><span class="math inline">\(n\times n\)</span></span> matrix <span><span class="math inline">\(A\)</span></span> is:</p>
<p><span>
<div class='myequationbox'><span class="math display">\[
\mathrm{det}(A) = \sum_{\pi \in S_n} \mathrm{sign}(\pi)\prod_{i\in [n]}A_{i,\pi(i)} \;\;(11.5)
\]</span><a id='determinanteq'></a></div></span> where <span><span class="math inline">\(S_n\)</span></span> is the set of all permutations from <span><span class="math inline">\([n]\)</span></span> to <span><span class="math inline">\([n]\)</span></span> and the <a href="https://en.wikipedia.org/wiki/Parity_of_a_permutation">sign of a permutation</a> <span><span class="math inline">\(\pi\)</span></span> is equal to <span><span class="math inline">\(-1\)</span></span> raised to the power of the number of <em>inversions</em> in <span><span class="math inline">\(\pi\)</span></span> (pairs <span><span class="math inline">\(i,j\)</span></span> such that <span><span class="math inline">\(i&gt;j\)</span></span> but <span><span class="math inline">\(\pi(i)&lt;\pi(j)\)</span></span>).</p>
<p>This definition suggests that computing <span><span class="math inline">\(\mathrm{det}(A)\)</span></span> might require summing over <span><span class="math inline">\(|S_n|\)</span></span> terms which would take exponential time since <span><span class="math inline">\(|S_n| = n! &gt; 2^n\)</span></span>. However, there are other ways to compute the determinant. For example, it is known that <span><span class="math inline">\(\mathrm{det}\)</span></span> is the only function that satisfies the following conditions:</p>
<ol type="1">
<li><p><span><span class="math inline">\(\mathrm{det}(\ensuremath{\mathit{AB}}) = \mathrm{det}(A)\mathrm{det}(B)\)</span></span> for every square matrices <span><span class="math inline">\(A,B\)</span></span>.</p></li>
<li><p>For every <span><span class="math inline">\(n\times n\)</span></span> <em>triangular</em> matrix <span><span class="math inline">\(T\)</span></span> with diagonal entries <span><span class="math inline">\(d_0,\ldots, d_{n-1}\)</span></span>, <span><span class="math inline">\(\mathrm{det}(T)=\prod_{i=0}^n d_i\)</span></span>. In particular <span><span class="math inline">\(\mathrm{det}(I)=1\)</span></span> where <span><span class="math inline">\(I\)</span></span> is the identity matrix. (A <em>triangular</em> matrix is one in which either all entries below the diagonal, or all entries above the diagonal, are zero.)</p></li>
<li><p><span><span class="math inline">\(\mathrm{det}(S)=-1\)</span></span> where <span><span class="math inline">\(S\)</span></span> is a “swap matrix” that corresponds to swapping two rows or two columns of <span><span class="math inline">\(I\)</span></span>. That is, there are two coordinates <span><span class="math inline">\(a,b\)</span></span> such that for every <span><span class="math inline">\(i,j\)</span></span>, <span><span class="math inline">\(S_{i,j} = \begin{cases}1 &amp; i=j\;, i \not\in \{a,b \} \\ 1 &amp; \{i,j\}=\{a,b\} \\ 0 &amp; \text{otherwise}\end{cases}\)</span></span>.</p></li>
</ol>
<p>Using these rules and the <a href="https://en.wikipedia.org/wiki/Gaussian_elimination">Gaussian elimination</a> algorithm, it is possible to tell whether <span><span class="math inline">\(A\)</span></span> is singular or not, and in the latter case, decompose <span><span class="math inline">\(A\)</span></span> as a product of a polynomial number of swap matrices and triangular matrices. (Indeed one can verify that the row operations in Gaussian elimination corresponds to either multiplying by a swap matrix or by a triangular matrix.) Hence we can compute the determinant for an <span><span class="math inline">\(n\times n\)</span></span> matrix using a polynomial time of arithmetic operations.</p>
<h3 id="permanent-of-a-matrix" data-number="11.3.2">Permanent of a matrix</h3>
<p>Given an <span><span class="math inline">\(n\times n\)</span></span> matrix <span><span class="math inline">\(A\)</span></span>, the <em>permanent</em> of <span><span class="math inline">\(A\)</span></span> is defined as</p>
<p><span>
<div class='myequationbox'><span class="math display">\[
\mathrm{perm}(A) = \sum_{\pi \in S_n} \prod_{i\in [n]}A_{i,\pi(i)} \;. \;\;(11.6) 
\]</span><a id='permanenteq'></a></div></span> That is, <span><span class="math inline">\(\mathrm{perm}(A)\)</span></span> is defined analogously to the determinant in <a href='#determinanteq'>Equation 11.5</a> except that we drop the term <span><span class="math inline">\(\mathrm{sign}(\pi)\)</span></span>. The permanent of a matrix is a natural quantity, and has been studied in several contexts including combinatorics and graph theory. It also arises in physics where it can be used to describe the quantum state of multiple Boson particles (see <a href="http://www.cs.huji.ac.il/labs/learning/Papers/perm.pdf">here</a> and <a href="https://en.wikipedia.org/wiki/Boson_sampling">here</a>).</p>
<p><strong>Permanent modulo 2.</strong> If the entries of <span><span class="math inline">\(A\)</span></span> are integers, then we can define the <em>Boolean</em> function <span><span class="math inline">\(perm_2\)</span></span> which outputs on input a matrix <span><span class="math inline">\(A\)</span></span> the result of the permanent of <span><span class="math inline">\(A\)</span></span> modulo <span><span class="math inline">\(2\)</span></span>. It turns out that we can compute <span><span class="math inline">\(perm_2(A)\)</span></span> in polynomial time. The key is that modulo <span><span class="math inline">\(2\)</span></span>, <span><span class="math inline">\(-x\)</span></span> and <span><span class="math inline">\(+x\)</span></span> are the same quantity and hence, since the only difference between <a href='#determinanteq'>Equation 11.5</a> and <a href='#permanenteq'>Equation 11.6</a> is that some terms are multiplied by <span><span class="math inline">\(-1\)</span></span>, <span><span class="math inline">\(\mathrm{det}(A) \mod 2 = \mathrm{perm}(A) \mod 2\)</span></span> for every <span><span class="math inline">\(A\)</span></span>.</p>
<p><strong>Permanent modulo 3.</strong> Emboldened by our good fortune above, we might hope to be able to compute the permanent modulo any prime <span><span class="math inline">\(p\)</span></span> and perhaps in full generality. Alas, we have no such luck. In a similar “two to three” type of a phenomenon, we do not know of a much better than brute force algorithm to even compute the permanent modulo <span><span class="math inline">\(3\)</span></span>.</p>
<h3 id="finding-a-zero-sum-equilibrium" data-number="11.3.3">Finding a zero-sum equilibrium</h3>
<p>A <em>zero sum game</em> is a game between two players where the payoff for one is the same as the penalty for the other. That is, whatever the first player gains, the second player loses. As much as we want to avoid them, zero sum games do arise in life, and the one good thing about them is that at least we can compute the optimal strategy.</p>
<p>A zero sum game can be specified by an <span><span class="math inline">\(n\times n\)</span></span> matrix <span><span class="math inline">\(A\)</span></span>, where if player 1 chooses action <span><span class="math inline">\(i\)</span></span> and player 2 chooses action <span><span class="math inline">\(j\)</span></span> then player one gets <span><span class="math inline">\(A_{i,j}\)</span></span> and player 2 loses the same amount. The famous <a href="https://en.wikipedia.org/wiki/Min-max_theorem">Min Max Theorem</a> by John von Neumann states that if we allow probabilistic or “mixed” strategies (where a player does not choose a single action but rather a <em>distribution</em> over actions) then it does not matter who plays first and the end result will be the same. Mathematically the min max theorem is that if we let <span><span class="math inline">\(\Delta_n\)</span></span> be the set of probability distributions over <span><span class="math inline">\([n]\)</span></span> (i.e., non-negative columns vectors in <span><span class="math inline">\(\R^n\)</span></span> whose entries sum to <span><span class="math inline">\(1\)</span></span>) then</p>
<p><span>
<div class='myequationbox'><span class="math display">\[
\max_{p \in \Delta_n} \min_{q\in \Delta_n} p^\top A q =  \min_{q \in \Delta_n} \max_{p\in \Delta_n} p^\top A q \;\;(11.7)
\]</span><a id='eq:minmax'></a></div></span></p>
<p>The min-max theorem turns out to be a corollary of linear programming duality, and indeed the value of <a href='#eq:minmax'>Equation 11.7</a> can be computed efficiently by a linear program.</p>
<h3 id="finding-a-nash-equilibrium" data-number="11.3.4">Finding a Nash equilibrium</h3>
<p>Fortunately, not all real-world games are zero sum, and we do have more general games, where the payoff of one player does not necessarily equal the loss of the other. <a href="https://en.wikipedia.org/wiki/John_Forbes_Nash_Jr.">John Nash</a> won the Nobel prize for showing that there is a notion of <em>equilibrium</em> for such games as well. In many economic texts it is taken as an article of faith that when actual agents are involved in such a game then they reach a Nash equilibrium. However, unlike zero sum games, we do not know of an efficient algorithm for finding a Nash equilibrium given the description of a general (non zero sum) game. In particular this means that, despite economists’ intuitions, there are games for which natural strategies will take an exponential number of steps to converge to an equilibrium.</p>
<h3 id="primality-testing" data-number="11.3.5">Primality testing</h3>
<p>Another classical computational problem, that has been of interest since the ancient Greeks, is to determine whether a given number <span><span class="math inline">\(N\)</span></span> is prime or composite. Clearly we can do so by trying to divide it with all the numbers in <span><span class="math inline">\(2,\ldots,N-1\)</span></span>, but this would take at least <span><span class="math inline">\(N\)</span></span> steps which is <em>exponential</em> in its bit complexity <span><span class="math inline">\(n = \log N\)</span></span>. We can reduce these <span><span class="math inline">\(N\)</span></span> steps to <span><span class="math inline">\(\sqrt{N}\)</span></span> by observing that if <span><span class="math inline">\(N\)</span></span> is a composite of the form <span><span class="math inline">\(N=\ensuremath{\mathit{PQ}}\)</span></span> then either <span><span class="math inline">\(P\)</span></span> or <span><span class="math inline">\(Q\)</span></span> is smaller than <span><span class="math inline">\(\sqrt{N}\)</span></span>. But this is still quite terrible. If <span><span class="math inline">\(N\)</span></span> is a <span><span class="math inline">\(1024\)</span></span> bit integer, <span><span class="math inline">\(\sqrt{N}\)</span></span> is about <span><span class="math inline">\(2^{512}\)</span></span>, and so running this algorithm on such an input would take much more than the lifetime of the universe.</p>
<p>Luckily, it turns out we can do radically better. In the 1970’s, Rabin and Miller gave <em>probabilistic</em> algorithms to determine whether a given number <span><span class="math inline">\(N\)</span></span> is prime or composite in time <span><span class="math inline">\(poly(n)\)</span></span> for <span><span class="math inline">\(n=\log N\)</span></span>. We will discuss the probabilistic model of computation later in this course. In 2002, Agrawal, Kayal, and Saxena found a deterministic <span><span class="math inline">\(poly(n)\)</span></span> time algorithm for this problem. This is surely a development that mathematicians from Archimedes till Gauss would have found exciting.</p>
<h3 id="integer-factoring" data-number="11.3.6">Integer factoring</h3>
<p>Given that we can efficiently determine whether a number <span><span class="math inline">\(N\)</span></span> is prime or composite, we could expect that in the latter case we could also efficiently <em>find</em> the factorization of <span><span class="math inline">\(N\)</span></span>. Alas, no such algorithm is known. In a surprising and exciting turn of events, the <em>non existence</em> of such an algorithm has been used as a basis for encryptions, and indeed it underlies much of the security of the world wide web. We will return to the factoring problem later in this course. We remark that we do know much better than brute force algorithms for this problem. While the brute force algorithms would require <span><span class="math inline">\(2^{\Omega(n)}\)</span></span> time to factor an <span><span class="math inline">\(n\)</span></span>-bit integer, there are known algorithms running in time roughly <span><span class="math inline">\(2^{O(\sqrt{n})}\)</span></span> and also algorithms that are widely believed (though not fully rigorously analyzed) to run in time roughly <span><span class="math inline">\(2^{O(n^{1/3})}\)</span></span>. (By “roughly” we mean that we neglect factors that are polylogarithmic in <span><span class="math inline">\(n\)</span></span>.)</p>
<h2 id="our-current-knowledge" data-number="11.4">Our current knowledge</h2>
<figure>
<img src="../figure/poly_vs_exp.png" alt="11.6: The current computational status of several interesting problems. For all of them we either know a polynomial-time algorithm or the known algorithms require at least 2^{n^c} for some c&gt;0. In fact for all except the factoring problem, we either know an O(n^3) time algorithm or the best known algorithm require at least 2^{\Omega(n)} time where n is a natural parameter such that there is a brute force algorithm taking roughly 2^n or n! time. Whether this “cliff” between the easy and hard problem is a real phenomenon or a reflection of our ignorance is still an open question." id="current_status" class="margin" /><figcaption>11.6: The current computational status of several interesting problems. For all of them we either know a polynomial-time algorithm or the known algorithms require at least <span><span class="math inline">\(2^{n^c}\)</span></span> for some <span><span class="math inline">\(c&gt;0\)</span></span>. In fact for all except the <em>factoring</em> problem, we either know an <span><span class="math inline">\(O(n^3)\)</span></span> time algorithm or the best known algorithm require at least <span><span class="math inline">\(2^{\Omega(n)}\)</span></span> time where <span><span class="math inline">\(n\)</span></span> is a natural parameter such that there is a brute force algorithm taking roughly <span><span class="math inline">\(2^n\)</span></span> or <span><span class="math inline">\(n!\)</span></span> time. Whether this “cliff” between the easy and hard problem is a real phenomenon or a reflection of our ignorance is still an open question.</figcaption>
</figure>
<p>The difference between an exponential and polynomial time algorithm might seem merely “quantitative” but it is in fact extremely significant. As we’ve already seen, the brute force exponential time algorithm runs out of steam very very fast, and as Edmonds says, in practice there might not be much difference between a problem where the best algorithm is exponential and a problem that is not solvable at all. Thus the efficient algorithms we mentiond above are widely used and power many computer science applications. Moreover, a polynomial-time algorithm often arises out of significant insight to the problem at hand, whether it is the “max-flow min-cut” result, the solvability of the determinant, or the group theoretic structure that enables primality testing. Such insight can be useful regardless of its computational implications.</p>
<p>At the moment we do not know whether the “hard” problems are truly hard, or whether it is merely because we haven’t yet found the right algorithms for them. However, we will now see that there are problems that do <em>inherently require</em> exponential time. We just don’t know if any of the examples above fall into that category.</p>
<div class="recap" name="Recap 11.4">
<ul>
<li><p>There are many natural problems that have polynomial-time algorithms, and other natural problems that we’d love to solve, but for which the best known algorithms are exponential.</p></li>
<li><p>Often a polynomial time algorithm relies on discovering some hidden structure in the problem, or finding a surprising equivalent formulation for it.</p></li>
<li><p>There are many interesting problems where there is an <em>exponential gap</em> between the best known algorithm and the best algorithm that we can rule out. Closing this gap is one of the main open questions of theoretical computer science.</p></li>
</ul>
</div>
<h2 id="exercises" data-number="11.5">Exercises</h2>
<div id="longest-path-ex" class="exercise" title="exponential time algorithm for longest path" name="Exercise 11.1 (exponential time algorithm for longest path) ">
<p>The naive algorithm for computing the longest path in a given graph could take more than <span><span class="math inline">\(n!\)</span></span> steps. Give a <span><span class="math inline">\(poly(n)2^n\)</span></span> time algorithm for the longest path problem in <span><span class="math inline">\(n\)</span></span> vertex graphs.<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup></p>
</div>
<div id="twosat_ex" class="exercise" title="2SAT algorithm" name="Exercise 11.2 (2SAT algorithm) ">
<p>For every 2CNF <span><span class="math inline">\(\varphi\)</span></span>, define the graph <span><span class="math inline">\(G_\varphi\)</span></span> on <span><span class="math inline">\(2n\)</span></span> vertices corresponding to the literals <span><span class="math inline">\(x_1,\ldots,x_n,\overline{x}_1,\ldots,\overline{x}_n\)</span></span>, such that there is an edge <span><span class="math inline">\(\overrightarrow{\ell_i\; \ell_j}\)</span></span> iff the constraint <span><span class="math inline">\(\overline{\ell}_i \vee \ell_j\)</span></span> is in <span><span class="math inline">\(\varphi\)</span></span>. Prove that <span><span class="math inline">\(\varphi\)</span></span> is unsatisfiable if and only if there is some <span><span class="math inline">\(i\)</span></span> such that there is a path from <span><span class="math inline">\(x_i\)</span></span> to <span><span class="math inline">\(\overline{x}_i\)</span></span> and from <span><span class="math inline">\(\overline{x}_i\)</span></span> to <span><span class="math inline">\(x_i\)</span></span> in <span><span class="math inline">\(G_\varphi\)</span></span>. Show how to use this to solve 2SAT in polynomial time.</p>
</div>
<div id="reduceP" class="exercise" title="Reductions for showing algorithms" name="Exercise 11.3 (Reductions for showing algorithms) ">
<p>The following fact is true: there is a polynomial-time algorithm <span><span class="math inline">\(\ensuremath{\mathit{BIP}}\)</span></span> that on input a graph <span><span class="math inline">\(G=(V,E)\)</span></span> outputs <span><span class="math inline">\(1\)</span></span> if and only if the graph is <em>bipartite</em>: there is a partition of <span><span class="math inline">\(V\)</span></span> to disjoint parts <span><span class="math inline">\(S\)</span></span> and <span><span class="math inline">\(T\)</span></span> such that every edge <span><span class="math inline">\((u,v) \in E\)</span></span> satisfies either <span><span class="math inline">\(u\in S\)</span></span> and <span><span class="math inline">\(v\in T\)</span></span> or <span><span class="math inline">\(u\in T\)</span></span> and <span><span class="math inline">\(v\in S\)</span></span>. Use this fact to prove that there is a polynomial-time algorithm to compute that following function <span><span class="math inline">\(\ensuremath{\mathit{CLIQUEPARTITION}}\)</span></span> that on input a graph <span><span class="math inline">\(G=(V,E)\)</span></span> outputs <span><span class="math inline">\(1\)</span></span> if and only if there is a partition of <span><span class="math inline">\(V\)</span></span> the graph into two parts <span><span class="math inline">\(S\)</span></span> and <span><span class="math inline">\(T\)</span></span> such that both <span><span class="math inline">\(S\)</span></span> and <span><span class="math inline">\(T\)</span></span> are <em>cliques</em>: for every pair of distinct vertices <span><span class="math inline">\(u,v \in S\)</span></span>, the edge <span><span class="math inline">\((u,v)\)</span></span> is in <span><span class="math inline">\(E\)</span></span> and similarly for every pair of distinct vertices <span><span class="math inline">\(u,v \in T\)</span></span>, the edge <span><span class="math inline">\((u,v)\)</span></span> is in <span><span class="math inline">\(E\)</span></span>.</p>
</div>
<h2 id="effalgnotes" data-number="11.6">Bibliographical notes</h2>
<p>The classic undergraduate introduction to algorithms text is  (<a href="https://scholar.google.com/scholar?hl=en&q=Cormen,+Leiserson,+Rivest,+Stein+Introduction+to+algorithms" target="_blank">Cormen, Leiserson, Rivest, Stein, 2009</a>) . Two texts that are less “encyclopedic” are Kleinberg and Tardos  (<a href="https://scholar.google.com/scholar?hl=en&q=Kleinberg,+Tardos+Algorithm+design" target="_blank">Kleinberg, Tardos, 2006</a>) , and Dasgupta, Papadimitriou and Vazirani  (<a href="https://scholar.google.com/scholar?hl=en&q=Dasgupta,+Papadimitriou,+Vazirani+Algorithms" target="_blank">Dasgupta, Papadimitriou, Vazirani, 2008</a>) . <a href="http://jeffe.cs.illinois.edu/teaching/algorithms/">Jeff Erickson’s book</a> is an excellent algorithms text that is freely available online.</p>
<p>The origins of the minimum cut problem date to the Cold War. Specifically, Ford and Fulkerson discovered their max-flow/min-cut algorithm in 1955 as a way to find out the minimum amount of train tracks that would need to be blown up to disconnect Russia from the rest of Europe. See the survey  (<a href="https://scholar.google.com/scholar?hl=en&q=Schrijver+On+the+history+of+combinatorial+optimization+(till+1960)" target="_blank">Schrijver, 2005</a>)  for more.</p>
<p>Some algorithms for the longest path problem are given in  (<a href="https://scholar.google.com/scholar?hl=en&q=Williams+Finding+paths+of+length+k+in+$O^*(2^k)$+time" target="_blank">Williams, 2009</a>)  (<a href="https://scholar.google.com/scholar?hl=en&q=Bjorklund+Determinant+sums+for+undirected+hamiltonicity" target="_blank">Bjorklund, 2014</a>) .</p>
<h2 id="further-explorations" data-number="11.7">Further explorations</h2>
<p>Some topics related to this chapter that might be accessible to advanced students include: (to be completed)</p>
<div id="footnotediv" class="footnotes">
<ol>
<li class="footnote" id="fn:1"><p>
<div>
<p>A <em>queue</em> is a data structure for storing a list of elements in “First In First Out (FIFO)” order. Each “pop” operation removes an element from the queue in the order that they were “pushed” into it; see the <a href="https://goo.gl/HY9BJD">Wikipedia page</a>.</p>
</div>
<a href="#fnref:1" title="return to article"> ↩</a><p></li>
<li class="footnote" id="fn:2"><p>
<div>
<p><strong>Hint:</strong> Use dynamic programming to compute for every <span><span class="math inline">\(s,t \in [n]\)</span></span> and <span><span class="math inline">\(S \subseteq [n]\)</span></span> the value <span><span class="math inline">\(P(s,t,S)\)</span></span> which equals <span><span class="math inline">\(1\)</span></span> if there is a simple path from <span><span class="math inline">\(s\)</span></span> to <span><span class="math inline">\(t\)</span></span> that uses exactly the vertices in <span><span class="math inline">\(S\)</span></span>. Do this iteratively for <span><span class="math inline">\(S\)</span></span>’s of growing sizes.</p>
</div>
<a href="#fnref:2" title="return to article"> ↩</a><p></li>
</ol>
</div>
<!--bookdown:body:end-->


<!-- end of  actual content -->

<!-- start of comments -->


<a name="commentform"></a>
<h2 id="comments" class="nocount">Comments</h2>

<p>Comments are posted on the <a href="https://github.com/boazbk/tcs/issues">GitHub repository</a> using the <a href="https://utteranc.es">utteranc.es</a> app.
A GitHub login is required to comment.
If you don't want to authorize the app to post on your behalf, you can also comment directly on the <a href="https://github.com/boazbk/tcs/issues?q=Defining Computation+in%3Atitle">GitHub issue for this page</a>.


<p>


<script src="https://utteranc.es/client.js" 
repo="boazbk/tcs" 
issue-term="title" 
label="comments"
theme="github-light" 
crossorigin="anonymous" async>
  </script>


<!-- end of comments -->

<p>Compiled on 12/02/2019 19:02:15</p>

<p>Copyright 2019, Boaz Barak.


<a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/"><img alt="Creative Commons License"
    style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png" /></a><br />This work is
licensed under a <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/">Creative Commons
  Attribution-NonCommercial-NoDerivatives 4.0 International License</a>.

<p>Produced using <a href="https://pandoc.org/">pandoc</a> and <a href="http://scorreia.com/software/panflute/">panflute</a> with templates derived from <a href="https://www.gitbook.com/">gitbook</a> and <a href="https://bookdown.org/">bookdown</a>.</p>



</div>


            </section>

          </div>
        </div>
      </div>
<!--bookdown:link_prev-->
<!--bookdown:link_next-->



    </div>
  </div>
<!--bookdown:config-->
<script src="js/app.min.js"></script>
<script src="js/lunr.js"></script>
<script src="js/plugin-search.js"></script>
<script src="js/plugin-sharing.js"></script>
<script src="js/plugin-fontsettings.js"></script>
<script src="js/fullscreen.js"></script>
<script src="js/plugin-bookdown.js"></script>
<script src="js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"history": {
"link": null,
"text": null
},
"download": ["https://files.boazbarak.org/introtcs/lec_10_efficient_alg.pdf"],
"toc": {
"collapse": "section"
}
});
});
</script>


</body>

</html>
