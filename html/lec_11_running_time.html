<!DOCTYPE html>
<html  lang="en">

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Introduction to Theoretical Computer Science: Modeling running time</title>
  <meta name="description" content="Textbook on Theoretical Computer Science by Boaz Barak">

  <meta property="og:title" content="Introduction to Theoretical Computer Science: Modeling running time" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://introtcs.org/" />
  <meta property="og:image" content="icons/cover.png" />
  <meta property="og:description" content="Textbook on Theoretical Computer Science by Boaz Barak" />
  <meta name="github-repo" content="boazbk/tcs" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Introduction to Theoretical Computer Science" />
  <meta name="twitter:description" content="Textbook on Theoretical Computer Science by Boaz Barak" />
  <meta name="twitter:image" content="https://introtcs.org/icons/cover.png" />

<meta name="author" content="Boaz Barak">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <link rel="apple-touch-icon-precomposed" sizes="120x120" href="icons/apple-touch-icon-120x120.png">
  <link rel="shortcut icon" href="icons/favicon.ico" type="image/x-icon">

<!-- Boaz: resources -->

<!-- <script src="https://kit.fontawesome.com/ab08ce82a8.js"></script> -->

<link rel="stylesheet" src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.9.0/css/all.min.css">


<!-- KaTeX -->


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css"
  integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">

<!-- The loading of KaTeX is deferred to speed up page rendering -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js"
  integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>

<!-- To automatically render math in text elements, include the auto-render extension: -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js"
  integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
  onload='renderMathInElement(document.body, {  throwOnError: false, macros: { "\\N": "\\mathbb{N}", "\\R": "\\mathbb{R}", "\\Z": "\\mathbb{Z}","\\E": "\\mathbb{E}","\\val": "\\mathrm{val}", "\\label": "\\;\\;\\;\\;\\;\\;\\;\\;","\\floor": "\\lfloor #1 \\rfloor","\\ceil": "\\lceil #1 \\rceil", "\\ensuremath": "#1"}});'>
</script>




<!-- KaTeX -->
<!-- pseudocode -->
<link rel="stylesheet" href="css/pseudocode.css">
<!-- <script src="js/pseudocode.min.js"></script> -->


<!-- Gitbook resources -->

  <script src="js/jquery.min.js"></script>
  <link href="css/style.css" rel="stylesheet" />
  
  <link href="css/plugin-table.css" rel="stylesheet" />
  <link href="css/plugin-bookdown.css" rel="stylesheet" />
  <link href="css/plugin-highlight.css" rel="stylesheet" />
  <link href="css/plugin-search.css" rel="stylesheet" />
  <link href="css/plugin-fontsettings.css" rel="stylesheet" />
  <link href="css/moregitbook.css" rel="stylesheet" />

  <link href="css/resmisc.css" rel="stylesheet" />





<!-- Boaz: end resources -->



<!--bookdown:link_prev-->
<!--bookdown:link_next-->



<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<!-- bigfoot-->

<link href="css/bigfoot-default.css" rel="stylesheet" />
<script type="text/javascript" src="js/bigfoot.js"></script>

<script type="text/javascript">
    var bigfoot = jQuery.bigfoot(
        {
            deleteOnUnhover: false,
            preventPageScroll: false,
            hoverDelay: 250
        }
    );
</script>

<!-- end bigfoot -->


</head>

<body>



<!--bookdown:title:start-->
<!--bookdown:title:end-->


<!--bookdown:toc:start-->
  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">
<!--bookdown:toc2:start-->
<ul class="summary">
<li><a href="./">Introduction to Theoretical Computer Science</a></li>
<li class="divider"></li><li class="chapter" data-level="p" data-path="lec_00_0_preface.html"><a href="lec_00_0_preface.html"><i class="fa fa-check"></i><b>p</b> Preface</a><ul><li class="chapter" data-level="p.1" data-path="lec_00_0_preface.html"><a href="lec_00_0_preface.html#to-the-student"><i class="fa fa-check"></i><b>p.1</b> To the student</a><ul><li class="chapter" data-level="p.1.1" data-path="lec_00_0_preface.html"><a href="lec_00_0_preface.html#is-the-effort-worth-it"><i class="fa fa-check"></i><b>p.1.1</b> Is the effort worth it?</a></li></ul></li><li class="chapter" data-level="p.2" data-path="lec_00_0_preface.html"><a href="lec_00_0_preface.html#to-potential-instructors"><i class="fa fa-check"></i><b>p.2</b> To potential instructors</a></li><li class="chapter" data-level="p.3" data-path="lec_00_0_preface.html"><a href="lec_00_0_preface.html#acknowledgements"><i class="fa fa-check"></i><b>p.3</b> Acknowledgements</a></li></ul></li><li class="chapter" data-level="0" data-path="lec_01_introduction.html"><a href="lec_01_introduction.html"><i class="fa fa-check"></i><b>0</b> Introduction</a><ul><li class="chapter" data-level="0.1" data-path="lec_01_introduction.html"><a href="lec_01_introduction.html#integer-multiplication-an-example-of-an-algorithm"><i class="fa fa-check"></i><b>0.1</b> Integer multiplication: an example of an algorithm</a></li><li class="chapter" data-level="0.2" data-path="lec_01_introduction.html"><a href="lec_01_introduction.html#karatsubasec"><i class="fa fa-check"></i><b>0.2</b> Extended Example: A faster way to multiply (optional)</a></li><li class="chapter" data-level="0.3" data-path="lec_01_introduction.html"><a href="lec_01_introduction.html#algsbeyondarithmetic"><i class="fa fa-check"></i><b>0.3</b> Algorithms beyond arithmetic</a></li><li class="chapter" data-level="0.4" data-path="lec_01_introduction.html"><a href="lec_01_introduction.html#on-the-importance-of-negative-results."><i class="fa fa-check"></i><b>0.4</b> On the importance of negative results.</a></li><li class="chapter" data-level="0.5" data-path="lec_01_introduction.html"><a href="lec_01_introduction.html#roadmapsec"><i class="fa fa-check"></i><b>0.5</b> Roadmap to the rest of this book</a><ul><li class="chapter" data-level="0.5.1" data-path="lec_01_introduction.html"><a href="lec_01_introduction.html#dependencies-between-chapters"><i class="fa fa-check"></i><b>0.5.1</b> Dependencies between chapters</a></li></ul></li><li class="chapter" data-level="0.6" data-path="lec_01_introduction.html"><a href="lec_01_introduction.html#exercises"><i class="fa fa-check"></i><b>0.6</b> Exercises</a></li><li class="chapter" data-level="0.7" data-path="lec_01_introduction.html"><a href="lec_01_introduction.html#bnotesintrosec"><i class="fa fa-check"></i><b>0.7</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="1" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html"><i class="fa fa-check"></i><b>1</b> Mathematical Background</a><ul><li class="chapter" data-level="1.1" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#manualbackground"><i class="fa fa-check"></i><b>1.1</b> This chapter: a reader’s manual</a></li><li class="chapter" data-level="1.2" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#secmathoverview"><i class="fa fa-check"></i><b>1.2</b> A quick overview of mathematical prerequisites</a></li><li class="chapter" data-level="1.3" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#reading-mathematical-texts"><i class="fa fa-check"></i><b>1.3</b> Reading mathematical texts</a><ul><li class="chapter" data-level="1.3.1" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#definitions"><i class="fa fa-check"></i><b>1.3.1</b> Definitions</a></li><li class="chapter" data-level="1.3.2" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#assertions-theorems-lemmas-claims"><i class="fa fa-check"></i><b>1.3.2</b> Assertions: Theorems, lemmas, claims</a></li><li class="chapter" data-level="1.3.3" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#proofs"><i class="fa fa-check"></i><b>1.3.3</b> Proofs</a></li></ul></li><li class="chapter" data-level="1.4" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#basic-discrete-math-objects"><i class="fa fa-check"></i><b>1.4</b> Basic discrete math objects</a><ul><li class="chapter" data-level="1.4.1" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#sets"><i class="fa fa-check"></i><b>1.4.1</b> Sets</a></li><li class="chapter" data-level="1.4.2" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#specialsets"><i class="fa fa-check"></i><b>1.4.2</b> Special sets</a></li><li class="chapter" data-level="1.4.3" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#functionsec"><i class="fa fa-check"></i><b>1.4.3</b> Functions</a></li><li class="chapter" data-level="1.4.4" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#graphsec"><i class="fa fa-check"></i><b>1.4.4</b> Graphs</a></li><li class="chapter" data-level="1.4.5" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#secquantifiers"><i class="fa fa-check"></i><b>1.4.5</b> Logic operators and quantifiers</a></li><li class="chapter" data-level="1.4.6" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#secquantifierssums"><i class="fa fa-check"></i><b>1.4.6</b> Quantifiers for summations and products</a></li><li class="chapter" data-level="1.4.7" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#boundvarsec"><i class="fa fa-check"></i><b>1.4.7</b> Parsing formulas: bound and free variables</a></li><li class="chapter" data-level="1.4.8" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#secbigohnotation"><i class="fa fa-check"></i><b>1.4.8</b> Asymptotics and Big-O notation</a></li><li class="chapter" data-level="1.4.9" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#some-rules-of-thumb-for-big-o-notation"><i class="fa fa-check"></i><b>1.4.9</b> Some rules of thumb for Big-O notation</a></li></ul></li><li class="chapter" data-level="1.5" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#proofsbackgroundsec"><i class="fa fa-check"></i><b>1.5</b> Proofs</a><ul><li class="chapter" data-level="1.5.1" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#proofs-and-programs"><i class="fa fa-check"></i><b>1.5.1</b> Proofs and programs</a></li><li class="chapter" data-level="1.5.2" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#proof-writing-style"><i class="fa fa-check"></i><b>1.5.2</b> Proof writing style</a></li><li class="chapter" data-level="1.5.3" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#patterns-in-proofs"><i class="fa fa-check"></i><b>1.5.3</b> Patterns in proofs</a></li></ul></li><li class="chapter" data-level="1.6" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#topsortsec"><i class="fa fa-check"></i><b>1.6</b> Extended example: Topological Sorting</a><ul><li class="chapter" data-level="1.6.1" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#inductionsec"><i class="fa fa-check"></i><b>1.6.1</b> Mathematical induction</a></li><li class="chapter" data-level="1.6.2" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#proving-the-result-by-induction"><i class="fa fa-check"></i><b>1.6.2</b> Proving the result by induction</a></li><li class="chapter" data-level="1.6.3" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#minimality-and-uniqueness"><i class="fa fa-check"></i><b>1.6.3</b> Minimality and uniqueness</a></li></ul></li><li class="chapter" data-level="1.7" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#notationsec"><i class="fa fa-check"></i><b>1.7</b> This book: notation and conventions</a><ul><li class="chapter" data-level="1.7.1" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#conventionsec"><i class="fa fa-check"></i><b>1.7.1</b> Variable name conventions</a></li><li class="chapter" data-level="1.7.2" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#some-idioms"><i class="fa fa-check"></i><b>1.7.2</b> Some idioms</a></li></ul></li><li class="chapter" data-level="1.8" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#exercises"><i class="fa fa-check"></i><b>1.8</b> Exercises</a></li><li class="chapter" data-level="1.9" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#notesmathchap"><i class="fa fa-check"></i><b>1.9</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="2" data-path="lec_02_representation.html"><a href="lec_02_representation.html"><i class="fa fa-check"></i><b>2</b> Computation and Representation</a><ul><li class="chapter" data-level="2.1" data-path="lec_02_representation.html"><a href="lec_02_representation.html#defining-representations"><i class="fa fa-check"></i><b>2.1</b> Defining representations</a><ul><li class="chapter" data-level="2.1.1" data-path="lec_02_representation.html"><a href="lec_02_representation.html#representing-natural-numbers"><i class="fa fa-check"></i><b>2.1.1</b> Representing natural numbers</a></li><li class="chapter" data-level="2.1.2" data-path="lec_02_representation.html"><a href="lec_02_representation.html#meaning-of-representations-discussion"><i class="fa fa-check"></i><b>2.1.2</b> Meaning of representations (discussion)</a></li></ul></li><li class="chapter" data-level="2.2" data-path="lec_02_representation.html"><a href="lec_02_representation.html#representations-beyond-natural-numbers"><i class="fa fa-check"></i><b>2.2</b> Representations beyond natural numbers</a><ul><li class="chapter" data-level="2.2.1" data-path="lec_02_representation.html"><a href="lec_02_representation.html#repnegativeintegerssec"><i class="fa fa-check"></i><b>2.2.1</b> Representing (potentially negative) integers</a></li><li class="chapter" data-level="2.2.2" data-path="lec_02_representation.html"><a href="lec_02_representation.html#twoscomplement"><i class="fa fa-check"></i><b>2.2.2</b> Two’s complement representation (optional)</a></li><li class="chapter" data-level="2.2.3" data-path="lec_02_representation.html"><a href="lec_02_representation.html#rational-numbers-and-representing-pairs-of-strings"><i class="fa fa-check"></i><b>2.2.3</b> Rational numbers, and representing pairs of strings</a></li></ul></li><li class="chapter" data-level="2.3" data-path="lec_02_representation.html"><a href="lec_02_representation.html#representing-real-numbers"><i class="fa fa-check"></i><b>2.3</b> Representing real numbers</a><ul><li class="chapter" data-level="2.3.1" data-path="lec_02_representation.html"><a href="lec_02_representation.html#cantorsec"><i class="fa fa-check"></i><b>2.3.1</b> Can we represent reals exactly?</a></li></ul></li><li class="chapter" data-level="2.4" data-path="lec_02_representation.html"><a href="lec_02_representation.html#representing-objects-beyond-numbers"><i class="fa fa-check"></i><b>2.4</b> Representing objects beyond numbers</a><ul><li class="chapter" data-level="2.4.1" data-path="lec_02_representation.html"><a href="lec_02_representation.html#finite-representations"><i class="fa fa-check"></i><b>2.4.1</b> Finite representations</a></li><li class="chapter" data-level="2.4.2" data-path="lec_02_representation.html"><a href="lec_02_representation.html#prefixfreesec"><i class="fa fa-check"></i><b>2.4.2</b> Prefix-free encoding</a></li><li class="chapter" data-level="2.4.3" data-path="lec_02_representation.html"><a href="lec_02_representation.html#making-representations-prefix-free"><i class="fa fa-check"></i><b>2.4.3</b> Making representations prefix-free</a></li><li class="chapter" data-level="2.4.4" data-path="lec_02_representation.html"><a href="lec_02_representation.html#proof-by-python-optional"><i class="fa fa-check"></i><b>2.4.4</b> Proof by Python (optional)</a></li><li class="chapter" data-level="2.4.5" data-path="lec_02_representation.html"><a href="lec_02_representation.html#representing-letters-and-text"><i class="fa fa-check"></i><b>2.4.5</b> Representing letters and text</a></li><li class="chapter" data-level="2.4.6" data-path="lec_02_representation.html"><a href="lec_02_representation.html#representing-vectors-matrices-images"><i class="fa fa-check"></i><b>2.4.6</b> Representing vectors, matrices, images</a></li><li class="chapter" data-level="2.4.7" data-path="lec_02_representation.html"><a href="lec_02_representation.html#representing-graphs"><i class="fa fa-check"></i><b>2.4.7</b> Representing graphs</a></li><li class="chapter" data-level="2.4.8" data-path="lec_02_representation.html"><a href="lec_02_representation.html#representing-lists-and-nested-lists"><i class="fa fa-check"></i><b>2.4.8</b> Representing lists and nested lists</a></li><li class="chapter" data-level="2.4.9" data-path="lec_02_representation.html"><a href="lec_02_representation.html#notation"><i class="fa fa-check"></i><b>2.4.9</b> Notation</a></li></ul></li><li class="chapter" data-level="2.5" data-path="lec_02_representation.html"><a href="lec_02_representation.html#defining-computational-tasks-as-mathematical-functions"><i class="fa fa-check"></i><b>2.5</b> Defining computational tasks as mathematical functions</a><ul><li class="chapter" data-level="2.5.1" data-path="lec_02_representation.html"><a href="lec_02_representation.html#secimplvsspec"><i class="fa fa-check"></i><b>2.5.1</b> Distinguish functions from programs!</a></li></ul></li><li class="chapter" data-level="2.6" data-path="lec_02_representation.html"><a href="lec_02_representation.html#exercises"><i class="fa fa-check"></i><b>2.6</b> Exercises</a></li><li class="chapter" data-level="2.7" data-path="lec_02_representation.html"><a href="lec_02_representation.html#bibnotesrepres"><i class="fa fa-check"></i><b>2.7</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="3" data-path="lec_03_computation.html"><a href="lec_03_computation.html"><i class="fa fa-check"></i><b>3</b> Defining computation</a><ul><li class="chapter" data-level="3.1" data-path="lec_03_computation.html"><a href="lec_03_computation.html#defining-computation"><i class="fa fa-check"></i><b>3.1</b> Defining computation</a></li><li class="chapter" data-level="3.2" data-path="lec_03_computation.html"><a href="lec_03_computation.html#computing-using-and-or-and-not."><i class="fa fa-check"></i><b>3.2</b> Computing using AND, OR, and NOT.</a><ul><li class="chapter" data-level="3.2.1" data-path="lec_03_computation.html"><a href="lec_03_computation.html#some-properties-of-and-and-or"><i class="fa fa-check"></i><b>3.2.1</b> Some properties of AND and OR</a></li><li class="chapter" data-level="3.2.2" data-path="lec_03_computation.html"><a href="lec_03_computation.html#xoraonexample"><i class="fa fa-check"></i><b>3.2.2</b> Extended example: Computing \ensuremath{\mathit{XOR}} from \ensuremath{\mathit{AND}}, \ensuremath{\mathit{OR}}, and \ensuremath{\mathit{NOT}}</a></li><li class="chapter" data-level="3.2.3" data-path="lec_03_computation.html"><a href="lec_03_computation.html#informally-defining-basic-operations-and-algorithms"><i class="fa fa-check"></i><b>3.2.3</b> Informally defining basic operations and algorithms</a></li></ul></li><li class="chapter" data-level="3.3" data-path="lec_03_computation.html"><a href="lec_03_computation.html#booleancircuitfig"><i class="fa fa-check"></i><b>3.3</b> Boolean Circuits</a><ul><li class="chapter" data-level="3.3.1" data-path="lec_03_computation.html"><a href="lec_03_computation.html#boolean-circuits-a-formal-definition"><i class="fa fa-check"></i><b>3.3.1</b> Boolean circuits: a formal definition</a></li><li class="chapter" data-level="3.3.2" data-path="lec_03_computation.html"><a href="lec_03_computation.html#equivalence-of-circuits-and-straight-line-programs"><i class="fa fa-check"></i><b>3.3.2</b> Equivalence of circuits and straight-line programs</a></li></ul></li><li class="chapter" data-level="3.4" data-path="lec_03_computation.html"><a href="lec_03_computation.html#physicalimplementationsec"><i class="fa fa-check"></i><b>3.4</b> Physical implementations of computing devices (digression)</a><ul><li class="chapter" data-level="3.4.1" data-path="lec_03_computation.html"><a href="lec_03_computation.html#transistors"><i class="fa fa-check"></i><b>3.4.1</b> Transistors</a></li><li class="chapter" data-level="3.4.2" data-path="lec_03_computation.html"><a href="lec_03_computation.html#logical-gates-from-transistors"><i class="fa fa-check"></i><b>3.4.2</b> Logical gates from transistors</a></li><li class="chapter" data-level="3.4.3" data-path="lec_03_computation.html"><a href="lec_03_computation.html#biological-computing"><i class="fa fa-check"></i><b>3.4.3</b> Biological computing</a></li><li class="chapter" data-level="3.4.4" data-path="lec_03_computation.html"><a href="lec_03_computation.html#cellular-automata-and-the-game-of-life"><i class="fa fa-check"></i><b>3.4.4</b> Cellular automata and the game of life</a></li><li class="chapter" data-level="3.4.5" data-path="lec_03_computation.html"><a href="lec_03_computation.html#neural-networks"><i class="fa fa-check"></i><b>3.4.5</b> Neural networks</a></li><li class="chapter" data-level="3.4.6" data-path="lec_03_computation.html"><a href="lec_03_computation.html#a-computer-made-from-marbles-and-pipes"><i class="fa fa-check"></i><b>3.4.6</b> A computer made from marbles and pipes</a></li></ul></li><li class="chapter" data-level="3.5" data-path="lec_03_computation.html"><a href="lec_03_computation.html#nandsec"><i class="fa fa-check"></i><b>3.5</b> The NAND function</a><ul><li class="chapter" data-level="3.5.1" data-path="lec_03_computation.html"><a href="lec_03_computation.html#nand-circuits"><i class="fa fa-check"></i><b>3.5.1</b> NAND Circuits</a></li><li class="chapter" data-level="3.5.2" data-path="lec_03_computation.html"><a href="lec_03_computation.html#more-examples-of-nand-circuits-optional"><i class="fa fa-check"></i><b>3.5.2</b> More examples of NAND circuits (optional)</a></li><li class="chapter" data-level="3.5.3" data-path="lec_03_computation.html"><a href="lec_03_computation.html#nandcircsec"><i class="fa fa-check"></i><b>3.5.3</b> The NAND-CIRC Programming language</a></li></ul></li><li class="chapter" data-level="3.6" data-path="lec_03_computation.html"><a href="lec_03_computation.html#equivalence-of-all-these-models"><i class="fa fa-check"></i><b>3.6</b> Equivalence of all these models</a><ul><li class="chapter" data-level="3.6.1" data-path="lec_03_computation.html"><a href="lec_03_computation.html#othergatessec"><i class="fa fa-check"></i><b>3.6.1</b> Circuits with other gate sets</a></li><li class="chapter" data-level="3.6.2" data-path="lec_03_computation.html"><a href="lec_03_computation.html#specvsimplrem"><i class="fa fa-check"></i><b>3.6.2</b> Specification vs. implementation (again)</a></li></ul></li><li class="chapter" data-level="3.7" data-path="lec_03_computation.html"><a href="lec_03_computation.html#exercises"><i class="fa fa-check"></i><b>3.7</b> Exercises</a></li><li class="chapter" data-level="3.8" data-path="lec_03_computation.html"><a href="lec_03_computation.html#biographical-notes"><i class="fa fa-check"></i><b>3.8</b> Biographical notes</a></li></ul></li><li class="chapter" data-level="4" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html"><i class="fa fa-check"></i><b>4</b> Syntactic sugar, and computing every function</a><ul><li class="chapter" data-level="4.1" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#secsyntacticsugar"><i class="fa fa-check"></i><b>4.1</b> Some examples of syntactic sugar</a><ul><li class="chapter" data-level="4.1.1" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#user-defined-procedures"><i class="fa fa-check"></i><b>4.1.1</b> User-defined procedures</a></li><li class="chapter" data-level="4.1.2" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#functionsynsugarthmpython"><i class="fa fa-check"></i><b>4.1.2</b> Proof by Python (optional)</a></li><li class="chapter" data-level="4.1.3" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#ifstatementsec"><i class="fa fa-check"></i><b>4.1.3</b> Conditional statements</a></li></ul></li><li class="chapter" data-level="4.2" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#addexample"><i class="fa fa-check"></i><b>4.2</b> Extended example: Addition and Multiplication (optional)</a></li><li class="chapter" data-level="4.3" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#seclookupfunc"><i class="fa fa-check"></i><b>4.3</b> The LOOKUP function</a><ul><li class="chapter" data-level="4.3.1" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#constructing-a-nand-circ-program-for-lookup"><i class="fa fa-check"></i><b>4.3.1</b> Constructing a NAND-CIRC program for \ensuremath{\mathit{LOOKUP}}</a></li></ul></li><li class="chapter" data-level="4.4" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#seccomputeallfunctions"><i class="fa fa-check"></i><b>4.4</b> Computing every function</a><ul><li class="chapter" data-level="4.4.1" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#proof-of-nands-universality"><i class="fa fa-check"></i><b>4.4.1</b> Proof of NAND’s Universality</a></li><li class="chapter" data-level="4.4.2" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#tight-upper-bound"><i class="fa fa-check"></i><b>4.4.2</b> Improving by a factor of n (optional)</a></li></ul></li><li class="chapter" data-level="4.5" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#seccomputalternative"><i class="fa fa-check"></i><b>4.5</b> Computing every function: An alternative proof</a></li><li class="chapter" data-level="4.6" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#secdefinesizeclasses"><i class="fa fa-check"></i><b>4.6</b> The class \ensuremath{\mathit{SIZE}}(T)</a></li><li class="chapter" data-level="4.7" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#exercises"><i class="fa fa-check"></i><b>4.7</b> Exercises</a></li><li class="chapter" data-level="4.8" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#computeeveryfunctionbibnotes"><i class="fa fa-check"></i><b>4.8</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="5" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html"><i class="fa fa-check"></i><b>5</b> Code as data, data as code</a><ul><li class="chapter" data-level="5.1" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#representprogramsec"><i class="fa fa-check"></i><b>5.1</b> Representing programs as strings</a></li><li class="chapter" data-level="5.2" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#countingcircuitsec"><i class="fa fa-check"></i><b>5.2</b> Counting programs, and lower bounds on the size of NAND-CIRC programs</a><ul><li class="chapter" data-level="5.2.1" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#size-hierarchy-theorem-optional"><i class="fa fa-check"></i><b>5.2.1</b> Size hierarchy theorem (optional)</a></li></ul></li><li class="chapter" data-level="5.3" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#listoftuplesrepsec"><i class="fa fa-check"></i><b>5.3</b> The tuples representation</a><ul><li class="chapter" data-level="5.3.1" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#stringrepresentationrpgoramsec"><i class="fa fa-check"></i><b>5.3.1</b> From tuples to strings</a></li></ul></li><li class="chapter" data-level="5.4" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#a-nand-circ-interpreter-in-nand-circ"><i class="fa fa-check"></i><b>5.4</b> A NAND-CIRC interpreter in NAND-CIRC</a><ul><li class="chapter" data-level="5.4.1" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#efficient-universal-programs"><i class="fa fa-check"></i><b>5.4.1</b> Efficient universal programs</a></li><li class="chapter" data-level="5.4.2" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#a-nand-circ-interpeter-in-pseudocode"><i class="fa fa-check"></i><b>5.4.2</b> A NAND-CIRC interpeter in pseudocode</a></li><li class="chapter" data-level="5.4.3" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#nandevalpythonsec"><i class="fa fa-check"></i><b>5.4.3</b> A NAND interpreter in Python</a></li><li class="chapter" data-level="5.4.4" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#constructing-the-nand-circ-interpreter-in-nand-circ"><i class="fa fa-check"></i><b>5.4.4</b> Constructing the NAND-CIRC interpreter in NAND-CIRC</a></li></ul></li><li class="chapter" data-level="5.5" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#a-python-interpreter-in-nand-circ-discussion"><i class="fa fa-check"></i><b>5.5</b> A Python interpreter in NAND-CIRC (discussion)</a></li><li class="chapter" data-level="5.6" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#PECTTsec"><i class="fa fa-check"></i><b>5.6</b> The physical extended Church-Turing thesis (discussion)</a><ul><li class="chapter" data-level="5.6.1" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#attempts-at-refuting-the-pectt"><i class="fa fa-check"></i><b>5.6.1</b> Attempts at refuting the PECTT</a></li></ul></li><li class="chapter" data-level="5.7" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#recap-of-part-i-finite-computation"><i class="fa fa-check"></i><b>5.7</b> Recap of Part I: Finite Computation</a></li><li class="chapter" data-level="5.8" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#exercises"><i class="fa fa-check"></i><b>5.8</b> Exercises</a></li><li class="chapter" data-level="5.9" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#bibnotescodeasdata"><i class="fa fa-check"></i><b>5.9</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="6" data-path="lec_06_loops.html"><a href="lec_06_loops.html"><i class="fa fa-check"></i><b>6</b> Loops and infinity</a><ul><li class="chapter" data-level="6.1" data-path="lec_06_loops.html"><a href="lec_06_loops.html#turing-machines"><i class="fa fa-check"></i><b>6.1</b> Turing Machines</a><ul><li class="chapter" data-level="6.1.1" data-path="lec_06_loops.html"><a href="lec_06_loops.html#turingmachinepalindrome"><i class="fa fa-check"></i><b>6.1.1</b> Extended example: A Turing machine for palindromes</a></li><li class="chapter" data-level="6.1.2" data-path="lec_06_loops.html"><a href="lec_06_loops.html#turing-machines-a-formal-definition"><i class="fa fa-check"></i><b>6.1.2</b> Turing machines: a formal definition</a></li><li class="chapter" data-level="6.1.3" data-path="lec_06_loops.html"><a href="lec_06_loops.html#computable-functions"><i class="fa fa-check"></i><b>6.1.3</b> Computable functions</a></li><li class="chapter" data-level="6.1.4" data-path="lec_06_loops.html"><a href="lec_06_loops.html#infinite-loops-and-partial-functions"><i class="fa fa-check"></i><b>6.1.4</b> Infinite loops and partial functions</a></li></ul></li><li class="chapter" data-level="6.2" data-path="lec_06_loops.html"><a href="lec_06_loops.html#turing-machines-as-programming-languages"><i class="fa fa-check"></i><b>6.2</b> Turing machines as programming languages</a><ul><li class="chapter" data-level="6.2.1" data-path="lec_06_loops.html"><a href="lec_06_loops.html#the-nand-tm-programming-language"><i class="fa fa-check"></i><b>6.2.1</b> The NAND-TM Programming language</a></li><li class="chapter" data-level="6.2.2" data-path="lec_06_loops.html"><a href="lec_06_loops.html#sneak-peak-nand-tm-vs-turing-machines"><i class="fa fa-check"></i><b>6.2.2</b> Sneak peak: NAND-TM vs Turing machines</a></li><li class="chapter" data-level="6.2.3" data-path="lec_06_loops.html"><a href="lec_06_loops.html#examples"><i class="fa fa-check"></i><b>6.2.3</b> Examples</a></li></ul></li><li class="chapter" data-level="6.3" data-path="lec_06_loops.html"><a href="lec_06_loops.html#equivalence-of-turing-machines-and-nand-tm-programs"><i class="fa fa-check"></i><b>6.3</b> Equivalence of Turing machines and NAND-TM programs</a><ul><li class="chapter" data-level="6.3.1" data-path="lec_06_loops.html"><a href="lec_06_loops.html#specification-vs-implementation-again"><i class="fa fa-check"></i><b>6.3.1</b> Specification vs implementation (again)</a></li></ul></li><li class="chapter" data-level="6.4" data-path="lec_06_loops.html"><a href="lec_06_loops.html#nand-tm-syntactic-sugar"><i class="fa fa-check"></i><b>6.4</b> NAND-TM syntactic sugar</a><ul><li class="chapter" data-level="6.4.1" data-path="lec_06_loops.html"><a href="lec_06_loops.html#nandtminnerloopssec"><i class="fa fa-check"></i><b>6.4.1</b> GOTO and inner loops</a></li></ul></li><li class="chapter" data-level="6.5" data-path="lec_06_loops.html"><a href="lec_06_loops.html#uniformity-and-nand-vs-nand-tm-discussion"><i class="fa fa-check"></i><b>6.5</b> Uniformity, and NAND vs NAND-TM (discussion)</a></li><li class="chapter" data-level="6.6" data-path="lec_06_loops.html"><a href="lec_06_loops.html#exercises"><i class="fa fa-check"></i><b>6.6</b> Exercises</a></li><li class="chapter" data-level="6.7" data-path="lec_06_loops.html"><a href="lec_06_loops.html#chaploopnotes"><i class="fa fa-check"></i><b>6.7</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="7" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html"><i class="fa fa-check"></i><b>7</b> Equivalent models of computation</a><ul><li class="chapter" data-level="7.1" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#ram-machines-and-nand-ram"><i class="fa fa-check"></i><b>7.1</b> RAM machines and NAND-RAM</a></li><li class="chapter" data-level="7.2" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#nandtmgorydetailssec"><i class="fa fa-check"></i><b>7.2</b> The gory details (optional)</a><ul><li class="chapter" data-level="7.2.1" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#indexed-access-in-nand-tm"><i class="fa fa-check"></i><b>7.2.1</b> Indexed access in NAND-TM</a></li><li class="chapter" data-level="7.2.2" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#two-dimensional-arrays-in-nand-tm"><i class="fa fa-check"></i><b>7.2.2</b> Two dimensional arrays in NAND-TM</a></li><li class="chapter" data-level="7.2.3" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#all-the-rest"><i class="fa fa-check"></i><b>7.2.3</b> All the rest</a></li></ul></li><li class="chapter" data-level="7.3" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#turing-equivalence-discussion"><i class="fa fa-check"></i><b>7.3</b> Turing equivalence (discussion)</a><ul><li class="chapter" data-level="7.3.1" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#the-best-of-both-worlds-paradigm"><i class="fa fa-check"></i><b>7.3.1</b> The Best of both worlds paradigm</a></li><li class="chapter" data-level="7.3.2" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#lets-talk-about-abstractions."><i class="fa fa-check"></i><b>7.3.2</b> Let’s talk about abstractions.</a></li><li class="chapter" data-level="7.3.3" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#turingcompletesec"><i class="fa fa-check"></i><b>7.3.3</b> Turing completeness and equivalence, a formal definition (optional)</a></li></ul></li><li class="chapter" data-level="7.4" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#cellularautomatasec"><i class="fa fa-check"></i><b>7.4</b> Cellular automata</a><ul><li class="chapter" data-level="7.4.1" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#one-dimensional-cellular-automata-are-turing-complete"><i class="fa fa-check"></i><b>7.4.1</b> One dimensional cellular automata are Turing complete</a></li><li class="chapter" data-level="7.4.2" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#turingmachinesconfigsec"><i class="fa fa-check"></i><b>7.4.2</b> Configurations of Turing machines and the next-step function</a></li></ul></li><li class="chapter" data-level="7.5" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#lambdacalculussec"><i class="fa fa-check"></i><b>7.5</b> Lambda calculus and functional programming languages</a><ul><li class="chapter" data-level="7.5.1" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#applying-functions-to-functions"><i class="fa fa-check"></i><b>7.5.1</b> Applying functions to functions</a></li><li class="chapter" data-level="7.5.2" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#curryingsec"><i class="fa fa-check"></i><b>7.5.2</b> Obtaining multi-argument functions via Currying</a></li><li class="chapter" data-level="7.5.3" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#formal-description-of-the-λ-calculus."><i class="fa fa-check"></i><b>7.5.3</b> Formal description of the λ calculus.</a></li><li class="chapter" data-level="7.5.4" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#infiniteloopslambda"><i class="fa fa-check"></i><b>7.5.4</b> Infinite loops in the λ calculus</a></li></ul></li><li class="chapter" data-level="7.6" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#the-enhanced-λ-calculus"><i class="fa fa-check"></i><b>7.6</b> The Enhanced λ calculus</a><ul><li class="chapter" data-level="7.6.1" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#computing-a-function-in-the-enhanced-λ-calculus"><i class="fa fa-check"></i><b>7.6.1</b> Computing a function in the enhanced λ calculus</a></li><li class="chapter" data-level="7.6.2" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#enhanced-λ-calculus-is-turing-complete"><i class="fa fa-check"></i><b>7.6.2</b> Enhanced λ calculus is Turing-complete</a></li></ul></li><li class="chapter" data-level="7.7" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#lambdacacluluspuresec"><i class="fa fa-check"></i><b>7.7</b> From enhanced to pure λ calculus</a><ul><li class="chapter" data-level="7.7.1" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#list-processing"><i class="fa fa-check"></i><b>7.7.1</b> List processing</a></li><li class="chapter" data-level="7.7.2" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#ycombinatorsec"><i class="fa fa-check"></i><b>7.7.2</b> The Y combinator, or recursion without recursion</a></li></ul></li><li class="chapter" data-level="7.8" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#churchturingdiscussionsec"><i class="fa fa-check"></i><b>7.8</b> The Church-Turing Thesis (discussion)</a><ul><li class="chapter" data-level="7.8.1" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#different-models-of-computation"><i class="fa fa-check"></i><b>7.8.1</b> Different models of computation</a></li></ul></li><li class="chapter" data-level="7.9" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#exercises"><i class="fa fa-check"></i><b>7.9</b> Exercises</a></li><li class="chapter" data-level="7.10" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#othermodelsbibnotes"><i class="fa fa-check"></i><b>7.10</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="8" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html"><i class="fa fa-check"></i><b>8</b> Universality and uncomputability</a><ul><li class="chapter" data-level="8.1" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#universality-or-a-meta-circular-evaluator"><i class="fa fa-check"></i><b>8.1</b> Universality or a meta-circular evaluator</a><ul><li class="chapter" data-level="8.1.1" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#representtmsec"><i class="fa fa-check"></i><b>8.1.1</b> Proving the existence of a universal Turing Machine</a></li><li class="chapter" data-level="8.1.2" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#implications-of-universality-discussion"><i class="fa fa-check"></i><b>8.1.2</b> Implications of universality (discussion)</a></li></ul></li><li class="chapter" data-level="8.2" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#is-every-function-computable"><i class="fa fa-check"></i><b>8.2</b> Is every function computable?</a></li><li class="chapter" data-level="8.3" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#haltingsec"><i class="fa fa-check"></i><b>8.3</b> The Halting problem</a><ul><li class="chapter" data-level="8.3.1" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#is-the-halting-problem-really-hard-discussion"><i class="fa fa-check"></i><b>8.3.1</b> Is the Halting problem really hard? (discussion)</a></li><li class="chapter" data-level="8.3.2" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#haltalternativesec"><i class="fa fa-check"></i><b>8.3.2</b> A direct proof of the uncomputability of \ensuremath{\mathit{HALT}} (optional)</a></li></ul></li><li class="chapter" data-level="8.4" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#reductionsuncompsec"><i class="fa fa-check"></i><b>8.4</b> Reductions</a><ul><li class="chapter" data-level="8.4.1" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#example-halting-on-the-zero-problem"><i class="fa fa-check"></i><b>8.4.1</b> Example: Halting on the zero problem</a></li></ul></li><li class="chapter" data-level="8.5" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#rices-theorem-and-the-impossibility-of-general-software-verification"><i class="fa fa-check"></i><b>8.5</b> Rice’s Theorem and the impossibility of general software verification</a><ul><li class="chapter" data-level="8.5.1" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#ricethmsec"><i class="fa fa-check"></i><b>8.5.1</b> Rice’s Theorem</a></li><li class="chapter" data-level="8.5.2" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#halting-and-rices-theorem-for-other-turing-complete-models"><i class="fa fa-check"></i><b>8.5.2</b> Halting and Rice’s Theorem for other Turing-complete models</a></li><li class="chapter" data-level="8.5.3" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#is-software-verification-doomed-discussion"><i class="fa fa-check"></i><b>8.5.3</b> Is software verification doomed? (discussion)</a></li></ul></li><li class="chapter" data-level="8.6" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#exercises"><i class="fa fa-check"></i><b>8.6</b> Exercises</a></li><li class="chapter" data-level="8.7" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#uncomputablebibnotes"><i class="fa fa-check"></i><b>8.7</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="9" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html"><i class="fa fa-check"></i><b>9</b> Restricted computational models</a><ul><li class="chapter" data-level="9.1" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#turing-completeness-as-a-bug"><i class="fa fa-check"></i><b>9.1</b> Turing completeness as a bug</a></li><li class="chapter" data-level="9.2" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#regular-expressions"><i class="fa fa-check"></i><b>9.2</b> Regular expressions</a></li><li class="chapter" data-level="9.3" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#deterministic-finite-automata-and-efficient-matching-of-regular-expressions-optional"><i class="fa fa-check"></i><b>9.3</b> Deterministic finite automata, and efficient matching of regular expressions (optional)</a><ul><li class="chapter" data-level="9.3.1" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#matching-regular-expressions-using-constant-memory"><i class="fa fa-check"></i><b>9.3.1</b> Matching regular expressions using constant memory</a></li><li class="chapter" data-level="9.3.2" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#secdfa"><i class="fa fa-check"></i><b>9.3.2</b> Deterministic Finite Automata</a></li><li class="chapter" data-level="9.3.3" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#regular-functions-are-closed-under-complement"><i class="fa fa-check"></i><b>9.3.3</b> Regular functions are closed under complement</a></li></ul></li><li class="chapter" data-level="9.4" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#limitations-of-regular-expressions"><i class="fa fa-check"></i><b>9.4</b> Limitations of regular expressions</a></li><li class="chapter" data-level="9.5" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#other-semantic-properties-of-regular-expressions"><i class="fa fa-check"></i><b>9.5</b> Other semantic properties of regular expressions</a></li><li class="chapter" data-level="9.6" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#seccfg"><i class="fa fa-check"></i><b>9.6</b> Context free grammars</a><ul><li class="chapter" data-level="9.6.1" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#context-free-grammars-as-a-computational-model"><i class="fa fa-check"></i><b>9.6.1</b> Context-free grammars as a computational model</a></li><li class="chapter" data-level="9.6.2" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#the-power-of-context-free-grammars"><i class="fa fa-check"></i><b>9.6.2</b> The power of context free grammars</a></li><li class="chapter" data-level="9.6.3" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#limitations-of-context-free-grammars-optional"><i class="fa fa-check"></i><b>9.6.3</b> Limitations of context-free grammars (optional)</a></li></ul></li><li class="chapter" data-level="9.7" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#semantic-properties-of-context-free-languages"><i class="fa fa-check"></i><b>9.7</b> Semantic properties of context free languages</a><ul><li class="chapter" data-level="9.7.1" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#uncomputability-of-context-free-grammar-equivalence-optional"><i class="fa fa-check"></i><b>9.7.1</b> Uncomputability of context-free grammar equivalence (optional)</a></li></ul></li><li class="chapter" data-level="9.8" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#summary-of-semantic-properties-for-regular-expressions-and-context-free-grammars"><i class="fa fa-check"></i><b>9.8</b> Summary of semantic properties for regular expressions and context-free grammars</a></li><li class="chapter" data-level="9.9" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#exercises"><i class="fa fa-check"></i><b>9.9</b> Exercises</a></li><li class="chapter" data-level="9.10" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#bibliographical-notes"><i class="fa fa-check"></i><b>9.10</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="10" data-path="lec_09_godel.html"><a href="lec_09_godel.html"><i class="fa fa-check"></i><b>10</b> Is every theorem provable?</a><ul><li class="chapter" data-level="10.1" data-path="lec_09_godel.html"><a href="lec_09_godel.html#godelproofdef"><i class="fa fa-check"></i><b>10.1</b> Hilbert’s Program and Gödel’s Incompleteness Theorem</a><ul><li class="chapter" data-level="10.1.1" data-path="lec_09_godel.html"><a href="lec_09_godel.html#godelproofsystemssec"><i class="fa fa-check"></i><b>10.1.1</b> Defining Proof Systems</a></li></ul></li><li class="chapter" data-level="10.2" data-path="lec_09_godel.html"><a href="lec_09_godel.html#gödels-incompleteness-theorem-computational-variant"><i class="fa fa-check"></i><b>10.2</b> Gödel’s Incompleteness Theorem: Computational variant</a></li><li class="chapter" data-level="10.3" data-path="lec_09_godel.html"><a href="lec_09_godel.html#quantified-integer-statements"><i class="fa fa-check"></i><b>10.3</b> Quantified integer statements</a></li><li class="chapter" data-level="10.4" data-path="lec_09_godel.html"><a href="lec_09_godel.html#diophantine-equations-and-the-mrdp-theorem"><i class="fa fa-check"></i><b>10.4</b> Diophantine equations and the MRDP Theorem</a></li><li class="chapter" data-level="10.5" data-path="lec_09_godel.html"><a href="lec_09_godel.html#hardness-of-quantified-integer-statements"><i class="fa fa-check"></i><b>10.5</b> Hardness of quantified integer statements</a><ul><li class="chapter" data-level="10.5.1" data-path="lec_09_godel.html"><a href="lec_09_godel.html#step-1-quantified-mixed-statements-and-computation-histories"><i class="fa fa-check"></i><b>10.5.1</b> Step 1: Quantified mixed statements and computation histories</a></li><li class="chapter" data-level="10.5.2" data-path="lec_09_godel.html"><a href="lec_09_godel.html#step-2-reducing-mixed-statements-to-integer-statements"><i class="fa fa-check"></i><b>10.5.2</b> Step 2: Reducing mixed statements to integer statements</a></li></ul></li><li class="chapter" data-level="10.6" data-path="lec_09_godel.html"><a href="lec_09_godel.html#exercises"><i class="fa fa-check"></i><b>10.6</b> Exercises</a></li><li class="chapter" data-level="10.7" data-path="lec_09_godel.html"><a href="lec_09_godel.html#bibliographical-notes"><i class="fa fa-check"></i><b>10.7</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="11" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html"><i class="fa fa-check"></i><b>11</b> Efficient computation</a><ul><li class="chapter" data-level="11.1" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#problems-on-graphs"><i class="fa fa-check"></i><b>11.1</b> Problems on graphs</a><ul><li class="chapter" data-level="11.1.1" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#finding-the-shortest-path-in-a-graph"><i class="fa fa-check"></i><b>11.1.1</b> Finding the shortest path in a graph</a></li><li class="chapter" data-level="11.1.2" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#finding-the-longest-path-in-a-graph"><i class="fa fa-check"></i><b>11.1.2</b> Finding the longest path in a graph</a></li><li class="chapter" data-level="11.1.3" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#mincutsec"><i class="fa fa-check"></i><b>11.1.3</b> Finding the minimum cut in a graph</a></li><li class="chapter" data-level="11.1.4" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#linerprogsec"><i class="fa fa-check"></i><b>11.1.4</b> Min-Cut Max-Flow and Linear programming</a></li><li class="chapter" data-level="11.1.5" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#finding-the-maximum-cut-in-a-graph"><i class="fa fa-check"></i><b>11.1.5</b> Finding the maximum cut in a graph</a></li><li class="chapter" data-level="11.1.6" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#a-note-on-convexity"><i class="fa fa-check"></i><b>11.1.6</b> A note on convexity</a></li></ul></li><li class="chapter" data-level="11.2" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#beyond-graphs"><i class="fa fa-check"></i><b>11.2</b> Beyond graphs</a><ul><li class="chapter" data-level="11.2.1" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#sat"><i class="fa fa-check"></i><b>11.2.1</b> SAT</a></li><li class="chapter" data-level="11.2.2" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#solving-linear-equations"><i class="fa fa-check"></i><b>11.2.2</b> Solving linear equations</a></li><li class="chapter" data-level="11.2.3" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#solving-quadratic-equations"><i class="fa fa-check"></i><b>11.2.3</b> Solving quadratic equations</a></li></ul></li><li class="chapter" data-level="11.3" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#more-advanced-examples"><i class="fa fa-check"></i><b>11.3</b> More advanced examples</a><ul><li class="chapter" data-level="11.3.1" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#determinant-of-a-matrix"><i class="fa fa-check"></i><b>11.3.1</b> Determinant of a matrix</a></li><li class="chapter" data-level="11.3.2" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#permanent-of-a-matrix"><i class="fa fa-check"></i><b>11.3.2</b> Permanent of a matrix</a></li><li class="chapter" data-level="11.3.3" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#finding-a-zero-sum-equilibrium"><i class="fa fa-check"></i><b>11.3.3</b> Finding a zero-sum equilibrium</a></li><li class="chapter" data-level="11.3.4" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#finding-a-nash-equilibrium"><i class="fa fa-check"></i><b>11.3.4</b> Finding a Nash equilibrium</a></li><li class="chapter" data-level="11.3.5" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#primality-testing"><i class="fa fa-check"></i><b>11.3.5</b> Primality testing</a></li><li class="chapter" data-level="11.3.6" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#integer-factoring"><i class="fa fa-check"></i><b>11.3.6</b> Integer factoring</a></li></ul></li><li class="chapter" data-level="11.4" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#our-current-knowledge"><i class="fa fa-check"></i><b>11.4</b> Our current knowledge</a></li><li class="chapter" data-level="11.5" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#exercises"><i class="fa fa-check"></i><b>11.5</b> Exercises</a></li><li class="chapter" data-level="11.6" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#effalgnotes"><i class="fa fa-check"></i><b>11.6</b> Bibliographical notes</a></li><li class="chapter" data-level="11.7" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#further-explorations"><i class="fa fa-check"></i><b>11.7</b> Further explorations</a></li></ul></li><li class="chapter" data-level="12" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html"><i class="fa fa-check"></i><b>12</b> Modeling running time</a><ul><li class="chapter" data-level="12.1" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#formally-defining-running-time"><i class="fa fa-check"></i><b>12.1</b> Formally defining running time</a><ul><li class="chapter" data-level="12.1.1" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#polynomial-and-exponential-time"><i class="fa fa-check"></i><b>12.1.1</b> Polynomial and Exponential Time</a></li></ul></li><li class="chapter" data-level="12.2" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#modeling-running-time-using-ram-machines-nand-ram"><i class="fa fa-check"></i><b>12.2</b> Modeling running time using RAM Machines / NAND-RAM</a></li><li class="chapter" data-level="12.3" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#ECTTsec"><i class="fa fa-check"></i><b>12.3</b> Extended Church-Turing Thesis (discussion)</a></li><li class="chapter" data-level="12.4" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#efficient-universal-machine-a-nand-ram-interpreter-in-nand-ram"><i class="fa fa-check"></i><b>12.4</b> Efficient universal machine: a NAND-RAM interpreter in NAND-RAM</a><ul><li class="chapter" data-level="12.4.1" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#timed-universal-turing-machine"><i class="fa fa-check"></i><b>12.4.1</b> Timed Universal Turing Machine</a></li></ul></li><li class="chapter" data-level="12.5" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#the-time-hierarchy-theorem"><i class="fa fa-check"></i><b>12.5</b> The time hierarchy theorem</a></li><li class="chapter" data-level="12.6" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#nonuniformcompsec"><i class="fa fa-check"></i><b>12.6</b> Non uniform computation</a><ul><li class="chapter" data-level="12.6.1" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#obliviousnandtm"><i class="fa fa-check"></i><b>12.6.1</b> Oblivious NAND-TM programs</a></li><li class="chapter" data-level="12.6.2" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#unrollloopsec"><i class="fa fa-check"></i><b>12.6.2</b> Unrolling the loop: algorithmic transformation of Turing Machines to circuits</a></li><li class="chapter" data-level="12.6.3" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#can-uniform-algorithms-simulate-non-uniform-ones"><i class="fa fa-check"></i><b>12.6.3</b> Can uniform algorithms simulate non uniform ones?</a></li><li class="chapter" data-level="12.6.4" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#uniform-vs.-nonuniform-computation-a-recap"><i class="fa fa-check"></i><b>12.6.4</b> Uniform vs. Nonuniform computation: A recap</a></li></ul></li><li class="chapter" data-level="12.7" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#exercises"><i class="fa fa-check"></i><b>12.7</b> Exercises</a></li><li class="chapter" data-level="12.8" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#bibnotesrunningtime"><i class="fa fa-check"></i><b>12.8</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="13" data-path="lec_12_NP.html"><a href="lec_12_NP.html"><i class="fa fa-check"></i><b>13</b> Polynomial-time reductions</a><ul><li class="chapter" data-level="13.1" data-path="lec_12_NP.html"><a href="lec_12_NP.html#formaldefdecisionexamplessec"><i class="fa fa-check"></i><b>13.1</b> Formal definitions of problems</a></li><li class="chapter" data-level="13.2" data-path="lec_12_NP.html"><a href="lec_12_NP.html#polytimeredsec"><i class="fa fa-check"></i><b>13.2</b> Polynomial-time reductions</a></li><li class="chapter" data-level="13.3" data-path="lec_12_NP.html"><a href="lec_12_NP.html#reducing-3sat-to-zero-one-equations"><i class="fa fa-check"></i><b>13.3</b> Reducing 3SAT to zero one equations</a><ul><li class="chapter" data-level="13.3.1" data-path="lec_12_NP.html"><a href="lec_12_NP.html#quadratic-equations"><i class="fa fa-check"></i><b>13.3.1</b> Quadratic equations</a></li></ul></li><li class="chapter" data-level="13.4" data-path="lec_12_NP.html"><a href="lec_12_NP.html#the-independent-set-problem"><i class="fa fa-check"></i><b>13.4</b> The independent set problem</a></li><li class="chapter" data-level="13.5" data-path="lec_12_NP.html"><a href="lec_12_NP.html#reducing-independent-set-to-maximum-cut"><i class="fa fa-check"></i><b>13.5</b> Reducing Independent Set to Maximum Cut</a></li><li class="chapter" data-level="13.6" data-path="lec_12_NP.html"><a href="lec_12_NP.html#reducing-3sat-to-longest-path"><i class="fa fa-check"></i><b>13.6</b> Reducing 3SAT to Longest Path</a><ul><li class="chapter" data-level="13.6.1" data-path="lec_12_NP.html"><a href="lec_12_NP.html#summary-of-relations"><i class="fa fa-check"></i><b>13.6.1</b> Summary of relations</a></li></ul></li><li class="chapter" data-level="13.7" data-path="lec_12_NP.html"><a href="lec_12_NP.html#exercises"><i class="fa fa-check"></i><b>13.7</b> Exercises</a></li><li class="chapter" data-level="13.8" data-path="lec_12_NP.html"><a href="lec_12_NP.html#reductionsbibnotes"><i class="fa fa-check"></i><b>13.8</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="14" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html"><i class="fa fa-check"></i><b>14</b> NP, NP completeness, and the Cook-Levin Theorem</a><ul><li class="chapter" data-level="14.1" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#the-class-mathbfnp"><i class="fa fa-check"></i><b>14.1</b> The class \mathbf{NP}</a><ul><li class="chapter" data-level="14.1.1" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#examples-of-functions-in-mathbfnp"><i class="fa fa-check"></i><b>14.1.1</b> Examples of functions in \mathbf{NP}</a></li><li class="chapter" data-level="14.1.2" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#basic-facts-about-mathbfnp"><i class="fa fa-check"></i><b>14.1.2</b> Basic facts about \mathbf{NP}</a></li></ul></li><li class="chapter" data-level="14.2" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#from-mathbfnp-to-3sat-the-cook-levin-theorem"><i class="fa fa-check"></i><b>14.2</b> From \mathbf{NP} to 3SAT: The Cook-Levin Theorem</a><ul><li class="chapter" data-level="14.2.1" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#what-does-this-mean"><i class="fa fa-check"></i><b>14.2.1</b> What does this mean?</a></li><li class="chapter" data-level="14.2.2" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#the-cook-levin-theorem-proof-outline"><i class="fa fa-check"></i><b>14.2.2</b> The Cook-Levin Theorem: Proof outline</a></li></ul></li><li class="chapter" data-level="14.3" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#the-nandsat-problem-and-why-it-is-mathbfnp-hard."><i class="fa fa-check"></i><b>14.3</b> The \ensuremath{\mathit{NANDSAT}} Problem, and why it is \mathbf{NP} hard.</a></li><li class="chapter" data-level="14.4" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#the-3nand-problem"><i class="fa fa-check"></i><b>14.4</b> The 3\ensuremath{\mathit{NAND}} problem</a></li><li class="chapter" data-level="14.5" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#from-3nand-to-3sat"><i class="fa fa-check"></i><b>14.5</b> From 3\ensuremath{\mathit{NAND}} to 3\ensuremath{\mathit{SAT}}</a></li><li class="chapter" data-level="14.6" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#wrapping-up"><i class="fa fa-check"></i><b>14.6</b> Wrapping up</a></li><li class="chapter" data-level="14.7" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#exercises"><i class="fa fa-check"></i><b>14.7</b> Exercises</a></li><li class="chapter" data-level="14.8" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#bibliographical-notes"><i class="fa fa-check"></i><b>14.8</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="15" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html"><i class="fa fa-check"></i><b>15</b> What if P equals NP?</a><ul><li class="chapter" data-level="15.1" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#search-to-decision-reduction"><i class="fa fa-check"></i><b>15.1</b> Search-to-decision reduction</a></li><li class="chapter" data-level="15.2" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#optimizationsection"><i class="fa fa-check"></i><b>15.2</b> Optimization</a><ul><li class="chapter" data-level="15.2.1" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#example-supervised-learning"><i class="fa fa-check"></i><b>15.2.1</b> Example: Supervised learning</a></li><li class="chapter" data-level="15.2.2" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#example-breaking-cryptosystems"><i class="fa fa-check"></i><b>15.2.2</b> Example: Breaking cryptosystems</a></li></ul></li><li class="chapter" data-level="15.3" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#finding-mathematical-proofs"><i class="fa fa-check"></i><b>15.3</b> Finding mathematical proofs</a></li><li class="chapter" data-level="15.4" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#quantifier-elimination-advanced"><i class="fa fa-check"></i><b>15.4</b> Quantifier elimination (advanced)</a><ul><li class="chapter" data-level="15.4.1" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#selfimprovingsat"><i class="fa fa-check"></i><b>15.4.1</b> Application: self improving algorithm for 3\ensuremath{\mathit{SAT}}</a></li></ul></li><li class="chapter" data-level="15.5" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#approximating-counting-problems-and-posterior-sampling-advanced-optional"><i class="fa fa-check"></i><b>15.5</b> Approximating counting problems and posterior sampling (advanced, optional)</a></li><li class="chapter" data-level="15.6" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#what-does-all-of-this-imply"><i class="fa fa-check"></i><b>15.6</b> What does all of this imply?</a></li><li class="chapter" data-level="15.7" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#can-mathbfp-neq-mathbfnp-be-neither-true-nor-false"><i class="fa fa-check"></i><b>15.7</b> Can \mathbf{P} \neq \mathbf{NP} be neither true nor false?</a></li><li class="chapter" data-level="15.8" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#is-mathbfpmathbfnp-in-practice"><i class="fa fa-check"></i><b>15.8</b> Is \mathbf{P}=\mathbf{NP} in practice?</a></li><li class="chapter" data-level="15.9" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#what-if-mathbfp-neq-mathbfnp"><i class="fa fa-check"></i><b>15.9</b> What if \mathbf{P} \neq \mathbf{NP}?</a></li><li class="chapter" data-level="15.10" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#exercises"><i class="fa fa-check"></i><b>15.10</b> Exercises</a></li><li class="chapter" data-level="15.11" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#bibliographical-notes"><i class="fa fa-check"></i><b>15.11</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="16" data-path="lec_14a_space_complexity.html"><a href="lec_14a_space_complexity.html"><i class="fa fa-check"></i><b>16</b> Space bounded computation</a><ul><li class="chapter" data-level="16.1" data-path="lec_14a_space_complexity.html"><a href="lec_14a_space_complexity.html#exercises"><i class="fa fa-check"></i><b>16.1</b> Exercises</a></li><li class="chapter" data-level="16.2" data-path="lec_14a_space_complexity.html"><a href="lec_14a_space_complexity.html#bibliographical-notes"><i class="fa fa-check"></i><b>16.2</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="17" data-path="lec_15_probability.html"><a href="lec_15_probability.html"><i class="fa fa-check"></i><b>17</b> Probability Theory 101</a><ul><li class="chapter" data-level="17.1" data-path="lec_15_probability.html"><a href="lec_15_probability.html#random-coins"><i class="fa fa-check"></i><b>17.1</b> Random coins</a><ul><li class="chapter" data-level="17.1.1" data-path="lec_15_probability.html"><a href="lec_15_probability.html#random-variables"><i class="fa fa-check"></i><b>17.1.1</b> Random variables</a></li><li class="chapter" data-level="17.1.2" data-path="lec_15_probability.html"><a href="lec_15_probability.html#distributions-over-strings"><i class="fa fa-check"></i><b>17.1.2</b> Distributions over strings</a></li><li class="chapter" data-level="17.1.3" data-path="lec_15_probability.html"><a href="lec_15_probability.html#more-general-sample-spaces."><i class="fa fa-check"></i><b>17.1.3</b> More general sample spaces.</a></li></ul></li><li class="chapter" data-level="17.2" data-path="lec_15_probability.html"><a href="lec_15_probability.html#correlations-and-independence"><i class="fa fa-check"></i><b>17.2</b> Correlations and independence</a><ul><li class="chapter" data-level="17.2.1" data-path="lec_15_probability.html"><a href="lec_15_probability.html#independent-random-variables"><i class="fa fa-check"></i><b>17.2.1</b> Independent random variables</a></li><li class="chapter" data-level="17.2.2" data-path="lec_15_probability.html"><a href="lec_15_probability.html#collections-of-independent-random-variables."><i class="fa fa-check"></i><b>17.2.2</b> Collections of independent random variables.</a></li></ul></li><li class="chapter" data-level="17.3" data-path="lec_15_probability.html"><a href="lec_15_probability.html#concentration-and-tail-bounds"><i class="fa fa-check"></i><b>17.3</b> Concentration and tail bounds</a><ul><li class="chapter" data-level="17.3.1" data-path="lec_15_probability.html"><a href="lec_15_probability.html#chebyshevs-inequality"><i class="fa fa-check"></i><b>17.3.1</b> Chebyshev’s Inequality</a></li><li class="chapter" data-level="17.3.2" data-path="lec_15_probability.html"><a href="lec_15_probability.html#the-chernoff-bound"><i class="fa fa-check"></i><b>17.3.2</b> The Chernoff bound</a></li></ul></li><li class="chapter" data-level="17.4" data-path="lec_15_probability.html"><a href="lec_15_probability.html#exercises"><i class="fa fa-check"></i><b>17.4</b> Exercises</a></li><li class="chapter" data-level="17.5" data-path="lec_15_probability.html"><a href="lec_15_probability.html#bibliographical-notes"><i class="fa fa-check"></i><b>17.5</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="18" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html"><i class="fa fa-check"></i><b>18</b> Probabilistic computation</a><ul><li class="chapter" data-level="18.1" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html#finding-approximately-good-maximum-cuts."><i class="fa fa-check"></i><b>18.1</b> Finding approximately good maximum cuts.</a><ul><li class="chapter" data-level="18.1.1" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html#amplifying-the-success-of-randomized-algorithms"><i class="fa fa-check"></i><b>18.1.1</b> Amplifying the success of randomized algorithms</a></li><li class="chapter" data-level="18.1.2" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html#success-amplification"><i class="fa fa-check"></i><b>18.1.2</b> Success amplification</a></li><li class="chapter" data-level="18.1.3" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html#two-sided-amplification"><i class="fa fa-check"></i><b>18.1.3</b> Two-sided amplification</a></li><li class="chapter" data-level="18.1.4" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html#what-does-this-mean"><i class="fa fa-check"></i><b>18.1.4</b> What does this mean?</a></li><li class="chapter" data-level="18.1.5" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html#solving-sat-through-randomization"><i class="fa fa-check"></i><b>18.1.5</b> Solving SAT through randomization</a></li><li class="chapter" data-level="18.1.6" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html#bipartite-matching."><i class="fa fa-check"></i><b>18.1.6</b> Bipartite matching.</a></li></ul></li><li class="chapter" data-level="18.2" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html#exercises"><i class="fa fa-check"></i><b>18.2</b> Exercises</a></li><li class="chapter" data-level="18.3" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html#bibliographical-notes"><i class="fa fa-check"></i><b>18.3</b> Bibliographical notes</a></li><li class="chapter" data-level="18.4" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html#acknowledgements"><i class="fa fa-check"></i><b>18.4</b> Acknowledgements</a></li></ul></li><li class="chapter" data-level="19" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html"><i class="fa fa-check"></i><b>19</b> Modeling randomized computation</a><ul><li class="chapter" data-level="19.1" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#modeling-randomized-computation"><i class="fa fa-check"></i><b>19.1</b> Modeling randomized computation</a><ul><li class="chapter" data-level="19.1.1" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#an-alternative-view-random-coins-as-an-extra-input"><i class="fa fa-check"></i><b>19.1.1</b> An alternative view: random coins as an extra input</a></li><li class="chapter" data-level="19.1.2" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#successamptwosided"><i class="fa fa-check"></i><b>19.1.2</b> Success amplification of two-sided error algorithms</a></li></ul></li><li class="chapter" data-level="19.2" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#mathbfbpp-and-mathbfnp-completeness"><i class="fa fa-check"></i><b>19.2</b> \mathbf{BPP} and \mathbf{NP} completeness</a></li><li class="chapter" data-level="19.3" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#the-power-of-randomization"><i class="fa fa-check"></i><b>19.3</b> The power of randomization</a><ul><li class="chapter" data-level="19.3.1" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#solving-mathbfbpp-in-exponential-time"><i class="fa fa-check"></i><b>19.3.1</b> Solving \mathbf{BPP} in exponential time</a></li><li class="chapter" data-level="19.3.2" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#simulating-randomized-algorithms-by-circuits"><i class="fa fa-check"></i><b>19.3.2</b> Simulating randomized algorithms by circuits</a></li></ul></li><li class="chapter" data-level="19.4" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#derandomization"><i class="fa fa-check"></i><b>19.4</b> Derandomization</a><ul><li class="chapter" data-level="19.4.1" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#pseudorandom-generators"><i class="fa fa-check"></i><b>19.4.1</b> Pseudorandom generators</a></li><li class="chapter" data-level="19.4.2" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#optimalprgconj"><i class="fa fa-check"></i><b>19.4.2</b> From existence to constructivity</a></li><li class="chapter" data-level="19.4.3" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#usefulness-of-pseudorandom-generators"><i class="fa fa-check"></i><b>19.4.3</b> Usefulness of pseudorandom generators</a></li></ul></li><li class="chapter" data-level="19.5" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#mathbfpmathbfnp-and-mathbfbpp-vs-mathbfp"><i class="fa fa-check"></i><b>19.5</b> \mathbf{P}=\mathbf{NP} and \mathbf{BPP} vs \mathbf{P}</a></li><li class="chapter" data-level="19.6" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#non-constructive-existence-of-pseudorandom-generators-advanced-optional"><i class="fa fa-check"></i><b>19.6</b> Non-constructive existence of pseudorandom generators (advanced, optional)</a></li><li class="chapter" data-level="19.7" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#exercises"><i class="fa fa-check"></i><b>19.7</b> Exercises</a></li><li class="chapter" data-level="19.8" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#modelrandbibnotes"><i class="fa fa-check"></i><b>19.8</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="20" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html"><i class="fa fa-check"></i><b>20</b> Cryptography</a><ul><li class="chapter" data-level="20.1" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#classical-cryptosystems"><i class="fa fa-check"></i><b>20.1</b> Classical cryptosystems</a></li><li class="chapter" data-level="20.2" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#defining-encryption"><i class="fa fa-check"></i><b>20.2</b> Defining encryption</a></li><li class="chapter" data-level="20.3" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#defining-security-of-encryption"><i class="fa fa-check"></i><b>20.3</b> Defining security of encryption</a></li><li class="chapter" data-level="20.4" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#perfect-secrecy"><i class="fa fa-check"></i><b>20.4</b> Perfect secrecy</a><ul><li class="chapter" data-level="20.4.1" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#example-perfect-secrecy-in-the-battlefield"><i class="fa fa-check"></i><b>20.4.1</b> Example: Perfect secrecy in the battlefield</a></li><li class="chapter" data-level="20.4.2" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#constructing-perfectly-secret-encryption"><i class="fa fa-check"></i><b>20.4.2</b> Constructing perfectly secret encryption</a></li></ul></li><li class="chapter" data-level="20.5" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#necessity-of-long-keys"><i class="fa fa-check"></i><b>20.5</b> Necessity of long keys</a></li><li class="chapter" data-level="20.6" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#computational-secrecy"><i class="fa fa-check"></i><b>20.6</b> Computational secrecy</a><ul><li class="chapter" data-level="20.6.1" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#stream-ciphers-or-the-derandomized-one-time-pad"><i class="fa fa-check"></i><b>20.6.1</b> Stream ciphers or the derandomized one-time pad</a></li></ul></li><li class="chapter" data-level="20.7" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#computational-secrecy-and-mathbfnp"><i class="fa fa-check"></i><b>20.7</b> Computational secrecy and \mathbf{NP}</a></li><li class="chapter" data-level="20.8" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#public-key-cryptography"><i class="fa fa-check"></i><b>20.8</b> Public key cryptography</a><ul><li class="chapter" data-level="20.8.1" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#defining-public-key-encryption"><i class="fa fa-check"></i><b>20.8.1</b> Defining public key encryption</a></li><li class="chapter" data-level="20.8.2" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#diffie-hellman-key-exchange"><i class="fa fa-check"></i><b>20.8.2</b> Diffie-Hellman key exchange</a></li></ul></li><li class="chapter" data-level="20.9" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#other-security-notions"><i class="fa fa-check"></i><b>20.9</b> Other security notions</a></li><li class="chapter" data-level="20.10" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#magic"><i class="fa fa-check"></i><b>20.10</b> Magic</a><ul><li class="chapter" data-level="20.10.1" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#zero-knowledge-proofs"><i class="fa fa-check"></i><b>20.10.1</b> Zero knowledge proofs</a></li><li class="chapter" data-level="20.10.2" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#fully-homomorphic-encryption"><i class="fa fa-check"></i><b>20.10.2</b> Fully homomorphic encryption</a></li><li class="chapter" data-level="20.10.3" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#multiparty-secure-computation"><i class="fa fa-check"></i><b>20.10.3</b> Multiparty secure computation</a></li></ul></li><li class="chapter" data-level="20.11" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#exercises"><i class="fa fa-check"></i><b>20.11</b> Exercises</a></li><li class="chapter" data-level="20.12" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#bibliographical-notes"><i class="fa fa-check"></i><b>20.12</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="21" data-path="lec_24_proofs.html"><a href="lec_24_proofs.html"><i class="fa fa-check"></i><b>21</b> Proofs and algorithms</a><ul><li class="chapter" data-level="21.1" data-path="lec_24_proofs.html"><a href="lec_24_proofs.html#exercises"><i class="fa fa-check"></i><b>21.1</b> Exercises</a></li><li class="chapter" data-level="21.2" data-path="lec_24_proofs.html"><a href="lec_24_proofs.html#bibliographical-notes"><i class="fa fa-check"></i><b>21.2</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="22" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html"><i class="fa fa-check"></i><b>22</b> Quantum computing</a><ul><li class="chapter" data-level="22.1" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#the-double-slit-experiment"><i class="fa fa-check"></i><b>22.1</b> The double slit experiment</a></li><li class="chapter" data-level="22.2" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantum-amplitudes"><i class="fa fa-check"></i><b>22.2</b> Quantum amplitudes</a><ul><li class="chapter" data-level="22.2.1" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#linear-algebra-quick-review"><i class="fa fa-check"></i><b>22.2.1</b> Linear algebra quick review</a></li></ul></li><li class="chapter" data-level="22.3" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#bellineqsec"><i class="fa fa-check"></i><b>22.3</b> Bell’s Inequality</a></li><li class="chapter" data-level="22.4" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantum-weirdness"><i class="fa fa-check"></i><b>22.4</b> Quantum weirdness</a></li><li class="chapter" data-level="22.5" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantum-computing-and-computation---an-executive-summary."><i class="fa fa-check"></i><b>22.5</b> Quantum computing and computation - an executive summary.</a></li><li class="chapter" data-level="22.6" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantum-systems"><i class="fa fa-check"></i><b>22.6</b> Quantum systems</a><ul><li class="chapter" data-level="22.6.1" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantum-amplitudes-1"><i class="fa fa-check"></i><b>22.6.1</b> Quantum amplitudes</a></li><li class="chapter" data-level="22.6.2" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantum-systems-an-executive-summary"><i class="fa fa-check"></i><b>22.6.2</b> Quantum systems: an executive summary</a></li></ul></li><li class="chapter" data-level="22.7" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#analysis-of-bells-inequality-optional"><i class="fa fa-check"></i><b>22.7</b> Analysis of Bell’s Inequality (optional)</a></li><li class="chapter" data-level="22.8" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantum-computation"><i class="fa fa-check"></i><b>22.8</b> Quantum computation</a><ul><li class="chapter" data-level="22.8.1" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantum-circuits"><i class="fa fa-check"></i><b>22.8.1</b> Quantum circuits</a></li><li class="chapter" data-level="22.8.2" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#qnand-circ-programs-optional"><i class="fa fa-check"></i><b>22.8.2</b> QNAND-CIRC programs (optional)</a></li><li class="chapter" data-level="22.8.3" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#uniform-computation"><i class="fa fa-check"></i><b>22.8.3</b> Uniform computation</a></li></ul></li><li class="chapter" data-level="22.9" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#physically-realizing-quantum-computation"><i class="fa fa-check"></i><b>22.9</b> Physically realizing quantum computation</a></li><li class="chapter" data-level="22.10" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#shors-algorithm-hearing-the-shape-of-prime-factors"><i class="fa fa-check"></i><b>22.10</b> Shor’s Algorithm: Hearing the shape of prime factors</a><ul><li class="chapter" data-level="22.10.1" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#period-finding"><i class="fa fa-check"></i><b>22.10.1</b> Period finding</a></li><li class="chapter" data-level="22.10.2" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#shors-algorithm-a-birds-eye-view"><i class="fa fa-check"></i><b>22.10.2</b> Shor’s Algorithm: A bird’s eye view</a></li></ul></li><li class="chapter" data-level="22.11" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantum-fourier-transform-advanced-optional"><i class="fa fa-check"></i><b>22.11</b> Quantum Fourier Transform (advanced, optional)</a><ul><li class="chapter" data-level="22.11.1" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantum-fourier-transform-over-the-boolean-cube-simons-algorithm"><i class="fa fa-check"></i><b>22.11.1</b> Quantum Fourier Transform over the Boolean Cube: Simon’s Algorithm</a></li><li class="chapter" data-level="22.11.2" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#from-fourier-to-period-finding-simons-algorithm-advanced-optional"><i class="fa fa-check"></i><b>22.11.2</b> From Fourier to Period finding: Simon’s Algorithm (advanced, optional)</a></li><li class="chapter" data-level="22.11.3" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#from-simon-to-shor-advanced-optional"><i class="fa fa-check"></i><b>22.11.3</b> From Simon to Shor (advanced, optional)</a></li></ul></li><li class="chapter" data-level="22.12" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#exercises"><i class="fa fa-check"></i><b>22.12</b> Exercises</a></li><li class="chapter" data-level="22.13" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantumbibnotessec"><i class="fa fa-check"></i><b>22.13</b> Bibliographical notes</a></li></ul></li><li class="divider"></li>
</ul>
<!--bookdown:toc2:end-->
      </nav>
    </div>

    <div class="book-header" role="navigation">
      <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Modeling running time</a>
      </h1>
    </div>

    <div class="book-body">
      <div class="body-inner">


        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<!--bookdown:toc:end-->
<!--bookdown:body:start-->

<div  class="section level2">

<!-- link to pdf version -->


<!-- start of header referring to comments -->
<div><p></p><p style="color:#871640;"><i class="fas fa-wrench"></i> See any bugs/typos/confusing explanations? <a href="https://github.com/boazbk/tcs/issues/new">Open a GitHub issue</a>. You can also <a href="#commentform">comment below</a> <i class="fas fa-wrench"></i></p></div>



<div><p style="color:#871640;">&#x2605; See also the <a id="pdflink" href='https://files.boazbarak.org/introtcs/lec_11_running_time.pdf'><b>PDF version of this chapter</b></a> (better formatting/references) &#x2605;</p></div>

<!-- end of header referring to comments -->

<!--- start of actual content -->

<h1 id="chapmodelruntime" data-number="12">Modeling running time</h1>
<div id="section" class="objectives" name="Objectives">
<ul>
<li>Formally modeling running time, and in particular notions such as <span><span class="math inline">\(O(n)\)</span></span> or <span><span class="math inline">\(O(n^3)\)</span></span> time algorithms.<br />
</li>
<li>The classes <span><span class="math inline">\(\mathbf{P}\)</span></span> and <span><span class="math inline">\(\mathbf{EXP}\)</span></span> modelling polynomial and exponential time respectively.<br />
</li>
<li>The <em>time hierarchy theorem</em>, that in particular says that for every <span><span class="math inline">\(k \geq 1\)</span></span> there are functions we <em>can</em> compute in <span><span class="math inline">\(O(n^{k+1})\)</span></span> time but <em>can not</em> compute in <span><span class="math inline">\(O(n^k)\)</span></span> time.</li>
<li>The class <span><span class="math inline">\(\mathbf{P_{/poly}}\)</span></span> of <em>non uniform</em> computation and the result that <span><span class="math inline">\(\mathbf{P} \subseteq \mathbf{P_{/poly}}\)</span></span></li>
</ul>
</div>
<blockquote>
<p>“When the measure of the problem-size is reasonable and when the sizes assume values arbitrarily large, an asymptotic estimate of … the order of difficulty of [an] algorithm .. is theoretically important. It cannot be rigged by making the algorithm artificially difficult for smaller sizes”, Jack Edmonds, “Paths, Trees, and Flowers”, 1963</p>
</blockquote>
<blockquote>
<div class="quote" name="Quote 12">
<p><em>Max Newman:</em> It is all very well to say that a machine could … do this or that, but … what about the time it would take to do it?</p>
<p><em>Alan Turing:</em> To my mind this time factor is the one question which will involve all the real technical difficulty.</p>
<p>BBC radio panel on “Can automatic Calculating Machines Be Said to Think?”, 1952</p>
</div>
</blockquote>
<p>In <a href='lec_10_efficient_alg.html#chapefficient'>Chapter 11</a> we saw examples of efficient algorithms, and made some claims about their running time, but did not give a mathematically precise definition for this concept. We do so in this chapter, using the models of Turing machines and RAM machines (or equivalently NAND-TM and NAND-RAM) we have seen before. The running time of an algorithm is not a fixed number since any non-trivial algorithm will take longer to run on longer inputs. Thus, what we want to measure is the <em>dependence</em> between the number of steps the algorithms takes and the length of the input. In particular we care about the distinction between algorithms that take at most <em>polynomial time</em> (i.e., <span><span class="math inline">\(O(n^c)\)</span></span> time for some constant <span><span class="math inline">\(c\)</span></span>) and problems for which every algorithm requires at least <em>exponential time</em> (i.e., <span><span class="math inline">\(\Omega(2^{n^c})\)</span></span> for some <span><span class="math inline">\(c\)</span></span>). As mentioned in Edmond’s quote in <a href='lec_10_efficient_alg.html#chapefficient'>Chapter 11</a>, the difference between these two can sometimes be as important as the difference between being computable and uncomputable.</p>
<figure>
<img src="../figure/runtimeoverview.png" alt="12.1: Overview of the results of this chapter." id="runtimeoverviewfig" /><figcaption>12.1: Overview of the results of this chapter.</figcaption>
</figure>
<p>In this chapter we formally define the notion of a function being computable in <span><span class="math inline">\(T(n)\)</span></span> time where <span><span class="math inline">\(T\)</span></span> is some function mapping the length of the input to a bound on the number of computation steps. We then do the following (see also <a href='#runtimeoverviewfig'>Figure 12.1</a>):</p>
<ul>
<li><p>Define the class <span><span class="math inline">\(\mathbf{P}\)</span></span> of Boolean functions that can be computed in polynomial time and its superset <span><span class="math inline">\(\mathbf{EXP}\)</span></span> of functions that can be computed in exponential time.</p></li>
<li><p>Show that the time to compute a function using a Turing Machine and using a RAM machine (or NAND-RAM program) is <em>polynomially related</em> which in particular means that the classes <span><span class="math inline">\(\mathbf{P}\)</span></span> and <span><span class="math inline">\(\mathbf{EXP}\)</span></span> can be equivalently defined using either Turing Machines or RAM machines / NAND-RAM programs.</p></li>
<li><p>Give an <em>efficient</em> universal NAND-RAM program and use this to establish the <em>time hierarchy theorem</em> that in particular implies that <span><span class="math inline">\(\mathbf{P} \subsetneq \mathbf{EXP}\)</span></span>.</p></li>
<li><p>We relate the notions defined here to the <em>non uniform</em> models of Boolean circuits and NAND-CIRC programs defined in <a href='lec_03_computation.html#compchap'>Chapter 3</a>. We define <span><span class="math inline">\(\mathbf{P_{/poly}}\)</span></span> to be the class of functions computed by a <em>sequence</em> of polynomial-sized circuits. We prove that <span><span class="math inline">\(\mathbf{P} \subseteq \mathbf{P_{/poly}}\)</span></span> and that <span><span class="math inline">\(\mathbf{P_{/poly}}\)</span></span> contains <em>uncomputable</em> functions.</p></li>
</ul>
<h2 id="formally-defining-running-time" data-number="12.1">Formally defining running time</h2>
<p>Our models of computation such Turing Machines, NAND-TM and NAND-RAM programs and others all operate by executing a sequence of instructions on an input one step at a time. We can define the <em>running time</em> of an algorithm <span><span class="math inline">\(M\)</span></span> in one of these models by measuring the number of steps <span><span class="math inline">\(M\)</span></span> takes on input <span><span class="math inline">\(x\)</span></span> as a <em>function of the length <span><span class="math inline">\(|x|\)</span></span> of the input</em>. We start by defining running time with respect to Turing Machines:</p>
<div id="time-TM-def" class="definition" title="Running time (Turing Machines)" name="Definition 12.1 (Running time (Turing Machines)) ">
<p>Let <span><span class="math inline">\(T:\N \rightarrow \N\)</span></span> be some function mapping natural numbers to natural numbers. We say that a function <span><span class="math inline">\(F:\{0,1\}^* \rightarrow \{0,1\}^*\)</span></span> is <em>computable in <span><span class="math inline">\(T(n)\)</span></span> Single-Tape-Turing-Machine time (TM-time for short)</em> if there exists a Turing Machine <span><span class="math inline">\(M\)</span></span> such that for every sufficiently large <span><span class="math inline">\(n\)</span></span> and every <span><span class="math inline">\(x\in \{0,1\}^n\)</span></span>, when given input <span><span class="math inline">\(x\)</span></span>, the machine <span><span class="math inline">\(M\)</span></span> halts after executing at most <span><span class="math inline">\(T(n)\)</span></span> steps and outputs <span><span class="math inline">\(F(x)\)</span></span>.</p>
<p>We define <span><span class="math inline">\(\ensuremath{\mathit{TIME}}_{\mathsf{TM}}(T(n))\)</span></span> to be the set of Boolean functions (functions mapping <span><span class="math inline">\(\{0,1\}^*\)</span></span> to <span><span class="math inline">\(\{0,1\}\)</span></span>) that are computable in <span><span class="math inline">\(T(n)\)</span></span> TM time.</p>
</div>
<div id="formaldefinetime" class="bigidea" name="Bigidea 16">
<p>For a function <span><span class="math inline">\(F:\{0,1\}^* \rightarrow \{0,1\}\)</span></span> and <span><span class="math inline">\(T:\N \rightarrow \N\)</span></span>, we can formally define what it means for <span><span class="math inline">\(F\)</span></span> to be computable in time at most <span><span class="math inline">\(T(n)\)</span></span> where <span><span class="math inline">\(n\)</span></span> is the size of the input.</p>
</div>
<div class="pause" name="Pause">
<p><a href='#time-TM-def'>Definition 12.1</a> is not very complicated but is one of the most important definitions of this book. As usual, <span><span class="math inline">\(\ensuremath{\mathit{TIME}}_{\mathsf{TM}}(T(n))\)</span></span> is a class of <em>functions</em>, not of <em>machines</em>. If <span><span class="math inline">\(M\)</span></span> is a Turing Machine then a statement such as “<span><span class="math inline">\(M\)</span></span> is a member of <span><span class="math inline">\(\ensuremath{\mathit{TIME}}_{\mathsf{TM}}(n^2)\)</span></span>” does not make sense.</p>
</div>
<p>The relaxation of considering only “sufficiently large” <span><span class="math inline">\(n\)</span></span>’s is not very important but it is convenient since it allows us to avoid dealing explicitly with un-interesting “edge cases”. In most cases we will anyway be interested in determining running time only up to constant and even polynomial factors. Note that we can always compute a function on a finite number of inputs using a lookup table.</p>
<p>While the notion of being computable within a certain running time can be defined for every function, the class <span><span class="math inline">\(\ensuremath{\mathit{TIME}}_{\mathsf{TM}}(T(n))\)</span></span> is a class of <em>Boolean functions</em> that have a single bit of output. This choice is not very important, but is made for simplicity and convenience later on. In fact, every non-Boolean function has a computationally equivalent Boolean variant, see <a href='#boolex'>Exercise 12.3</a>.</p>
<div id="timeboundexample" class="solvedexercise" title="Example of time bounds" name="Solvedexercise 12.1 (Example of time bounds) ">
<p>Prove that <span><span class="math inline">\(\ensuremath{\mathit{TIME}}_{\mathsf{TM}}(10\cdot n^3) \subseteq \ensuremath{\mathit{TIME}}_{\mathsf{TM}}(2^n)\)</span></span>.</p>
</div>
<figure>
<img src="../figure/exampletimebounds.png" alt="12.2: Comparing T(n)=10n^3 with T&#39;(n) = 2^n (on the right figure the Y axis is in log scale). Since for every large enough n, T&#39;(n) \geq T(n), \ensuremath{\mathit{TIME}}_{\mathsf{TM}}(T(n)) \subseteq \ensuremath{\mathit{TIME}}_{\mathsf{TM}}(T&#39;(n))." id="examplefimeboundsfig" class="margin" /><figcaption>12.2: Comparing <span><span class="math inline">\(T(n)=10n^3\)</span></span> with <span><span class="math inline">\(T&#39;(n) = 2^n\)</span></span> (on the right figure the Y axis is in log scale). Since for every large enough <span><span class="math inline">\(n\)</span></span>, <span><span class="math inline">\(T&#39;(n) \geq T(n)\)</span></span>, <span><span class="math inline">\(\ensuremath{\mathit{TIME}}_{\mathsf{TM}}(T(n)) \subseteq \ensuremath{\mathit{TIME}}_{\mathsf{TM}}(T&#39;(n))\)</span></span>.</figcaption>
</figure>
<div class="solution" data-ref="timeboundexample" name="Proofidea 20.6.1">
<p>The proof is illustrated in <a href='#examplefimeboundsfig'>Figure 12.2</a>. Suppose that <span><span class="math inline">\(F\in \ensuremath{\mathit{TIME}}_{\mathsf{TM}}(10\cdot n^3)\)</span></span> and hence there some number <span><span class="math inline">\(N_0\)</span></span> and a machine <span><span class="math inline">\(M\)</span></span> such that for every <span><span class="math inline">\(n&gt; N_0\)</span></span>, and <span><span class="math inline">\(x\in \{0,1\}^*\)</span></span>, <span><span class="math inline">\(M(x)\)</span></span> outputs <span><span class="math inline">\(F(x)\)</span></span> within at most <span><span class="math inline">\(10\cdot n^3\)</span></span> steps. Since <span><span class="math inline">\(10\cdot n^3 = o(2^n)\)</span></span>, there is some number <span><span class="math inline">\(N_1\)</span></span> such that for every <span><span class="math inline">\(n&gt;N_1\)</span></span>, <span><span class="math inline">\(10\cdot n^3 &lt; 2^n\)</span></span>. Hence for every <span><span class="math inline">\(n &gt; \max\{ N_0, N_1 \}\)</span></span>, <span><span class="math inline">\(M(x)\)</span></span> will output <span><span class="math inline">\(F(x)\)</span></span> within at most <span><span class="math inline">\(2^n\)</span></span> steps, just demonstrating that <span><span class="math inline">\(F \in \ensuremath{\mathit{TIME}}_{\mathsf{TM}}(2^n)\)</span></span>.</p>
</div>
<h3 id="polynomial-and-exponential-time" data-number="12.1.1">Polynomial and Exponential Time</h3>
<p>Unlike the notion of computability, the exact running time can be a function of the model we use. However, it turns out that if we only care about “coarse enough” resolution (as will most often be the case) then the choice of the model, whether Turing Machines, RAM machines, NAND-TM/NAND-RAM programs, or C/Python programs, does not matter. This is known as the <em>extended</em> Church-Turing Thesis. Specifically we will mostly care about the difference between <em>polynomial</em> and <em>exponential</em> time.</p>
<p>The two main time complexity classes we will be interested in are the following:</p>
<ul>
<li><p><strong>Polynomial time:</strong> A function <span><span class="math inline">\(F:\{0,1\}^* \rightarrow \{0,1\}\)</span></span> is <em>computable in polynomial time</em> if it is in the class <span><span class="math inline">\(\mathbf{P} = \cup_{c\in \{1,2,3,\ldots \}} \ensuremath{\mathit{TIME}}_{\mathsf{TM}}(n^c)\)</span></span>. That is, <span><span class="math inline">\(F\in \mathbf{P}\)</span></span> if there is an algorithm to compute <span><span class="math inline">\(F\)</span></span> that runs in time at most <em>polynomial</em> (i.e., at most <span><span class="math inline">\(n^c\)</span></span> for some constant <span><span class="math inline">\(c\)</span></span>) in the length of the input.</p></li>
<li><p><strong>Exponential time:</strong> A function <span><span class="math inline">\(F:\{0,1\}^* \rightarrow \{0,1\}\)</span></span> is <em>computable in exponential time</em> if it is in the class <span><span class="math inline">\(\mathbf{EXP} = \cup_{c\in \{1,2,3,\ldots \}} \ensuremath{\mathit{TIME}}_{\mathsf{TM}}(2^{n^c})\)</span></span>. That is, <span><span class="math inline">\(F\in \mathbf{EXP}\)</span></span> if there is an algorithm to compute <span><span class="math inline">\(F\)</span></span> that runs in time at most <em>exponential</em> (i.e., at most <span><span class="math inline">\(2^{n^c}\)</span></span> for some constant <span><span class="math inline">\(c\)</span></span>) in the length of the input.</p></li>
</ul>
<p>In other words, these are defined as follows:</p>
<div id="PandEXPdef" class="definition" title="$\mathbf{P}$ and $\mathbf{EXP}$" name="Definition 12.2 ($\mathbf{P}$ and $\mathbf{EXP}$) ">
<p>Let <span><span class="math inline">\(F:\{0,1\}^* \rightarrow \{0,1\}\)</span></span>. We say that <span><span class="math inline">\(F\in \mathbf{P}\)</span></span> if there is a polynomial <span><span class="math inline">\(p:\N \rightarrow \R\)</span></span> and a Turing Machine <span><span class="math inline">\(M\)</span></span> such that for every <span><span class="math inline">\(x\in \{0,1\}^*\)</span></span>, when given input <span><span class="math inline">\(x\)</span></span>, the Turing machine halts within at most <span><span class="math inline">\(p(|x|)\)</span></span> steps and outputs <span><span class="math inline">\(F(x)\)</span></span>.</p>
<p>We say that <span><span class="math inline">\(F\in \mathbf{EXP}\)</span></span> if there is a polynomial <span><span class="math inline">\(p:\N \rightarrow \R\)</span></span> and a Turing Machine <span><span class="math inline">\(M\)</span></span> such that for every <span><span class="math inline">\(x\in \{0,1\}^*\)</span></span>, when given input <span><span class="math inline">\(x\)</span></span>, <span><span class="math inline">\(M\)</span></span> halts within at most <span><span class="math inline">\(2^{p(|x|)}\)</span></span> steps and outputs <span><span class="math inline">\(F(x)\)</span></span>.</p>
</div>
<div class="pause" name="Pause 12.1.1">
<p>Please take the time to make sure you understand these definitions. In particular, sometimes students think of the class <span><span class="math inline">\(\mathbf{EXP}\)</span></span> as corresponding to functions that are <em>not</em> in <span><span class="math inline">\(\mathbf{P}\)</span></span>. However, this is not the case. If <span><span class="math inline">\(F\)</span></span> is in <span><span class="math inline">\(\mathbf{EXP}\)</span></span> then it <em>can</em> be computed in exponential time. This does not mean that it cannot be computed in polynomial time as well.</p>
</div>
<div id="diffdefofP" class="solvedexercise" title="Differerent definitions of $\mathbf{P}$" name="Solvedexercise 12.2 (Differerent definitions of $\mathbf{P}$) ">
<p>Prove that <span><span class="math inline">\(\mathbf{P}\)</span></span> as defined in <a href='#PandEXPdef'>Definition 12.2</a> is equal to <span><span class="math inline">\(\cup_{c\in \{1,2,3,\ldots \}} \ensuremath{\mathit{TIME}}_{\mathsf{TM}}(n^c)\)</span></span></p>
</div>
<div class="solution" data-ref="diffdefofP" name="Solution 12.1.1">
<p>To show these two sets are equal we need to show that <span><span class="math inline">\(\mathbf{P} \subseteq \cup_{c\in \{1,2,3,\ldots \}} \ensuremath{\mathit{TIME}}_{\mathsf{TM}}(n^c)\)</span></span> and <span><span class="math inline">\(\cup_{c\in \{1,2,3,\ldots \}} \ensuremath{\mathit{TIME}}_{\mathsf{TM}}(n^c) \subseteq \mathbf{P}\)</span></span>. We start with the former inclusion. Suppose that <span><span class="math inline">\(F \in \mathbf{P}\)</span></span>. Then there is some polynomial <span><span class="math inline">\(p:\N \rightarrow \R\)</span></span> and a Turing machine <span><span class="math inline">\(M\)</span></span> such that <span><span class="math inline">\(M\)</span></span> computes <span><span class="math inline">\(F\)</span></span> and <span><span class="math inline">\(M\)</span></span> halts on every input <span><span class="math inline">\(x\)</span></span> within at most <span><span class="math inline">\(p(|x|)\)</span></span> steps. We can write the polynomial <span><span class="math inline">\(p:\N \rightarrow \R\)</span></span> in the form <span><span class="math inline">\(p(n) = \sum_{i=0}^d a_i n^i\)</span></span> where <span><span class="math inline">\(a_0,\ldots,a_d \in \R\)</span></span>, and we assume that <span><span class="math inline">\(a_d\)</span></span> is nonzero (or otherwise we just let <span><span class="math inline">\(d\)</span></span> correspond to the largest number such that <span><span class="math inline">\(a_d\)</span></span> is nonzero). The <em>degree</em> if <span><span class="math inline">\(p\)</span></span> the number <span><span class="math inline">\(d\)</span></span>. Since <span><span class="math inline">\(n^d = o(n^{d+1})\)</span></span>, no matter what is the coefficient <span><span class="math inline">\(a_d\)</span></span>, for large enough <span><span class="math inline">\(n\)</span></span>, <span><span class="math inline">\(p(n) &lt; n^{d+1}\)</span></span> which means that the Turing machine <span><span class="math inline">\(M\)</span></span> will halt on inputs of length <span><span class="math inline">\(n\)</span></span> within fewer than <span><span class="math inline">\(n^{d+1}\)</span></span> steps, and hence <span><span class="math inline">\(F \in \ensuremath{\mathit{TIME}}_{\mathsf{TM}}(n^{d+1}) \subseteq \cup_{c\in \{1,2,3,\ldots \}} \ensuremath{\mathit{TIME}}_{\mathsf{TM}}(n^c)\)</span></span>.</p>
<p>For the second inclusion, suppose that <span><span class="math inline">\(F \in \cup_{c\in \{1,2,3,\ldots \}} \ensuremath{\mathit{TIME}}_{\mathsf{TM}}(n^c)\)</span></span>. Then there is some positive <span><span class="math inline">\(c \in \N\)</span></span> such that <span><span class="math inline">\(F \in \ensuremath{\mathit{TIME}}_{\mathsf{TM}}(n^c)\)</span></span> which means that there is a Turing Machine <span><span class="math inline">\(M\)</span></span> and some number <span><span class="math inline">\(N_0\)</span></span> such that <span><span class="math inline">\(M\)</span></span> computes <span><span class="math inline">\(F\)</span></span> and for every <span><span class="math inline">\(n&gt;N_0\)</span></span>, <span><span class="math inline">\(M\)</span></span> halts on length <span><span class="math inline">\(n\)</span></span> inputs within at most <span><span class="math inline">\(n^c\)</span></span> steps. Let <span><span class="math inline">\(T_0\)</span></span> be the maximum number of steps that <span><span class="math inline">\(M\)</span></span> takes on inputs of length at most <span><span class="math inline">\(N_0\)</span></span>. Then if we define the polynomial <span><span class="math inline">\(p(n) = n^c + T_0\)</span></span> then we see that <span><span class="math inline">\(M\)</span></span> halts on every input <span><span class="math inline">\(x\)</span></span> within at most <span><span class="math inline">\(p(|x|)\)</span></span> steps and hence the existence of <span><span class="math inline">\(M\)</span></span> demonstrates that <span><span class="math inline">\(F\in \mathbf{P}\)</span></span>.</p>
</div>
<p>Since exponential time is much larger than polynomial time, <span><span class="math inline">\(\mathbf{P}\subseteq \mathbf{EXP}\)</span></span>. All of the problems we listed in <a href='lec_10_efficient_alg.html#chapefficient'>Chapter 11</a> are in <span><span class="math inline">\(\mathbf{EXP}\)</span></span>, but as we’ve seen, for some of them there are much better algorithms that demonstrate that they are in fact in the smaller class <span><span class="math inline">\(\mathbf{P}\)</span></span>.</p>
<table>
<thead>
<tr class="header">
<th><span><span class="math inline">\(\mathbf{P}\)</span></span></th>
<th><span><span class="math inline">\(\mathbf{EXP}\)</span></span> (but not known to be in <span><span class="math inline">\(\mathbf{P}\)</span></span>)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Shortest path</td>
<td>Longest Path</td>
</tr>
<tr class="even">
<td>Min cut</td>
<td>Max cut</td>
</tr>
<tr class="odd">
<td>2SAT</td>
<td>3SAT</td>
</tr>
<tr class="even">
<td>Linear eqs</td>
<td>Quad. eqs</td>
</tr>
<tr class="odd">
<td>Zerosum</td>
<td>Nash</td>
</tr>
<tr class="even">
<td>Determinant</td>
<td>Permanent</td>
</tr>
<tr class="odd">
<td>Primality</td>
<td>Factoring</td>
</tr>
</tbody>
</table>
<p>Table : A table of the examples from <a href='lec_10_efficient_alg.html#chapefficient'>Chapter 11</a>. All these problems are in <span><span class="math inline">\(\mathbf{EXP}\)</span></span> but the only the ones on the left column are currently known to be in <span><span class="math inline">\(\mathbf{P}\)</span></span> as well (i.e., they have a polynomial-time algorithm). See also <a href='#PvsEXPfig'>Figure 12.3</a>.</p>
<figure>
<img src="../figure/PvsEXP.png" alt="12.3: Some examples of problems that are known to be in \mathbf{P} and problems that are known to be in \mathbf{EXP} but not known whether or not they are in \mathbf{P}. Since both \mathbf{P} and \mathbf{EXP} are classes of Boolean functions, in this figure we always refer to the Boolean (i.e., Yes/No) variant of the problems." id="PvsEXPfig" class="margin" /><figcaption>12.3: Some examples of problems that are known to be in <span><span class="math inline">\(\mathbf{P}\)</span></span> and problems that are known to be in <span><span class="math inline">\(\mathbf{EXP}\)</span></span> but not known whether or not they are in <span><span class="math inline">\(\mathbf{P}\)</span></span>. Since both <span><span class="math inline">\(\mathbf{P}\)</span></span> and <span><span class="math inline">\(\mathbf{EXP}\)</span></span> are classes of Boolean functions, in this figure we always refer to the <em>Boolean</em> (i.e., Yes/No) variant of the problems.</figcaption>
</figure>
<div id="booleanversion" class="remark" title="Boolean versions of problems" name="Remark 12.3 (Boolean versions of problems) ">
<p>Many of the problems defined in <a href='lec_10_efficient_alg.html#chapefficient'>Chapter 11</a> correspond to <em>non Boolean</em> functions (functions with more than one bit of output) while <span><span class="math inline">\(\mathbf{P}\)</span></span> and <span><span class="math inline">\(\mathbf{EXP}\)</span></span> are sets of Boolean functions. However, for every non-Boolean function <span><span class="math inline">\(F\)</span></span> we can always define a computationally-equivalent Boolean function <span><span class="math inline">\(G\)</span></span> by letting <span><span class="math inline">\(G(x,i)\)</span></span> be the <span><span class="math inline">\(i\)</span></span>-th bit of <span><span class="math inline">\(F(x)\)</span></span> (see <a href='#boolex'>Exercise 12.3</a>). Hence the table above, as well as <a href='#PvsEXPfig'>Figure 12.3</a>, refer to the computationally-equivalent Boolean variants of these problems.</p>
</div>
<h2 id="modeling-running-time-using-ram-machines-nand-ram" data-number="12.2">Modeling running time using RAM Machines / NAND-RAM</h2>
<p>Turing Machines are a clean theoretical model of computation, but do not closely correspond to real-world computing architectures. The discrepancy between Turing Machines and actual computers does not matter much when we consider the question of which functions are <em>computable</em>, but can make a difference in the context of <em>efficiency</em>. Even a basic staple of undergraduate algorithms such as “merge sort” cannot be implemented on a Turing Machine in <span><span class="math inline">\(O(n\log n)\)</span></span> time (see <a href='#bibnotesrunningtime'>Section 12.8</a>). <em>RAM machines</em> (or equivalently, NAND-RAM programs) match more closely actual computing architecture and what we mean when we say <span><span class="math inline">\(O(n)\)</span></span> or <span><span class="math inline">\(O(n \log n)\)</span></span> algorithms in algorithms courses or whiteboard coding interviews. We can define running time with respect to NAND-RAM programs just as we did for Turing Machines.</p>
<div id="time-def" class="definition" title="Running time (RAM)" name="Definition 12.4 (Running time (RAM)) ">
<p>Let <span><span class="math inline">\(T:\N \rightarrow \N\)</span></span> be some function mapping natural numbers to natural numbers. We say that a function <span><span class="math inline">\(F:\{0,1\}^* \rightarrow \{0,1\}^*\)</span></span> is <em>computable in <span><span class="math inline">\(T(n)\)</span></span> RAM time (RAM-time for short)</em> if there exists a NAND-RAM program <span><span class="math inline">\(P\)</span></span> such that for every sufficiently large <span><span class="math inline">\(n\)</span></span> and every <span><span class="math inline">\(x\in \{0,1\}^n\)</span></span>, when given input <span><span class="math inline">\(x\)</span></span>, the program <span><span class="math inline">\(P\)</span></span> halts after executing at most <span><span class="math inline">\(T(n)\)</span></span> lines and outputs <span><span class="math inline">\(F(x)\)</span></span>.</p>
<p>We define <span><span class="math inline">\(\ensuremath{\mathit{TIME}}_{\mathsf{RAM}}(T(n))\)</span></span> to be the set of Boolean functions (functions mapping <span><span class="math inline">\(\{0,1\}^*\)</span></span> to <span><span class="math inline">\(\{0,1\}\)</span></span>) that are computable in <span><span class="math inline">\(T(n)\)</span></span> RAM time.</p>
</div>
<p>Because NAND-RAM programs correspond more closely to our natural notions of running time, we will use NAND-RAM as our “default” model of running time, and hence use <span><span class="math inline">\(\ensuremath{\mathit{TIME}}(T(n))\)</span></span> (without any subscript) to denote <span><span class="math inline">\(\ensuremath{\mathit{TIME}}_{\mathsf{RAM}}(T(n))\)</span></span>. However, it turns out that as long as we only care about the difference between exponential and polynomial time, this does not make much difference. The reason is that Turing Machines can simulate NAND-RAM programs with at most a polynomial overhead (see also <a href='#RAMTMsimulationfig'>Figure 12.4</a>):</p>
<div id="polyRAMTM-thm" class="theorem" title="Relating RAM and Turing machines" name="Theorem 12.5 (Relating RAM and Turing machines) ">
<p>Let <span><span class="math inline">\(T:\N \rightarrow \N\)</span></span> be a function such that <span><span class="math inline">\(T(n) \geq n\)</span></span> for every <span><span class="math inline">\(n\)</span></span> and the map <span><span class="math inline">\(n \mapsto T(n)\)</span></span> can be computed by a Turing machine in time <span><span class="math inline">\(O(T(n))\)</span></span>. Then <span>
<div class='myequationbox'><span class="math display">\[
\ensuremath{\mathit{TIME}}_{\mathsf{TM}}(T(n)) \subseteq \ensuremath{\mathit{TIME}}_{\mathsf{RAM}}(10\cdot T(n)) \subseteq \ensuremath{\mathit{TIME}}_{\mathsf{TM}}(T(n)^4) \;.  \;\;(12.1)
\]</span><a id='eqtmrambisimulation'></a></div></span></p>
</div>
<div class="pause" name="Pause 12.2">
<p>The technical details of <a href='#polyRAMTM-thm'>Theorem 12.5</a>, such as the condition that <span><span class="math inline">\(n \mapsto T(n)\)</span></span> is computable in <span><span class="math inline">\(O(T(n))\)</span></span> time or the constants <span><span class="math inline">\(10\)</span></span> and <span><span class="math inline">\(4\)</span></span> in <a href='#eqtmrambisimulation'>Equation 12.1</a> (which are not tight and can be improved), are not very important. In particular, all non pathological time bound functions we encounter in practice such as <span><span class="math inline">\(T(n)=n\)</span></span>, <span><span class="math inline">\(T(n)=n\log n\)</span></span>, <span><span class="math inline">\(T(n)=2^n\)</span></span> etc. will satisfy the conditions of <a href='#polyRAMTM-thm'>Theorem 12.5</a>, see also <a href='#nicefunctionsrem'>Remark 12.6</a>.</p>
<p>The main message of the theorem is Turing Machines and RAM machines are “roughly equivalent” in the sense that one can simulate the other with polynomial overhead. Similarly, while the proof involves some technical details, it’s not very deep or hard, and merely follows the simulation of RAM machines with Turing Machines we saw in <a href='lec_07_other_models.html#RAMTMequivalencethm'>Theorem 7.1</a> with more careful “book keeping”.</p>
</div>
<figure>
<img src="../figure/RAMTMsimulation.png" alt="12.4: The proof of  shows that we can simulate T steps of a Turing Machine with T steps of a NAND-RAM program, and can simulate T steps of a NAND-RAM program with o(T^4) steps of a Turing Machine. Hence \ensuremath{\mathit{TIME}}_{\mathsf{TM}}(T(n)) \subseteq \ensuremath{\mathit{TIME}}_{\mathsf{RAM}}(10\cdot T(n)) \subseteq \ensuremath{\mathit{TIME}}_{\mathsf{TM}}(T(n)^4)." id="RAMTMsimulationfig" class="margin" /><figcaption>12.4: The proof of <a href='#polyRAMTM-thm'>Theorem 12.5</a> shows that we can simulate <span><span class="math inline">\(T\)</span></span> steps of a Turing Machine with <span><span class="math inline">\(T\)</span></span> steps of a NAND-RAM program, and can simulate <span><span class="math inline">\(T\)</span></span> steps of a NAND-RAM program with <span><span class="math inline">\(o(T^4)\)</span></span> steps of a Turing Machine. Hence <span><span class="math inline">\(\ensuremath{\mathit{TIME}}_{\mathsf{TM}}(T(n)) \subseteq \ensuremath{\mathit{TIME}}_{\mathsf{RAM}}(10\cdot T(n)) \subseteq \ensuremath{\mathit{TIME}}_{\mathsf{TM}}(T(n)^4)\)</span></span>.</figcaption>
</figure>
<p>For example, by instantiating <a href='#polyRAMTM-thm'>Theorem 12.5</a> with <span><span class="math inline">\(T(n)=n^a\)</span></span> and using the fact that <span><span class="math inline">\(10n^a = o(n^{a+1})\)</span></span>, we see that <span><span class="math inline">\(\ensuremath{\mathit{TIME}}_{\mathsf{TM}}(n^a) \subseteq \ensuremath{\mathit{TIME}}_{\mathsf{RAM}}(n^{a+1}) \subseteq \ensuremath{\mathit{TIME}}_{\mathsf{TM}}(n^{4a+4})\)</span></span> which means that (by <a href='#diffdefofP'>Solvedexercise 12.2</a>) <span>
<div class='myequationbox'><span class="math display">\[
\mathbf{P} = \cup_{a = 1,2,\ldots} \ensuremath{\mathit{TIME}}_{\mathsf{TM}}(n^a) = \cup_{a = 1,2,\ldots} \ensuremath{\mathit{TIME}}_{\mathsf{RAM}}(n^a) \;.
\]</span></div></span> That is, we could have equally well defined <span><span class="math inline">\(\mathbf{P}\)</span></span> as the class of functions computable by <em>NAND-RAM programs</em> (instead of Turing Machines) that run in time polynomial in the length of the input. Similarly, by instantiating <a href='#polyRAMTM-thm'>Theorem 12.5</a> with <span><span class="math inline">\(T(n)=2^{n^a}\)</span></span> we see that the class <span><span class="math inline">\(\mathbf{EXP}\)</span></span> can also be defined as the set of functions computable by NAND-RAM programs in time at most <span><span class="math inline">\(2^{p(n)}\)</span></span> where <span><span class="math inline">\(p\)</span></span> is some polynomial. Similar equivalence results are known for many models including cellular automata, C/Python/Javascript programs, parallel computers, and a great many other models, which justifies the choice of <span><span class="math inline">\(\mathbf{P}\)</span></span> as capturing a technology-independent notion of tractability. (See <a href='#ECTTsec'>Section 12.3</a> for more discussion of this issue.) This equivalence between Turing machines and NAND-RAM (as well as other models) allows us to pick our favorite model depending on the task at hand (i.e., “have our cake and eat it too”) even when we study questions of efficiency, as long as we only care about the gap between <em>polynomial</em> and <em>exponential</em> time. When we want to <em>design</em> an algorithm, we can use the extra power and convenience afforded by NAND-RAM. When we want to <em>analyze</em> a program or prove a <em>negative result</em>, we can restrict our attention to Turing machines.</p>
<div id="polyvsnot" class="bigidea" name="Bigidea 17">
<p>All “reasonable” computational models are equivalent if we only care about the distinction between polynomial and exponential.</p>
</div>
<p>The adjective “reasonable” above refers to all scalable computational models that have been implemented, with the possible exception of <em>quantum computers</em>, see <a href='#ECTTsec'>Section 12.3</a> and <a href='lec_26_quantum_computing.html#quantumchap'>Chapter 22</a>.</p>
<div id="section-1" class="proofidea" data-ref="polyRAMTM-thm" name="Proofidea">
<p>The direction <span><span class="math inline">\(\ensuremath{\mathit{TIME}}_{\mathsf{TM}}(T(n)) \subseteq \ensuremath{\mathit{TIME}}_{\mathsf{RAM}}(10 \cdot T(n))\)</span></span> is not hard to show, since a NAND-RAM program <span><span class="math inline">\(P\)</span></span> can simulate a Turing Machine <span><span class="math inline">\(M\)</span></span> with constant overhead by storing the transition table of <span><span class="math inline">\(M\)</span></span> in an array (as is done in the proof of <a href='lec_08_uncomputability.html#universaltmthm'>Theorem 8.1</a>). Simulating every step of the Turing machine can be done in a constant number <span><span class="math inline">\(c\)</span></span> of steps of RAM, and it can be shown this constant <span><span class="math inline">\(c\)</span></span> is smaller than <span><span class="math inline">\(10\)</span></span>. Thus the heart of the theorem is to prove that <span><span class="math inline">\(\ensuremath{\mathit{TIME}}_{\mathsf{RAM}}(T(n)) \subseteq \ensuremath{\mathit{TIME}}_{\mathsf{TM}}(T(n)^4)\)</span></span>. This proof closely follows the proof of <a href='lec_07_other_models.html#RAMTMequivalencethm'>Theorem 7.1</a>, where we have shown that every function <span><span class="math inline">\(F\)</span></span> that is computable by a NAND-RAM program <span><span class="math inline">\(P\)</span></span> is computable by a Turing Machine (or equivalently a NAND-TM program) <span><span class="math inline">\(M\)</span></span>. To prove <a href='#polyRAMTM-thm'>Theorem 12.5</a>, we follow the exact same proof but just check that the overhead of the simulation of <span><span class="math inline">\(P\)</span></span> by <span><span class="math inline">\(M\)</span></span> is polynomial. The proof has many details, but is not deep. It is therefore much more important that you understand the <em>statement</em> of this theorem than its proof.</p>
</div>
<div class="proof" data-ref="polyRAMTM-thm" name="Proof 12.2">
<p>We only focus on the non-trivial direction <span><span class="math inline">\(\ensuremath{\mathit{TIME}}_{\mathsf{RAM}}(T(n)) \subseteq \ensuremath{\mathit{TIME}}_{\mathsf{TM}}(T(n)^4)\)</span></span>. Let <span><span class="math inline">\(F\in \ensuremath{\mathit{TIME}}_{\mathsf{RAM}}(T(n))\)</span></span>. <span><span class="math inline">\(F\)</span></span> can be computed in time <span><span class="math inline">\(T(n)\)</span></span> by some NAND-RAM program <span><span class="math inline">\(P\)</span></span> and we need to show that it can also be computed in time <span><span class="math inline">\(T(n)^4\)</span></span> by a Turing Machine <span><span class="math inline">\(M\)</span></span>. This will follow from showing that <span><span class="math inline">\(F\)</span></span> can be computed in time <span><span class="math inline">\(T(n)^4\)</span></span> by a NAND-TM program, since for every NAND-TM program <span><span class="math inline">\(Q\)</span></span> there is a Turing Machine <span><span class="math inline">\(M\)</span></span> simulating it such that each iteration of <span><span class="math inline">\(Q\)</span></span> corresponds to a single step of <span><span class="math inline">\(M\)</span></span>.</p>
<p>As mentioned above, we follow the proof of <a href='lec_07_other_models.html#RAMTMequivalencethm'>Theorem 7.1</a> (simulation of NAND-RAM programs using NAND-TM programs) and use the exact same simulation, but with a more careful accounting of the number of steps that the simulation costs. Recall, that the simulation of NAND-RAM works by “peeling off” features of NAND-RAM one by one, until we are left with NAND-TM.</p>
<p>We will not provide the full details but will present the main ideas used in showing that every feature of NAND-RAM can be simulated by NAND-TM with at most a polynomial overhead:</p>
<ol type="1">
<li><p>Recall that every NAND-RAM variable or array element can contain an integer between <span><span class="math inline">\(0\)</span></span> and <span><span class="math inline">\(T\)</span></span> where <span><span class="math inline">\(T\)</span></span> is the number of lines that have been executed so far. Therefore if <span><span class="math inline">\(P\)</span></span> is a NAND-RAM program that computes <span><span class="math inline">\(F\)</span></span> in <span><span class="math inline">\(T(n)\)</span></span> time, then on inputs of length <span><span class="math inline">\(n\)</span></span>, all integers used by <span><span class="math inline">\(P\)</span></span> are of magnitude at most <span><span class="math inline">\(T(n)\)</span></span>. This means that the largest value <code>i</code> can ever reach is at most <span><span class="math inline">\(T(n)\)</span></span> and so each one of <span><span class="math inline">\(P\)</span></span>’s variables can be thought of as an array of at most <span><span class="math inline">\(T(n)\)</span></span> indices, each of which holds a natural number of magnitude at most <span><span class="math inline">\(T(n)\)</span></span>. We let <span><span class="math inline">\(\ell = \ceil{\log T(n)}\)</span></span> be the number of bits needed to encode such numbers. (We can start off the simulation by computing <span><span class="math inline">\(T(n)\)</span></span> and <span><span class="math inline">\(\ell\)</span></span>.)</p></li>
<li><p>We can encode a NAND-RAM array of length <span><span class="math inline">\(\leq T(n)\)</span></span> containing numbers in <span><span class="math inline">\(\{0,\ldots, T(n)-1 \}\)</span></span> as an Boolean (i.e., NAND-TM) array of <span><span class="math inline">\(T(n)\ell =O(T(n)\log T(n))\)</span></span> bits, which we can also think of as a <em>two dimensional array</em> as we did in the proof of <a href='lec_07_other_models.html#RAMTMequivalencethm'>Theorem 7.1</a>. We encode a NAND-RAM scalar containing a number in <span><span class="math inline">\(\{0,\ldots, T(n)-1 \}\)</span></span> simply by a shorter NAND-TM array of <span><span class="math inline">\(\ell\)</span></span> bits.</p></li>
<li><p>We can simulate the two dimensional arrays using one-dimensional arrays of length <span><span class="math inline">\(T(n)\ell = O(T(n) \log T(n)\)</span></span>. All the arithmetic operations on integers use the grade-school algorithms, that take time that is polynomial in the number <span><span class="math inline">\(\ell\)</span></span> of bits of the integers, which is <span><span class="math inline">\(poly(\log T(n))\)</span></span> in our case. Hence we can simulate <span><span class="math inline">\(T(n)\)</span></span> steps of NAND-RAM with <span><span class="math inline">\(O(T(n)poly(\log T(n))\)</span></span> steps of a model that uses random access memory but only <em>Boolean-valued</em> one-dimensional arrays.</p></li>
<li><p>The most expensive step is to translate from random access memory to the sequential memory model of NAND-TM/Turing Machines. As we did in the proof of <a href='lec_07_other_models.html#RAMTMequivalencethm'>Theorem 7.1</a> (see <a href='lec_07_other_models.html#nandtmgorydetailssec'>Section 7.2</a>), we can simulate accessing an array <code>Foo</code> at some location encoded in an array <code>Bar</code> by:</p>
<ol type="a">
<li>Copying <code>Bar</code> to some temporary array <code>Temp</code></li>
<li>Having an array <code>Index</code> which is initially all zeros except <span><span class="math inline">\(1\)</span></span> at the first location.</li>
<li>Repeating the following until <code>Temp</code> encodes the number <span><span class="math inline">\(0\)</span></span>: <em>(Number of repetitions is at most <span><span class="math inline">\(T(n)\)</span></span>.)</em>
<ul>
<li>Decrease the number encoded temp by <span><span class="math inline">\(1\)</span></span>. <em>(Take number of steps polynomial in <span><span class="math inline">\(\ell = \ceil{\log T(n)}\)</span></span>.)</em></li>
<li>Decrease <code>i</code> until it is equal to <span><span class="math inline">\(0\)</span></span>. <em>(Take <span><span class="math inline">\(O(T(n)\)</span></span> steps.)</em></li>
<li>Scan <code>Index</code> until we reach the point in which it equals <span><span class="math inline">\(1\)</span></span> and then change this <span><span class="math inline">\(1\)</span></span> to <span><span class="math inline">\(0\)</span></span> and go one step further and write <span><span class="math inline">\(1\)</span></span> in this location. <em>(Takes <span><span class="math inline">\(O(T(n))\)</span></span> steps.)</em></li>
</ul></li>
<li>When we are done we know that if we scan <code>Index</code> until we reach the point in which <code>Index[i]</code><span><span class="math inline">\(=1\)</span></span> then <code>i</code> contains the value that was encoded by <code>Bar</code> <em>(Takes <span><span class="math inline">\(O(T(n)\)</span></span> steps.)</em></li>
</ol></li>
</ol>
<p>The total cost for each such operation is <span><span class="math inline">\(O(T(n)^2 + T(n)poly(\log T(n))) = O(T(n)^2)\)</span></span> steps.</p>
<p>In sum, we simulate a single step of NAND-RAM using <span><span class="math inline">\(O(T(n)^2 poly(\log T(n)))\)</span></span> steps of NAND-TM, and hence the total simulation time is <span><span class="math inline">\(O(T(n)^3 poly(\log T(n)))\)</span></span> which is smaller than <span><span class="math inline">\(T(n)^4\)</span></span> for sufficiently large <span><span class="math inline">\(n\)</span></span>.</p>
</div>
<div id="nicefunctionsrem" class="remark" title="Nice time bounds" name="Remark 12.6 (Nice time bounds) ">
<p>When considering general time bounds such we need to make sure to rule out some “pathological” cases such as functions <span><span class="math inline">\(T\)</span></span> that don’t give enough time for the algorithm to read the input, or functions where the time bound itself is uncomputable. We say that a function <span><span class="math inline">\(T:\N \rightarrow \N\)</span></span> is a <em>nice time bound function</em> (or nice function for short) if for every <span><span class="math inline">\(n\in \N\)</span></span>, <span><span class="math inline">\(T(n) \geq n\)</span></span> (i.e., <span><span class="math inline">\(T\)</span></span> allows enough time to read the input), for every <span><span class="math inline">\(n&#39; \geq n\)</span></span>, <span><span class="math inline">\(T(n&#39;) \geq T(n)\)</span></span> (i.e., <span><span class="math inline">\(T\)</span></span> allows more time on longer inputs), and the map <span><span class="math inline">\(F(x) = 1^{T(|x|)}\)</span></span> (i.e., mapping a string of length <span><span class="math inline">\(n\)</span></span> to a sequence of <span><span class="math inline">\(T(n)\)</span></span> ones) can be computed by a NAND-RAM program in <span><span class="math inline">\(O(T(n))\)</span></span> time.</p>
<p>All the “normal” time complexity bounds we encounter in applications such as <span><span class="math inline">\(T(n)= 100 n\)</span></span>, <span><span class="math inline">\(T(n) = n^2 \log n\)</span></span>,<span><span class="math inline">\(T(n) = 2^{\sqrt{n}}\)</span></span>, etc. are “nice”. Hence from now on we will only care about the class <span><span class="math inline">\(\ensuremath{\mathit{TIME}}(T(n))\)</span></span> when <span><span class="math inline">\(T\)</span></span> is a “nice” function. The computability condition is in particular typically easily satisfied. For example, for arithmetic functions such as <span><span class="math inline">\(T(n) = n^3\)</span></span>, we can typically compute the binary representation of <span><span class="math inline">\(T(n)\)</span></span> in time polynomial <em>in the number of bits</em> of <span><span class="math inline">\(T(n)\)</span></span> and hence poly-logarithmic in <span><span class="math inline">\(T(n)\)</span></span>. Hence the time to write the string <span><span class="math inline">\(1^{T(n)}\)</span></span> in such cases will be <span><span class="math inline">\(T(n) + poly(\log T(n)) = O(T(n))\)</span></span>.</p>
</div>
<h2 id="ECTTsec" data-number="12.3">Extended Church-Turing Thesis (discussion)</h2>
<p><a href='#polyRAMTM-thm'>Theorem 12.5</a> shows that the computational models of <em>Turing Machines</em> and <em>RAM Machines / NAND-RAM programs</em> are equivalent up to polynomial factors in the running time. Other examples of polynomially equivalent models include:</p>
<ul>
<li><p>All standard programming languages, including C/Python/JavaScript/Lisp/etc.</p></li>
<li><p>The <span><span class="math inline">\(\lambda\)</span></span> calculus (see also <a href='#bibnotesrunningtime'>Section 12.8</a>).</p></li>
<li><p>Cellular automata</p></li>
<li><p>Parallel computers</p></li>
<li><p>Biological computing devices such as DNA-based computers.</p></li>
</ul>
<p>The <em>Extended Church Turing Thesis</em> is the statement this is true for all physically realizable computing models. In other words, the extended Church Turing thesis says that for every <em>scalable computing device</em> <span><span class="math inline">\(C\)</span></span> (which has a finite description but can be in principle used to run computation on arbitrarily large inputs), there is some constant <span><span class="math inline">\(a\)</span></span> such that for every function <span><span class="math inline">\(F:\{0,1\}^* \rightarrow \{0,1\}\)</span></span> that <span><span class="math inline">\(C\)</span></span> can compute on <span><span class="math inline">\(n\)</span></span> length inputs using an <span><span class="math inline">\(S(n)\)</span></span> amount of physical resources, <span><span class="math inline">\(F\)</span></span> is in <span><span class="math inline">\(\ensuremath{\mathit{TIME}}(S(n)^a)\)</span></span>. This is a strengthening of the (“plain”) Church-Turing Thesis, discussed in <a href='lec_07_other_models.html#churchturingdiscussionsec'>Section 7.8</a>, which states that the set of computable functions is the same for all physically realizable models, but without requiring the overhead in the simulation between different models to be at most polynomial.</p>
<p>All the current constructions of scalable computational models and programming language conform to the Extended Church-Turing Thesis, in the sense that they can be with polynomial overhead by Turing Machines (and hence also by NAND-TM or NAND-RAM programs). consequently, the classes <span><span class="math inline">\(\mathbf{P}\)</span></span> and <span><span class="math inline">\(\mathbf{EXP}\)</span></span> are robust to the choice of model, and we can use the programming language of our choice, or high level descriptions of an algorithm, to determine whether or not a problem is in <span><span class="math inline">\(\mathbf{P}\)</span></span>.</p>
<p>Like the Church-Turing thesis itself, the extended Church-Turing thesis is in the asymptotic setting and does not directly yield an experimentally testable prediction. However, it can be instantiated with more concrete bounds on the overhead, yielding experimentally-testable predictions such as the <em>Physical Extended Church-Turing Thesis</em> we mentioned in <a href='lec_04_code_and_data.html#PECTTsec'>Section 5.6</a>.</p>
<p>In the last hundred+ years of studying and mechanizing computation, no one has yet constructed a scalable computing device that violates the extended Church Turing Thesis. However, <em>quantum computing</em>, if realized, will pose a serious challenge to the extended Church-Turing Thesis (see <a href='lec_26_quantum_computing.html#quantumchap'>Chapter 22</a>). However, even if the promises of quantum computing are fully realized, the extended Church-Turing thesis is “morally” correct, in the sense that, while we do need to adapt the thesis to account for the possibility of quantum computing, its broad outline remains unchanged. We are still able to model computation mathematically, we can still treat programs as strings and have a universal program, we still have time hierarchy and uncomputability results, and there is still no reason to doubt the (“plain”) Church-Turing thesis. Moreover, the prospect of quantum computing does not seem to make a difference for the time complexity of many (though not all!) of the concrete problems that we care about. In particular, as far as we know, out of all the example problems mentioned in <a href='lec_10_efficient_alg.html#chapefficient'>Chapter 11</a> the complexity of only one— integer factoring— is affected by modifying our model to include quantum computers as well.</p>
<h2 id="efficient-universal-machine-a-nand-ram-interpreter-in-nand-ram" data-number="12.4">Efficient universal machine: a NAND-RAM interpreter in NAND-RAM</h2>
<p>We have seen in <a href='lec_08_uncomputability.html#universaltmthm'>Theorem 8.1</a> the “universal Turing Machine”. Examining that proof, and combining it with <a href='#polyRAMTM-thm'>Theorem 12.5</a> , we can see that the program <span><span class="math inline">\(U\)</span></span> has a <em>polynomial</em> overhead, in the sense that it can simulate <span><span class="math inline">\(T\)</span></span> steps of a given NAND-TM (or NAND-RAM) program <span><span class="math inline">\(P\)</span></span> on an input <span><span class="math inline">\(x\)</span></span> in <span><span class="math inline">\(O(T^4)\)</span></span> steps. But in fact, by directly simulating NAND-RAM programs we can do better with only a <em>constant</em> multiplicative overhead. That is, there is a <em>universal NAND-RAM program</em> <span><span class="math inline">\(U\)</span></span> such that for every NAND-RAM program <span><span class="math inline">\(P\)</span></span>, <span><span class="math inline">\(U\)</span></span> simulates <span><span class="math inline">\(T\)</span></span> steps of <span><span class="math inline">\(P\)</span></span> using only <span><span class="math inline">\(O(T)\)</span></span> steps. (The implicit constant in the <span><span class="math inline">\(O\)</span></span> notation can depend on the program <span><span class="math inline">\(P\)</span></span> but does <em>not</em> depend on the length of the input.)</p>
<div id="univ-nandpp" class="theorem" title="Efficient universality of NAND-RAM" name="Theorem 12.7 (Efficient universality of NAND-RAM) ">
<p>There exists a NAND-RAM program <span><span class="math inline">\(U\)</span></span> satisfying the following:</p>
<ol type="1">
<li><p><em>(<span><span class="math inline">\(U\)</span></span> is a universal NAND-RAM program.)</em> For every NAND-RAM program <span><span class="math inline">\(P\)</span></span> and input <span><span class="math inline">\(x\)</span></span>, <span><span class="math inline">\(U(P,x)=P(x)\)</span></span> where by <span><span class="math inline">\(U(P,x)\)</span></span> we denote the output of <span><span class="math inline">\(U\)</span></span> on a string encoding the pair <span><span class="math inline">\((P,x)\)</span></span>.</p></li>
<li><p><em>(<span><span class="math inline">\(U\)</span></span> is efficient.)</em> There are some constants <span><span class="math inline">\(a,b\)</span></span> such that for every NAND-RAM program <span><span class="math inline">\(P\)</span></span>, if <span><span class="math inline">\(P\)</span></span> halts on input <span><span class="math inline">\(x\)</span></span> after most <span><span class="math inline">\(T\)</span></span> steps, then <span><span class="math inline">\(U(P,x)\)</span></span> halts after at most <span><span class="math inline">\(C\cdot T\)</span></span> steps where <span><span class="math inline">\(C \leq a |P|^b\)</span></span>.</p></li>
</ol>
</div>
<div id="section-2" class="pause" name="Pause">
<p>As in the case of <a href='#polyRAMTM-thm'>Theorem 12.5</a>, the proof of <a href='#univ-nandpp'>Theorem 12.7</a> is not very deep and so it is more important to understand its <em>statement</em>. Specifically, if you understand how you would go about writing an interpreter for NAND-RAM using a modern programming language such as Python, then you know everything you need to know about the proof of this theorem.</p>
</div>
<figure>
<img src="../figure/universalrammachine.png" alt="12.5: The universal NAND-RAM program U simulates an input NAND-RAM program P by storing all of P’s variables inside a single array Vars of U. If P has t variables, then the array Vars is divided into blocks of length t, where the j-th coordinate of the i-th block contains the i-th element of the j-th array of P. If the j-th variable of P is scalar, then we just store its value in the zeroth block of Vars." id="universalrammachinefig" class="margin" /><figcaption>12.5: The universal NAND-RAM program <span><span class="math inline">\(U\)</span></span> simulates an input NAND-RAM program <span><span class="math inline">\(P\)</span></span> by storing all of <span><span class="math inline">\(P\)</span></span>’s variables inside a single array <code>Vars</code> of <span><span class="math inline">\(U\)</span></span>. If <span><span class="math inline">\(P\)</span></span> has <span><span class="math inline">\(t\)</span></span> variables, then the array <code>Vars</code> is divided into blocks of length <span><span class="math inline">\(t\)</span></span>, where the <span><span class="math inline">\(j\)</span></span>-th coordinate of the <span><span class="math inline">\(i\)</span></span>-th block contains the <span><span class="math inline">\(i\)</span></span>-th element of the <span><span class="math inline">\(j\)</span></span>-th array of <span><span class="math inline">\(P\)</span></span>. If the <span><span class="math inline">\(j\)</span></span>-th variable of <span><span class="math inline">\(P\)</span></span> is scalar, then we just store its value in the zeroth block of <code>Vars</code>.</figcaption>
</figure>
<div class="proof" data-ref="univ-nandpp" name="Proof 12.4">
<p>To present a universal NAND-RAM program in full we would need to describe a precise representation scheme, as well as the full NAND-RAM instructions for the program. While this can be done, it is more important to focus on the main ideas, and so we just sketch the proof here. A specification of NAND-RAM is given in the <a href="http://tiny.cc/introtcsappendix">appendix</a>, and for the purposes of this simulation, we can simply use the representation of the code NAND-RAM as an ASCII string.</p>
<p>The program <span><span class="math inline">\(U\)</span></span> gets as input a NAND-RAM program <span><span class="math inline">\(P\)</span></span> and an input <span><span class="math inline">\(x\)</span></span> and simulates <span><span class="math inline">\(P\)</span></span> one step at a time. To do so, <span><span class="math inline">\(U\)</span></span> does the following:</p>
<ol type="1">
<li><p><span><span class="math inline">\(U\)</span></span> maintains variables <code>program_counter</code>, and <code>number_steps</code> for the current line to be executed and the number of steps executed so far.</p></li>
<li><p><span><span class="math inline">\(U\)</span></span> initially scans the code of <span><span class="math inline">\(P\)</span></span> to find the number <span><span class="math inline">\(t\)</span></span> of unique variable names that <span><span class="math inline">\(P\)</span></span> uses. It will translate each variable name into a number between <span><span class="math inline">\(0\)</span></span> and <span><span class="math inline">\(t-1\)</span></span> and use an array <code>Program</code> to store <span><span class="math inline">\(P\)</span></span>’s code where for every line <span><span class="math inline">\(\ell\)</span></span>, <code>Program[</code><span><span class="math inline">\(\ell\)</span></span><code>]</code> will store the <span><span class="math inline">\(\ell\)</span></span>-th line of <span><span class="math inline">\(P\)</span></span> where the variable names have been translated to numbers. (More concretely, we will use a constant number of arrays to separately encode the operation used in this line, and the variable names and indices of the operands.)</p></li>
<li><p><span><span class="math inline">\(U\)</span></span> maintains a single array <code>Vars</code> that contains all the values of <span><span class="math inline">\(P\)</span></span>’s variables. We divide <code>Vars</code> into blocks of length <span><span class="math inline">\(t\)</span></span>. If <span><span class="math inline">\(s\)</span></span> is a number corresponding to an array variable <code>Foo</code> of <span><span class="math inline">\(P\)</span></span>, then we store <code>Foo[0]</code> in <code>Vars[</code><span><span class="math inline">\(s\)</span></span><code>]</code>, we store <code>Foo[1]</code> in <code>Var_values[</code><span><span class="math inline">\(t+s\)</span></span><code>]</code>, <code>Foo[2]</code> in <code>Vars[</code><span><span class="math inline">\(2t + s\)</span></span><code>]</code> and so on and so forth (see <a href='#universalrammachinefig'>Figure 12.5</a>). Generally,if the <span><span class="math inline">\(s\)</span></span>-th variable of <span><span class="math inline">\(P\)</span></span> is a scalar variable, then its value will be stored in location <code>Vars[</code><span><span class="math inline">\(s\)</span></span><code>]</code>. If it is an array variable then the value of its <span><span class="math inline">\(i\)</span></span>-th element will be stored in location <code>Vars[</code><span><span class="math inline">\(t\cdot i + s\)</span></span><code>]</code>.</p></li>
<li><p>To simulate a single step of <span><span class="math inline">\(P\)</span></span>, the program <span><span class="math inline">\(U\)</span></span> recovers from <code>Program</code> the line corresponding to <code>program_counter</code> and executes it. Since NAND-RAM has a constant number of arithmetic operations, we can implement the logic of which operation to execute using a sequence of a constant number of if-then-else’s. Retrieving from <code>Vars</code> the values of the operands of each instruction can be done using a constant number of arithmetic operations.</p></li>
</ol>
<p>The setup stages take only a constant (depending on <span><span class="math inline">\(|P|\)</span></span> but not on the input <span><span class="math inline">\(x\)</span></span>) number of steps. Once we are done with the setup, to simulate a single step of <span><span class="math inline">\(P\)</span></span>, we just need to retrieve the corresponding line and do a constant number of “if elses” and accesses to <code>Vars</code> to simulate it. Hence the total running time to simulate <span><span class="math inline">\(T\)</span></span> steps of the program <span><span class="math inline">\(P\)</span></span> is at most <span><span class="math inline">\(O(T)\)</span></span> when suppressing constants that depend on the program <span><span class="math inline">\(P\)</span></span>.</p>
</div>
<h3 id="timed-universal-turing-machine" data-number="12.4.1">Timed Universal Turing Machine</h3>
<p>One corollary of the efficient universal machine is the following. Given any Turing Machine <span><span class="math inline">\(M\)</span></span>, input <span><span class="math inline">\(x\)</span></span>, and “step budget” <span><span class="math inline">\(T\)</span></span>, we can simulate the execution of <span><span class="math inline">\(M\)</span></span> for <span><span class="math inline">\(T\)</span></span> steps in time that is polynomial in <span><span class="math inline">\(T\)</span></span>. Formally, we define a function <span><span class="math inline">\(\ensuremath{\mathit{TIMEDEVAL}}\)</span></span> that takes the three parameters <span><span class="math inline">\(M\)</span></span>, <span><span class="math inline">\(x\)</span></span>, and the time budget, and outputs <span><span class="math inline">\(M(x)\)</span></span> if <span><span class="math inline">\(M\)</span></span> halts within at most <span><span class="math inline">\(T\)</span></span> steps, and outputs <span><span class="math inline">\(0\)</span></span> otherwise. The timed universal Turing Machine computes <span><span class="math inline">\(\ensuremath{\mathit{TIMEDEVAL}}\)</span></span> in polynomial time (see <a href='#timeduniversaltmfig'>?? ??</a>). (Since we measure time as a function of the input length, we define <span><span class="math inline">\(\ensuremath{\mathit{TIMEDEVAL}}\)</span></span> as taking the input <span><span class="math inline">\(T\)</span></span> represented in <em>unary</em>: a string of <span><span class="math inline">\(T\)</span></span> ones.)</p>
<div id="timeduniversalTM" class="theorem" title="Timed Universal Turing Machine" name="Theorem 12.8 (Timed Universal Turing Machine) ">
<p>Let <span><span class="math inline">\(\ensuremath{\mathit{TIMEDEVAL}}:\{0,1\}^* \rightarrow \{0,1\}^*\)</span></span> be the function defined as <span>
<div class='myequationbox'><span class="math display">\[\ensuremath{\mathit{TIMEDEVAL}}(M,x,1^T) = \begin{cases} M(x) &amp; M \text{ halts within $\leq T$ steps on $x$} \\ 0 &amp; \text{otherwise}\end{cases} \;.\]</span></div></span> Then <span><span class="math inline">\(\ensuremath{\mathit{TIMEDEVAL}} \in \mathbf{P}\)</span></span>.</p>
</div>
<figure>
<img src="../figure/timeduniversaltm.png" alt="??: The timed universal Turing Machine takes as input a Turing machine M, an input x, and a time bound T, and outputs M(x) if M halts within at most T steps. states that there is such a machine that runs in time polynomial in T." id="timeduniversaltmfig" class="margin" /><figcaption>??: The <em>timed</em> universal Turing Machine takes as input a Turing machine <span><span class="math inline">\(M\)</span></span>, an input <span><span class="math inline">\(x\)</span></span>, and a time bound <span><span class="math inline">\(T\)</span></span>, and outputs <span><span class="math inline">\(M(x)\)</span></span> if <span><span class="math inline">\(M\)</span></span> halts within at most <span><span class="math inline">\(T\)</span></span> steps. <a href='#timeduniversalTM'>Theorem 12.8</a>states that there is such a machine that runs in time polynomial in <span><span class="math inline">\(T\)</span></span>.</figcaption>
</figure>
<div class="proof" data-ref="timeduniversalTM" name="Solution 12.6.2">
<p>We only sketch the proof since the result follows fairly directly from <a href='#polyRAMTM-thm'>Theorem 12.5</a> and <a href='#univ-nandpp'>Theorem 12.7</a>. By <a href='#polyRAMTM-thm'>Theorem 12.5</a> to show that <span><span class="math inline">\(\ensuremath{\mathit{TIMEDEVAL}} \in \mathbf{P}\)</span></span>, it suffices to give a polynomial-time NAND-RAM program to compute <span><span class="math inline">\(\ensuremath{\mathit{TIMEDEVAL}}\)</span></span>.</p>
<p>Such a program can be obtained as follows. Given a Turing Machine <span><span class="math inline">\(M\)</span></span>, by <a href='#polyRAMTM-thm'>Theorem 12.5</a> we can transform it in time polynomial in its description into a functionally-equivalent NAND-RAM program <span><span class="math inline">\(P\)</span></span> such that the execution of <span><span class="math inline">\(M\)</span></span> on <span><span class="math inline">\(T\)</span></span> steps can be simulated by the execution of <span><span class="math inline">\(P\)</span></span> on <span><span class="math inline">\(c\cdot T\)</span></span> steps. We can then run the universal NAND-RAM machine of <a href='#univ-nandpp'>Theorem 12.7</a> to simulate <span><span class="math inline">\(P\)</span></span> for <span><span class="math inline">\(c\cdot T\)</span></span> steps, using <span><span class="math inline">\(O(T)\)</span></span> time, and output <span><span class="math inline">\(0\)</span></span> if the execution did not halt within this budget. This shows that <span><span class="math inline">\(\ensuremath{\mathit{TIMEDEVAL}}\)</span></span> can be computed by a NAND-RAM program in time polynomial in <span><span class="math inline">\(|M|\)</span></span> and linear in <span><span class="math inline">\(T\)</span></span>, which means <span><span class="math inline">\(\ensuremath{\mathit{TIMEDEVAL}} \in \mathbf{P}\)</span></span>.</p>
</div>
<h2 id="the-time-hierarchy-theorem" data-number="12.5">The time hierarchy theorem</h2>
<p>Some functions are <em>uncomputable</em>, but are there functions that can be computed, but only at an exorbitant cost? For example, is there a function that <em>can</em> be computed in time <span><span class="math inline">\(2^n\)</span></span>, but <em>can not</em> be computed in time <span><span class="math inline">\(2^{0.9 n}\)</span></span>? It turns out that the answer is <strong>Yes</strong>:</p>
<div id="time-hierarchy-thm" class="theorem" title="Time Hierarchy Theorem" name="Theorem 12.9 (Time Hierarchy Theorem) ">
<p>For every nice function <span><span class="math inline">\(T:\N \rightarrow \N\)</span></span>, there is a function <span><span class="math inline">\(F:\{0,1\}^* \rightarrow \{0,1\}\)</span></span> in <span><span class="math inline">\(\ensuremath{\mathit{TIME}}(T(n)\log n) \setminus \ensuremath{\mathit{TIME}}(T(n))\)</span></span>.</p>
</div>
<p>There is nothing special about <span><span class="math inline">\(\log n\)</span></span>, and we could have used any other efficiently computable function that tends to infinity with <span><span class="math inline">\(n\)</span></span>.</p>
<div id="timehierarchy" class="bigidea" name="Bigidea 18">
<p>If we have more time, we can compute more functions.</p>
</div>
<div id="hierarchytoyrem" class="remark" title="Simpler corollary of the time hierarchy theorem" name="Remark 12.10 (Simpler corollary of the time hierarchy theorem) ">
<p>The generality of the time hierarchy theorem can make its proof a little hard to read. It might be easier to follow the proof if you first try to prove by yourself the easier statement <span><span class="math inline">\(\mathbf{P} \subsetneq \mathbf{EXP}\)</span></span>.</p>
<p>You can do so by showing that the following function <span><span class="math inline">\(F:\{0,1\}^* :\rightarrow \{0,1\}\)</span></span> is in <span><span class="math inline">\(\mathbf{EXP} \setminus \mathbf{P}\)</span></span>: for every Turing Machine <span><span class="math inline">\(M\)</span></span> and input <span><span class="math inline">\(x\)</span></span>, <span><span class="math inline">\(F(M,x)=1\)</span></span> if and only if <span><span class="math inline">\(M\)</span></span> halts on <span><span class="math inline">\(x\)</span></span> within at most <span><span class="math inline">\(|x|^{\log |x|}\)</span></span> steps. One can show that <span><span class="math inline">\(F \in \ensuremath{\mathit{TIME}}(n^{O(\log n)}) \subseteq \mathbf{EXP}\)</span></span> using the universal Turing machine (or the efficient universal NAND-RAM program of <a href='#univ-nandpp'>Theorem 12.7</a>). On the other harnd, we can use similar ideas to those used to show the uncomputability of <span><span class="math inline">\(\ensuremath{\mathit{HALT}}\)</span></span> in <a href='lec_08_uncomputability.html#haltalternativesec'>Subsection 8.3.2</a> to prove that <span><span class="math inline">\(F \not\in \mathbf{P}\)</span></span>.</p>
</div>
<figure>
<img src="../figure/timehierarchythm.png" alt="12.7: The Time Hierarchy Theorem () states that all of these classes are distinct." id="timehierarchythmfig" /><figcaption>12.7: The <em>Time Hierarchy Theorem</em> (<a href='#time-hierarchy-thm'>Theorem 12.9</a>) states that all of these classes are <em>distinct</em>.</figcaption>
</figure>
<div id="section-3" class="proofidea" data-ref="time-hierarchy-thm" name="Proofidea">
<p>In the proof of <a href='lec_08_uncomputability.html#halt-thm'>Theorem 8.7</a> (the uncomputability of the Halting problem), we have shown that the function <span><span class="math inline">\(\ensuremath{\mathit{HALT}}\)</span></span> cannot be computed in any finite time. An examination of the proof shows that it gives something stronger. Namely, the proof shows that if we fix our computational budget to be <span><span class="math inline">\(T\)</span></span> steps, then not only we can’t distinguish between programs that halt and those that do not, but cannot even distinguish between programs that halt within at most <span><span class="math inline">\(T&#39;\)</span></span> steps and those that take more than that (where <span><span class="math inline">\(T&#39;\)</span></span> is some number depending on <span><span class="math inline">\(T\)</span></span>). Therefore, the proof of <a href='#time-hierarchy-thm'>Theorem 12.9</a> follows the ideas of the uncomputability of the halting problem, but again with a more careful accounting of the running time.</p>
</div>
<div class="proof" data-ref="time-hierarchy-thm" name="Proof 12.5">
<p>Our proof is inspired by the proof of the uncomputability of the halting problem. Specifically, for every function <span><span class="math inline">\(T\)</span></span> as in the theorem’s statement, we define the <em>Bounded Halting</em> function <span><span class="math inline">\(\ensuremath{\mathit{HALT}}_T\)</span></span> as follows. The input to <span><span class="math inline">\(\ensuremath{\mathit{HALT}}_T\)</span></span> is a pair <span><span class="math inline">\((P,x)\)</span></span> such that <span><span class="math inline">\(|P| \leq \log \log |x|\)</span></span> encodes some NAND-RAM program. We define</p>
<p><span>
<div class='myequationbox'><span class="math display">\[
\ensuremath{\mathit{HALT}}_T(P,x) = \begin{cases}1, &amp; P \text{ halts on } x \text{ within } \leq 100\cdot T(|P|+|x|) \text{ steps} \\
0, &amp; \text{otherwise} \end{cases} \;.
\]</span></div></span> (The constant <span><span class="math inline">\(100\)</span></span> and the function <span><span class="math inline">\(\log \log n\)</span></span> are rather arbitrary, and are chosen for convenience in this proof.)</p>
<p><a href='#time-hierarchy-thm'>Theorem 12.9</a> is an immediate consequence of the following two claims:</p>
<p><strong>Claim 1:</strong> <span><span class="math inline">\(\ensuremath{\mathit{HALT}}_T \in \ensuremath{\mathit{TIME}}(T(n)\cdot \log n)\)</span></span></p>
<p>and</p>
<p><strong>Claim 2:</strong> <span><span class="math inline">\(\ensuremath{\mathit{HALT}}_T \not\in \ensuremath{\mathit{TIME}}(T(n))\)</span></span>.</p>
<p>Please make sure you understand why indeed the theorem follows directly from the combination of these two claims. We now turn to proving them.</p>
<p><strong>Proof of claim 1:</strong> We can easily check in linear time whether an input has the form <span><span class="math inline">\(P,x\)</span></span> where <span><span class="math inline">\(|P| \leq \log\log |x|\)</span></span>. Since <span><span class="math inline">\(T(\cdot)\)</span></span> is a nice function, we can evaluate it in <span><span class="math inline">\(O(T(n))\)</span></span> time. Thus, we can compute <span><span class="math inline">\(\ensuremath{\mathit{HALT}}_T(P,x)\)</span></span> as follows:</p>
<ol type="1">
<li><p>Compute <span><span class="math inline">\(T_0=T(|P|+|x|)\)</span></span> in <span><span class="math inline">\(O(T_0)\)</span></span> steps.</p></li>
<li><p>Use the universal NAND-RAM program of <a href='#univ-nandpp'>Theorem 12.7</a> to simulate <span><span class="math inline">\(100\cdot T_0\)</span></span> steps of <span><span class="math inline">\(P\)</span></span> on the input <span><span class="math inline">\(x\)</span></span> using at most <span><span class="math inline">\(poly(|P|)T_0\)</span></span> steps. (Recall that we use <span><span class="math inline">\(poly(\ell)\)</span></span> to denote a quantity that is bounded by <span><span class="math inline">\(a\ell^b\)</span></span> for some constants <span><span class="math inline">\(a,b\)</span></span>.)</p></li>
<li><p>If <span><span class="math inline">\(P\)</span></span> halts within these <span><span class="math inline">\(100\cdot T_0\)</span></span> steps then output <span><span class="math inline">\(1\)</span></span>, else output <span><span class="math inline">\(0\)</span></span>.</p></li>
</ol>
<p>The length of the input is <span><span class="math inline">\(n=|P|+|x|\)</span></span>. Since <span><span class="math inline">\(|x| \leq n\)</span></span> and <span><span class="math inline">\((\log \log |x|)^b = o(\log |x|)\)</span></span> for every <span><span class="math inline">\(b\)</span></span>, the running time will be <span><span class="math inline">\(o(T(|P|+|x|) \log n)\)</span></span> and hence the above algorithm demonstrates that <span><span class="math inline">\(\ensuremath{\mathit{HALT}}_T \in \ensuremath{\mathit{TIME}}(T(n)\cdot \log n)\)</span></span>, completing the proof of Claim 1.</p>
<p><strong>Proof of claim 2:</strong> This proof is the heart of <a href='#time-hierarchy-thm'>Theorem 12.9</a>, and is very reminiscent of the proof that <span><span class="math inline">\(\ensuremath{\mathit{HALT}}\)</span></span> is not computable. Assume, for the sake of contradiction, that there is some NAND-RAM program <span><span class="math inline">\(P^*\)</span></span> that computes <span><span class="math inline">\(\ensuremath{\mathit{HALT}}_T(P,x)\)</span></span> within <span><span class="math inline">\(T(|P|+|x|)\)</span></span> steps. We are going to show a contradiction by creating a program <span><span class="math inline">\(Q\)</span></span> and showing that under our assumptions, if <span><span class="math inline">\(Q\)</span></span> runs for less than <span><span class="math inline">\(T(n)\)</span></span> steps when given (a padded version of) its own code as input then it actually runs for more than <span><span class="math inline">\(T(n)\)</span></span> steps and vice versa. (It is worth re-reading the last sentence twice or thrice to make sure you understand this logic. It is very similar to the direct proof of the uncomputability of the halting problem where we obtained a contradiction by using an assumed “halting solver” to construct a program that, given its own code as input, halts if and only if it does not halt.)</p>
<p>We will define <span><span class="math inline">\(Q^*\)</span></span> to be the program that on input a string <span><span class="math inline">\(z\)</span></span> does the following:</p>
<ol type="1">
<li><p>If <span><span class="math inline">\(z\)</span></span> does not have the form <span><span class="math inline">\(z=P1^m\)</span></span> where <span><span class="math inline">\(P\)</span></span> represents a NAND-RAM program and <span><span class="math inline">\(|P|&lt; 0.1 \log\log m\)</span></span> then return <span><span class="math inline">\(0\)</span></span>. (Recall that <span><span class="math inline">\(1^m\)</span></span> denotes the string of <span><span class="math inline">\(m\)</span></span> ones.)</p></li>
<li><p>Compute <span><span class="math inline">\(b= P^*(P,z)\)</span></span> (at a cost of at most <span><span class="math inline">\(T(|P|+|z|)\)</span></span> steps, under our assumptions).</p></li>
<li><p>If <span><span class="math inline">\(b=1\)</span></span> then <span><span class="math inline">\(Q^*\)</span></span> goes into an infinite loop, otherwise it halts.</p></li>
</ol>
<p>Let <span><span class="math inline">\(\ell\)</span></span> be the length description of <span><span class="math inline">\(Q^*\)</span></span> as a string, and let <span><span class="math inline">\(m\)</span></span> be larger than <span><span class="math inline">\(2^{2^{1000 \ell}}\)</span></span>. We will reach a contradiction by splitting into cases according to whether or not <span><span class="math inline">\(\ensuremath{\mathit{HALT}}_T(Q^*,Q^*1^m)\)</span></span> equals <span><span class="math inline">\(0\)</span></span> or <span><span class="math inline">\(1\)</span></span>.</p>
<p>On the one hand, if <span><span class="math inline">\(\ensuremath{\mathit{HALT}}_T(Q^*,Q^*1^m)=1\)</span></span>, then under our assumption that <span><span class="math inline">\(P^*\)</span></span> computes <span><span class="math inline">\(\ensuremath{\mathit{HALT}}_T\)</span></span>, <span><span class="math inline">\(Q^*\)</span></span> will go into an infinite loop on input <span><span class="math inline">\(z=Q^*1^m\)</span></span>, and hence in particular <span><span class="math inline">\(Q^*\)</span></span> does <em>not</em> halt within <span><span class="math inline">\(100 T(|Q^*|+m)\)</span></span> steps on the input <span><span class="math inline">\(z\)</span></span>. But this contradicts our assumption that <span><span class="math inline">\(\ensuremath{\mathit{HALT}}_T(Q^*,Q^*1^m)=1\)</span></span>.</p>
<p>This means that it must hold that <span><span class="math inline">\(\ensuremath{\mathit{HALT}}_T(Q^*,Q^*1^m)=0\)</span></span>. But in this case, since we assume <span><span class="math inline">\(P^*\)</span></span> computes <span><span class="math inline">\(\ensuremath{\mathit{HALT}}_T\)</span></span>, <span><span class="math inline">\(Q^*\)</span></span> does not do anything in phase 3 of its computation, and so the only computation costs come in phases 1 and 2 of the computation. It is not hard to verify that Phase 1 can be done in linear and in fact less than <span><span class="math inline">\(5|z|\)</span></span> steps. Phase 2 involves executing <span><span class="math inline">\(P^*\)</span></span>, which under our assumption requires <span><span class="math inline">\(T(|Q^*|+m)\)</span></span> steps. In total we can perform both phases in less than <span><span class="math inline">\(10 T(|Q^*|+m)\)</span></span> in steps, which by definition means that <span><span class="math inline">\(\ensuremath{\mathit{HALT}}_T(Q^*,Q^*1^m)=1\)</span></span>, but this is of course a contradiction. This completes the proof of Claim 2 and hence of <a href='#time-hierarchy-thm'>Theorem 12.9</a>.</p>
</div>
<div id="PvsEXPexercise" class="solvedexercise" title="$\mathbf{P}$ vs $\mathbf{EXP}$" name="Solvedexercise 12.3 ($\mathbf{P}$ vs $\mathbf{EXP}$) ">
<p>Prove that <span><span class="math inline">\(\mathbf{P} \subsetneq \mathbf{EXP}\)</span></span>.</p>
</div>
<div class="solution" data-ref="PvsEXP" name="Solution 12.5">
<p>We show why this statement follows from the time hierarchy theorem, but it can be an instructive exercise to prove it directly, see <a href='#hierarchytoyrem'>Remark 12.10</a>. We need to show that there exists <span><span class="math inline">\(F \in \mathbf{EXP} \setminus \mathbf{P}\)</span></span>. Let <span><span class="math inline">\(T(n) = n^{\log n}\)</span></span> and <span><span class="math inline">\(T&#39;(n) = n^{\log n / 2}\)</span></span>. Both are nice functions. Since <span><span class="math inline">\(T(n)/T&#39;(n) = \omega(\log n)\)</span></span>, by <a href='#time-hierarchy-thm'>Theorem 12.9</a> there exists some <span><span class="math inline">\(F\)</span></span> in <span><span class="math inline">\(\ensuremath{\mathit{TIME}}(T&#39;(n)) \subsetneq \ensuremath{\mathit{TIME}}(T(n))\)</span></span>. Since for sufficiently large <span><span class="math inline">\(n\)</span></span>, <span><span class="math inline">\(2^n &gt; n^{\log n}\)</span></span>, <span><span class="math inline">\(F \in \ensuremath{\mathit{TIME}}(2^n) \subseteq \mathbf{EXP}\)</span></span>. On the other hand, <span><span class="math inline">\(F \not\in \mathbf{P}\)</span></span>. Indeed, suppose otherwise that there was a constant <span><span class="math inline">\(c&gt;0\)</span></span> and a Turing Machine computing <span><span class="math inline">\(F\)</span></span> on <span><span class="math inline">\(n\)</span></span>-length input in at most <span><span class="math inline">\(n^c\)</span></span> steps for all sufficiently large <span><span class="math inline">\(n\)</span></span>. Then since for <span><span class="math inline">\(n\)</span></span> large enough <span><span class="math inline">\(n^c &lt; n^{\log n/2}\)</span></span>, it would have followed that <span><span class="math inline">\(F \in \ensuremath{\mathit{TIME}}(n^{\log n /2})\)</span></span> contradicting our choice of <span><span class="math inline">\(F\)</span></span>.</p>
</div>
<p>The time hierarchy theorem tells us that there are functions we can compute in <span><span class="math inline">\(O(n^2)\)</span></span> time but not <span><span class="math inline">\(O(n)\)</span></span>, in <span><span class="math inline">\(2^n\)</span></span> time, but not <span><span class="math inline">\(2^{\sqrt{n}}\)</span></span>, etc.. In particular there are most definitely functions that we can compute in time <span><span class="math inline">\(2^n\)</span></span> but not <span><span class="math inline">\(O(n)\)</span></span>. We have seen that we have no shortage of natural functions for which the best <em>known</em> algorithm requires roughly <span><span class="math inline">\(2^n\)</span></span> time, and that many people have invested significant effort in trying to improve that. However, unlike in the finite vs. infinite case, for all of the examples above at the moment we do not know how to rule out even an <span><span class="math inline">\(O(n)\)</span></span> time algorithm. We will however see that there is a single unproven conjecture that would imply such a result for most of these problems.</p>
<figure>
<img src="../figure/time_complexity_map.png" alt="12.8: Some complexity classes and some of the functions we know (or conjecture) to be contained in them." id="complexityclassinclusionfig" class="margin" /><figcaption>12.8: Some complexity classes and some of the functions we know (or conjecture) to be contained in them.</figcaption>
</figure>
<p>The time hierarchy theorem relies on the existence of an efficient universal NAND-RAM program, as proven in <a href='#univ-nandpp'>Theorem 12.7</a>. For other models such as Turing Machines we have similar time hierarchy results showing that there are functions computable in time <span><span class="math inline">\(T(n)\)</span></span> and not in time <span><span class="math inline">\(T(n)/f(n)\)</span></span> where <span><span class="math inline">\(f(n)\)</span></span> corresponds to the overhead in the corresponding universal machine.</p>
<h2 id="nonuniformcompsec" data-number="12.6">Non uniform computation</h2>
<p>We have now seen two measures of “computation cost” for functions. In <a href='lec_03a_computing_every_function.html#secdefinesizeclasses'>Section 4.6</a> we defined the complexity of computing <em>finite</em> functions using circuits / straightline programs. Specifically, for a finite function <span><span class="math inline">\(g:\{0,1\}^n \rightarrow \{0,1\}\)</span></span> and number <span><span class="math inline">\(T\in \N\)</span></span>, <span><span class="math inline">\(g\in \ensuremath{\mathit{SIZE}}(T)\)</span></span> if there is a circuit of at most <span><span class="math inline">\(T\)</span></span> NAND gates (or equivalently a <span><span class="math inline">\(T\)</span></span>-line NAND-CIRC program) that computes <span><span class="math inline">\(g\)</span></span>. To relate this to the classes <span><span class="math inline">\(\ensuremath{\mathit{TIME}}(T(n))\)</span></span> defined in this chapter we first need to extend the class <span><span class="math inline">\(\ensuremath{\mathit{SIZE}}(T(n))\)</span></span> from finite functions to functions with unbounded input length.</p>
<div id="nonuniformdef" class="definition" title="Non uniform computation" name="Definition 12.11 (Non uniform computation) ">
<p>Let <span><span class="math inline">\(F:\{0,1\}^* \rightarrow \{0,1\}\)</span></span> and <span><span class="math inline">\(T:\N \rightarrow \N\)</span></span> be a nice time bound. For every <span><span class="math inline">\(n\in \N\)</span></span>, define <span><span class="math inline">\(F_{\upharpoonright n} : \{0,1\}^n \rightarrow \{0,1\}\)</span></span> to be the <em>restriction</em> of <span><span class="math inline">\(F\)</span></span> to inputs of size <span><span class="math inline">\(n\)</span></span>. That is, <span><span class="math inline">\(F_{\upharpoonright n}\)</span></span> is the function mapping <span><span class="math inline">\(\{0,1\}^n\)</span></span> to <span><span class="math inline">\(\{0,1\}\)</span></span> such that for every <span><span class="math inline">\(x\in \{0,1\}^n\)</span></span>, <span><span class="math inline">\(F_{\upharpoonright n}(x)=F(x)\)</span></span>.</p>
<p>We say that <span><span class="math inline">\(F\)</span></span> is <em>non-uniformly computable in at most <span><span class="math inline">\(T(n)\)</span></span> size</em>, denoted by <span><span class="math inline">\(F \in \ensuremath{\mathit{SIZE}}(T(n))\)</span></span> if there exists a sequence <span><span class="math inline">\((C_0,C_1,C_2,\ldots)\)</span></span> of NAND circuits such that:</p>
<ul>
<li><p>For every <span><span class="math inline">\(n\in \N\)</span></span>, <span><span class="math inline">\(C_n\)</span></span> computes the function <span><span class="math inline">\(F_{\upharpoonright n}\)</span></span></p></li>
<li><p>For every sufficiently large <span><span class="math inline">\(n\)</span></span>, <span><span class="math inline">\(C_n\)</span></span> has at most <span><span class="math inline">\(T(n)\)</span></span> gates.</p></li>
</ul>
</div>
<p>The non uniform analog to the class <span><span class="math inline">\(\mathbf{P}\)</span></span> is the class <span><span class="math inline">\(\mathbf{P_{/poly}}\)</span></span> defined as</p>
<p><span>
<div class='myequationbox'><span class="math display">\[
\mathbf{P_{/poly}} = \cup_{c\in \N} \ensuremath{\mathit{SIZE}}(n^c)  \; . \;\;(12.5)
\]</span><a id='eqppolydef'></a></div></span> There is a big difference between non uniform computation and uniform complexity classes such as <span><span class="math inline">\(\ensuremath{\mathit{TIME}}(T(n))\)</span></span> or <span><span class="math inline">\(\mathbf{P}\)</span></span>. The condition <span><span class="math inline">\(F\in \mathbf{P}\)</span></span> means that there is a <em>single</em> Turing machine <span><span class="math inline">\(M\)</span></span> that computes <span><span class="math inline">\(F\)</span></span> on all inputs in polynomial time. The condition <span><span class="math inline">\(F\in \mathbf{P_{/poly}}\)</span></span> only means that for every input length <span><span class="math inline">\(n\)</span></span> there can be a <em>different</em> circuit <span><span class="math inline">\(C_n\)</span></span> that computes <span><span class="math inline">\(F\)</span></span> using polynomially many gates on inputs of these lengths. As we will see, <span><span class="math inline">\(F\in \mathbf{P_{/poly}}\)</span></span> does not necessarily imply that <span><span class="math inline">\(F\in \mathbf{P}\)</span></span>. However, the other direction is true:</p>
<figure>
<img src="../figure/Ppoly.png" alt="12.9: We can think of an infinite function F:\{0,1\}^* \rightarrow \{0,1\} as a collection of finite functions F_0,F_1,F_2,\ldots where F_{\upharpoonright n}:\{0,1\}^n \rightarrow \{0,1\} is the restriction of F to inputs of length n. We say F is in \mathbf{P_{/poly}} if for every n, the function F_{\upharpoonright n} is computable by a polynomial size NAND-CIRC program, or equivalently, a polynomial sized Boolean circuit." id="Ppolyfig" class="margin" /><figcaption>12.9: We can think of an infinite function <span><span class="math inline">\(F:\{0,1\}^* \rightarrow \{0,1\}\)</span></span> as a collection of finite functions <span><span class="math inline">\(F_0,F_1,F_2,\ldots\)</span></span> where <span><span class="math inline">\(F_{\upharpoonright n}:\{0,1\}^n \rightarrow \{0,1\}\)</span></span> is the restriction of <span><span class="math inline">\(F\)</span></span> to inputs of length <span><span class="math inline">\(n\)</span></span>. We say <span><span class="math inline">\(F\)</span></span> is in <span><span class="math inline">\(\mathbf{P_{/poly}}\)</span></span> if for every <span><span class="math inline">\(n\)</span></span>, the function <span><span class="math inline">\(F_{\upharpoonright n}\)</span></span> is computable by a polynomial size NAND-CIRC program, or equivalently, a polynomial sized Boolean circuit.</figcaption>
</figure>
<div id="non-uniform-thm" class="theorem" title="Nonuniform computation contains uniform computation" name="Theorem 12.12 (Nonuniform computation contains uniform computation) ">
<p>There is some <span><span class="math inline">\(a\in \N\)</span></span> s.t. for every nice <span><span class="math inline">\(T:\N \rightarrow \N\)</span></span> and <span><span class="math inline">\(F:\{0,1\}^* \rightarrow \{0,1\}\)</span></span>, <span>
<div class='myequationbox'><span class="math display">\[\ensuremath{\mathit{TIME}}(T(n)) \subseteq \ensuremath{\mathit{SIZE}}(T(n)^a)\;.\]</span></div></span></p>
</div>
<p>In particular, <a href='#non-uniform-thm'>Theorem 12.12</a> shows that for every <span><span class="math inline">\(c\)</span></span>, <span><span class="math inline">\(\ensuremath{\mathit{TIME}}(n^c) \subseteq \ensuremath{\mathit{SIZE}}(n^{ca})\)</span></span> and hence <span><span class="math inline">\(\mathbf{P} \subseteq \mathbf{P_{/poly}}\)</span></span>.</p>
<div class="proofidea" data-ref="non-uniform-thm" name="Proofidea 22.11.1">
<p>The idea behind the proof is to “unroll the loop”. Specifically, we will use the programming language variants of non-uniform and uniform computation: namely NAND-CIRC and NAND-TM. The main difference between the two is that NAND-TM has <em>loops</em>. However, for every fixed <span><span class="math inline">\(n\)</span></span>, if we know that a NAND-TM program runs in at most <span><span class="math inline">\(T(n)\)</span></span> steps, then we can replace its loop by simply “copying and pasting” its code <span><span class="math inline">\(T(n)\)</span></span> times, similar to how in Python we can replace code such as</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" title="1"><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>):</a>
<a class="sourceLine" id="cb1-2" title="2">    <span class="bu">print</span>(i)</a></code></pre></div>
<p>with the “loop free” code</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb2-1" title="1"><span class="bu">print</span>(<span class="dv">0</span>)</a>
<a class="sourceLine" id="cb2-2" title="2"><span class="bu">print</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb2-3" title="3"><span class="bu">print</span>(<span class="dv">2</span>)</a>
<a class="sourceLine" id="cb2-4" title="4"><span class="bu">print</span>(<span class="dv">3</span>)</a></code></pre></div>
<p>To make this idea into an actual proof we need to tackle one technical difficulty, and this is to ensure that the NAND-TM program is <em>oblivious</em> in the sense that the value of the index variable <code>i</code> in the <span><span class="math inline">\(j\)</span></span>-th iteration of the loop will depend only on <span><span class="math inline">\(j\)</span></span> and not on the contents of the input. We make a digression to do just that in <a href='#obliviousnandtm'>Subsection 12.6.1</a> and then complete the proof of <a href='#non-uniform-thm'>Theorem 12.12</a>.</p>
</div>
<h3 id="obliviousnandtm" data-number="12.6.1">Oblivious NAND-TM programs</h3>
<p>Our approach for proving <a href='#non-uniform-thm'>Theorem 12.12</a> involves “unrolling the loop”. For example, consider the following NAND-TM to compute the <span><span class="math inline">\(\ensuremath{\mathit{XOR}}\)</span></span> function on inputs of arbitrary length:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb3-1" title="1">temp_0 <span class="op">=</span> NAND(X[<span class="dv">0</span>],X[<span class="dv">0</span>])</a>
<a class="sourceLine" id="cb3-2" title="2">Y_nonblank[<span class="dv">0</span>] <span class="op">=</span> NAND(X[<span class="dv">0</span>],temp_0)</a>
<a class="sourceLine" id="cb3-3" title="3">temp_2 <span class="op">=</span> NAND(X[i],Y[<span class="dv">0</span>])</a>
<a class="sourceLine" id="cb3-4" title="4">temp_3 <span class="op">=</span> NAND(X[i],temp_2)</a>
<a class="sourceLine" id="cb3-5" title="5">temp_4 <span class="op">=</span> NAND(Y[<span class="dv">0</span>],temp_2)</a>
<a class="sourceLine" id="cb3-6" title="6">Y[<span class="dv">0</span>] <span class="op">=</span> NAND(temp_3,temp_4)</a>
<a class="sourceLine" id="cb3-7" title="7">MODANDJUMP(X_nonblank[i],X_nonblank[i])</a></code></pre></div>
<p>Setting (as an example) <span><span class="math inline">\(n=3\)</span></span>, we can attempt to translate this NAND-TM program into a NAND-CIRC program for computing <span><span class="math inline">\(\ensuremath{\mathit{XOR}}_3:\{0,1\}^3 \rightarrow \{0,1\}\)</span></span> by simply “copying and pasting” the loop three times (dropping the <code>MODANDJMP</code> line):</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb4-1" title="1">temp_0 <span class="op">=</span> NAND(X[<span class="dv">0</span>],X[<span class="dv">0</span>])</a>
<a class="sourceLine" id="cb4-2" title="2">Y_nonblank[<span class="dv">0</span>] <span class="op">=</span> NAND(X[<span class="dv">0</span>],temp_0)</a>
<a class="sourceLine" id="cb4-3" title="3">temp_2 <span class="op">=</span> NAND(X[i],Y[<span class="dv">0</span>])</a>
<a class="sourceLine" id="cb4-4" title="4">temp_3 <span class="op">=</span> NAND(X[i],temp_2)</a>
<a class="sourceLine" id="cb4-5" title="5">temp_4 <span class="op">=</span> NAND(Y[<span class="dv">0</span>],temp_2)</a>
<a class="sourceLine" id="cb4-6" title="6">Y[<span class="dv">0</span>] <span class="op">=</span> NAND(temp_3,temp_4)</a>
<a class="sourceLine" id="cb4-7" title="7">temp_0 <span class="op">=</span> NAND(X[<span class="dv">0</span>],X[<span class="dv">0</span>])</a>
<a class="sourceLine" id="cb4-8" title="8">Y_nonblank[<span class="dv">0</span>] <span class="op">=</span> NAND(X[<span class="dv">0</span>],temp_0)</a>
<a class="sourceLine" id="cb4-9" title="9">temp_2 <span class="op">=</span> NAND(X[i],Y[<span class="dv">0</span>])</a>
<a class="sourceLine" id="cb4-10" title="10">temp_3 <span class="op">=</span> NAND(X[i],temp_2)</a>
<a class="sourceLine" id="cb4-11" title="11">temp_4 <span class="op">=</span> NAND(Y[<span class="dv">0</span>],temp_2)</a>
<a class="sourceLine" id="cb4-12" title="12">Y[<span class="dv">0</span>] <span class="op">=</span> NAND(temp_3,temp_4)</a>
<a class="sourceLine" id="cb4-13" title="13">temp_0 <span class="op">=</span> NAND(X[<span class="dv">0</span>],X[<span class="dv">0</span>])</a>
<a class="sourceLine" id="cb4-14" title="14">Y_nonblank[<span class="dv">0</span>] <span class="op">=</span> NAND(X[<span class="dv">0</span>],temp_0)</a>
<a class="sourceLine" id="cb4-15" title="15">temp_2 <span class="op">=</span> NAND(X[i],Y[<span class="dv">0</span>])</a>
<a class="sourceLine" id="cb4-16" title="16">temp_3 <span class="op">=</span> NAND(X[i],temp_2)</a>
<a class="sourceLine" id="cb4-17" title="17">temp_4 <span class="op">=</span> NAND(Y[<span class="dv">0</span>],temp_2)</a>
<a class="sourceLine" id="cb4-18" title="18">Y[<span class="dv">0</span>] <span class="op">=</span> NAND(temp_3,temp_4)</a></code></pre></div>
<p>However, the above is still not a valid NAND-CIRC program since it contains references to the special variable <code>i</code>. To make it into a valid NAND-CIRC program, we replace references to <code>i</code> in the first iteration with <span><span class="math inline">\(0\)</span></span>, references in the second iteration with <span><span class="math inline">\(1\)</span></span>, and references in the third iteration with <span><span class="math inline">\(2\)</span></span>. (We also create a variable <code>zero</code> and use it for the first time any variable is instantiated, as well as remove assignments to non-output variables that are never used later on.) The resulting program is a standard “loop free and index free” NAND-CIRC program that computes <span><span class="math inline">\(\ensuremath{\mathit{XOR}}_3\)</span></span> (see also <a href='#unrolledcircuitfig'>Figure 12.10</a>):</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb5-1" title="1">temp_0 <span class="op">=</span> NAND(X[<span class="dv">0</span>],X[<span class="dv">0</span>])</a>
<a class="sourceLine" id="cb5-2" title="2">one <span class="op">=</span> NAND(X[<span class="dv">0</span>],temp_0)</a>
<a class="sourceLine" id="cb5-3" title="3">zero <span class="op">=</span> NAND(one,one)</a>
<a class="sourceLine" id="cb5-4" title="4">temp_2 <span class="op">=</span> NAND(X[<span class="dv">0</span>],zero)</a>
<a class="sourceLine" id="cb5-5" title="5">temp_3 <span class="op">=</span> NAND(X[<span class="dv">0</span>],temp_2)</a>
<a class="sourceLine" id="cb5-6" title="6">temp_4 <span class="op">=</span> NAND(zero,temp_2)</a>
<a class="sourceLine" id="cb5-7" title="7">Y[<span class="dv">0</span>] <span class="op">=</span> NAND(temp_3,temp_4)</a>
<a class="sourceLine" id="cb5-8" title="8">temp_2 <span class="op">=</span> NAND(X[<span class="dv">1</span>],Y[<span class="dv">0</span>])</a>
<a class="sourceLine" id="cb5-9" title="9">temp_3 <span class="op">=</span> NAND(X[<span class="dv">1</span>],temp_2)</a>
<a class="sourceLine" id="cb5-10" title="10">temp_4 <span class="op">=</span> NAND(Y[<span class="dv">0</span>],temp_2)</a>
<a class="sourceLine" id="cb5-11" title="11">Y[<span class="dv">0</span>] <span class="op">=</span> NAND(temp_3,temp_4)</a>
<a class="sourceLine" id="cb5-12" title="12">temp_2 <span class="op">=</span> NAND(X[<span class="dv">2</span>],Y[<span class="dv">0</span>])</a>
<a class="sourceLine" id="cb5-13" title="13">temp_3 <span class="op">=</span> NAND(X[<span class="dv">2</span>],temp_2)</a>
<a class="sourceLine" id="cb5-14" title="14">temp_4 <span class="op">=</span> NAND(Y[<span class="dv">0</span>],temp_2)</a>
<a class="sourceLine" id="cb5-15" title="15">Y[<span class="dv">0</span>] <span class="op">=</span> NAND(temp_3,temp_4)</a></code></pre></div>
<figure>
<img src="../figure/unrolled_circuit.png" alt="12.10: A NAND circuit for \ensuremath{\mathit{XOR}}_3 obtained by “unrolling the loop” of the NAND-TM program for computing \ensuremath{\mathit{XOR}} three times." id="unrolledcircuitfig" class="margin" /><figcaption>12.10: A NAND circuit for <span><span class="math inline">\(\ensuremath{\mathit{XOR}}_3\)</span></span> obtained by “unrolling the loop” of the NAND-TM program for computing <span><span class="math inline">\(\ensuremath{\mathit{XOR}}\)</span></span> three times.</figcaption>
</figure>
<p>Key to this transformation was the fact that in our original NAND-TM program for <span><span class="math inline">\(\ensuremath{\mathit{XOR}}\)</span></span>, regardless of whether the input is <span><span class="math inline">\(011\)</span></span>, <span><span class="math inline">\(100\)</span></span>, or any other string, the index variable <code>i</code> is guaranteed to equal <span><span class="math inline">\(0\)</span></span> in the first iteration, <span><span class="math inline">\(1\)</span></span> in the second iteration, <span><span class="math inline">\(2\)</span></span> in the third iteration, and so on and so forth. The particular sequence <span><span class="math inline">\(0,1,2,\ldots\)</span></span> is immaterial: the crucial property is that the NAND-TM program for <span><span class="math inline">\(\ensuremath{\mathit{XOR}}\)</span></span> is <em>oblivious</em> in the sense that the value of the index <code>i</code> in the <span><span class="math inline">\(j\)</span></span>-th iteration depends only on <span><span class="math inline">\(j\)</span></span> and does not depend on the particular choice of the input. Luckily, it is possible to transform every NAND-TM program into a functionally equivalent oblivious program with at most quadratic . (Similarly we can transform any Turing machine into a functionally equivalent oblivious Turing machine, see <a href='#oblivious-ex'>Exercise 12.6</a>.)</p>
<div id="obliviousnandtmthm" class="theorem" title="Making NAND-TM oblivious" name="Theorem 12.13 (Making NAND-TM oblivious) ">
<p>Let <span><span class="math inline">\(T:\N \rightarrow \N\)</span></span> be a nice function and let <span><span class="math inline">\(F\in \ensuremath{\mathit{TIME}}_{\mathsf{TM}}(T(n))\)</span></span>. Then there is a NAND-TM program <span><span class="math inline">\(P\)</span></span> that computes <span><span class="math inline">\(F\)</span></span> in <span><span class="math inline">\(O(T(n)^2)\)</span></span> steps and satisfying the following. For every <span><span class="math inline">\(n\in \N\)</span></span> there is a sequence <span><span class="math inline">\(i_0,i_1,\ldots, i_{m-1}\)</span></span> such that for every <span><span class="math inline">\(x\in \{0,1\}^n\)</span></span>, if <span><span class="math inline">\(P\)</span></span> is executed on input <span><span class="math inline">\(x\)</span></span> then in the <span><span class="math inline">\(j\)</span></span>-th iteration the variable <code>i</code> is equal to <span><span class="math inline">\(i_j\)</span></span>.</p>
</div>
<p>In other words, <a href='#obliviousnandtmthm'>Theorem 12.13</a> implies that if we can compute <span><span class="math inline">\(F\)</span></span> in <span><span class="math inline">\(T(n)\)</span></span> steps, then we can compute it in <span><span class="math inline">\(O(T(n)^2)\)</span></span> steps with a program <span><span class="math inline">\(P\)</span></span> in which the position of <code>i</code> in the <span><span class="math inline">\(j\)</span></span>-th iteration depends only on <span><span class="math inline">\(j\)</span></span> and the length of the input, and not on the contents of the input. Such a program can be easily translated into a NAND-CIRC program of <span><span class="math inline">\(O(T(n)^2)\)</span></span> lines by “unrolling the loop”.</p>
<div id="section-4" class="proofidea" data-ref="obliviousnandtmthm" name="Proofidea">
<p>We can translate any NAND-TM program <span><span class="math inline">\(P&#39;\)</span></span> into an oblivious program <span><span class="math inline">\(P\)</span></span> by making <span><span class="math inline">\(P\)</span></span> “sweep” its arrays. That is, the index <code>i</code> in <span><span class="math inline">\(P\)</span></span> will always move all the way from position <span><span class="math inline">\(0\)</span></span> to position <span><span class="math inline">\(T(n)-1\)</span></span> and back again. We can then simulate the program <span><span class="math inline">\(P&#39;\)</span></span> with at most <span><span class="math inline">\(T(n)\)</span></span> overhead: if <span><span class="math inline">\(P&#39;\)</span></span> wants to move <code>i</code> left when we are in a rightward sweep then we simply wait the at most <span><span class="math inline">\(2T(n)\)</span></span> steps until the next time we are back in the same position while sweeping to the left.</p>
</div>
<figure>
<img src="../figure/obliviousnandtm.png" alt="12.11: We simulate a T(n)-time NAND-TM program P&#39; with an oblivious NAND-TM program P by adding special arrays Atstart and Atend to mark positions 0 and T-1 respectively. The program P will simply “sweep” its arrays from right to left and back again. If the original program P&#39; would have moved i in a different direction then we wait O(T) steps until we reach the same point back again, and so P runs in O(T(n)^2) time." id="obliviousnandtmfig" class="margin" /><figcaption>12.11: We simulate a <span><span class="math inline">\(T(n)\)</span></span>-time NAND-TM program <span><span class="math inline">\(P&#39;\)</span></span> with an <em>oblivious</em> NAND-TM program <span><span class="math inline">\(P\)</span></span> by adding special arrays <code>Atstart</code> and <code>Atend</code> to mark positions <span><span class="math inline">\(0\)</span></span> and <span><span class="math inline">\(T-1\)</span></span> respectively. The program <span><span class="math inline">\(P\)</span></span> will simply “sweep” its arrays from right to left and back again. If the original program <span><span class="math inline">\(P&#39;\)</span></span> would have moved <code>i</code> in a different direction then we wait <span><span class="math inline">\(O(T)\)</span></span> steps until we reach the same point back again, and so <span><span class="math inline">\(P\)</span></span> runs in <span><span class="math inline">\(O(T(n)^2)\)</span></span> time.</figcaption>
</figure>
<div class="proof" data-ref="obliviousnandtmthm" name="Proof 12.6.1">
<p>Let <span><span class="math inline">\(P&#39;\)</span></span> be a NAND-TM program computing <span><span class="math inline">\(F\)</span></span> in <span><span class="math inline">\(T(n)\)</span></span> steps. We construct an oblivious NAND-TM program <span><span class="math inline">\(P\)</span></span> for computing <span><span class="math inline">\(F\)</span></span> as follows (see also <a href='#obliviousnandtmfig'>Figure 12.11</a>).</p>
<ol type="1">
<li><p>On input <span><span class="math inline">\(x\)</span></span>, <span><span class="math inline">\(P\)</span></span> will compute <span><span class="math inline">\(T=T(|x|)\)</span></span> and set up arrays <code>Atstart</code> and <code>Atend</code> satisfying <code>Atstart[</code><span><span class="math inline">\(0\)</span></span><code>]</code><span><span class="math inline">\(=1\)</span></span> and <code>Atstart[</code><span><span class="math inline">\(i\)</span></span><code>]</code><span><span class="math inline">\(=0\)</span></span> for <span><span class="math inline">\(i&gt;0\)</span></span> and <code>Atend[</code><span><span class="math inline">\(T-1\)</span></span><code>]</code><span><span class="math inline">\(=1\)</span></span> and <code>Atend[</code>i<code>]</code><span><span class="math inline">\(=0\)</span></span> for all <span><span class="math inline">\(i \neq T-1\)</span></span>. We can do this because <span><span class="math inline">\(T\)</span></span> is a nice function. Note that since this computation does not depend on <span><span class="math inline">\(x\)</span></span> but only on its length, it is oblivious.</p></li>
<li><p><span><span class="math inline">\(P\)</span></span> will also have a special array <code>Marker</code> initialized to all zeroes.</p></li>
<li><p>The index variable of <span><span class="math inline">\(P\)</span></span> will change direction of movement to the right whenever <code>Atstart[i]</code><span><span class="math inline">\(=1\)</span></span> and to the left whenever <code>Atend[i]</code><span><span class="math inline">\(=1\)</span></span>.</p></li>
<li><p>The program <span><span class="math inline">\(P\)</span></span> simulates the execution of <span><span class="math inline">\(P&#39;\)</span></span>. However, if the <code>MODANDJMP</code> instruction in <span><span class="math inline">\(P&#39;\)</span></span> attempts to move to the right when <span><span class="math inline">\(P\)</span></span> is moving left (or vice versa) then <span><span class="math inline">\(P\)</span></span> will set <code>Marker[i]</code> to <span><span class="math inline">\(1\)</span></span> and enter into a special “waiting mode”. In this mode <span><span class="math inline">\(P\)</span></span> will wait until the next time in which <code>Marker[i]</code><span><span class="math inline">\(=1\)</span></span> (at the next sweep) at which points <span><span class="math inline">\(P\)</span></span> zeroes <code>Marker[i]</code> and continues with the simulation. In the worst case this will take <span><span class="math inline">\(2T(n)\)</span></span> steps (if <span><span class="math inline">\(P\)</span></span> has to go all the way from one end to the other and back again.)</p></li>
<li><p>We also modify <span><span class="math inline">\(P\)</span></span> to ensure it ends the computation after simulating exactly <span><span class="math inline">\(T(n)\)</span></span> steps of <span><span class="math inline">\(P&#39;\)</span></span>, adding “dummy steps” if <span><span class="math inline">\(P&#39;\)</span></span> ends early.</p></li>
</ol>
<p>We see that <span><span class="math inline">\(P\)</span></span> simulates the execution of <span><span class="math inline">\(P&#39;\)</span></span> with an overhead of <span><span class="math inline">\(O(T(n))\)</span></span> steps of <span><span class="math inline">\(P\)</span></span> per one step of <span><span class="math inline">\(P&#39;\)</span></span>, hence completing the proof.</p>
</div>
<p><a href='#obliviousnandtmthm'>Theorem 12.13</a> implies <a href='#non-uniform-thm'>Theorem 12.12</a>. Indeed, if <span><span class="math inline">\(P\)</span></span> is a <span><span class="math inline">\(k\)</span></span>-line oblivious NAND-TM program computing <span><span class="math inline">\(F\)</span></span> in time <span><span class="math inline">\(T(n)\)</span></span> then for every <span><span class="math inline">\(n\)</span></span> we can obtain a NAND-CIRC program of <span><span class="math inline">\((k-1)\cdot T(n)\)</span></span> lines by simply making <span><span class="math inline">\(T(n)\)</span></span> copies of <span><span class="math inline">\(P\)</span></span> (dropping the final <code>MODANDJMP</code> line). In the <span><span class="math inline">\(j\)</span></span>-th copy we replace all references of the form <code>Foo[i]</code> to <code>foo_</code><span><span class="math inline">\(i_j\)</span></span> where <span><span class="math inline">\(i_j\)</span></span> is the value of <code>i</code> in the <span><span class="math inline">\(j\)</span></span>-th iteration.</p>
<h3 id="unrollloopsec" data-number="12.6.2">“Unrolling the loop”: algorithmic transformation of Turing Machines to circuits</h3>
<p>The proof of <a href='#non-uniform-thm'>Theorem 12.12</a> is <em>algorithmic</em>, in the sense that the proof yields a polynomial-time algorithm that given a Turing Machine <span><span class="math inline">\(M\)</span></span> and parameters <span><span class="math inline">\(T\)</span></span> and <span><span class="math inline">\(n\)</span></span>, produces a circuit of <span><span class="math inline">\(O(T^2)\)</span></span> gates that agrees with <span><span class="math inline">\(M\)</span></span> on all inputs <span><span class="math inline">\(x\in \{0,1\}^n\)</span></span> (as long as <span><span class="math inline">\(M\)</span></span> runs for less than <span><span class="math inline">\(T\)</span></span> steps these inputs.) We record this fact in the following theorem, since it will be useful for us later on:</p>
<figure>
<img src="../figure/unrollloop_alg.png" alt="12.12: The function \ensuremath{\mathit{UNROLL}} takes as input a Turing Machine M, an input length parameter n, a step budget parameter T, and outputs a circuit C of size poly(T) that takes n bits of inputs and outputs M(x) if M halts on x within at most T steps." id="unrollloopfig" class="margin" /><figcaption>12.12: The function <span><span class="math inline">\(\ensuremath{\mathit{UNROLL}}\)</span></span> takes as input a Turing Machine <span><span class="math inline">\(M\)</span></span>, an input length parameter <span><span class="math inline">\(n\)</span></span>, a step budget parameter <span><span class="math inline">\(T\)</span></span>, and outputs a circuit <span><span class="math inline">\(C\)</span></span> of size <span><span class="math inline">\(poly(T)\)</span></span> that takes <span><span class="math inline">\(n\)</span></span> bits of inputs and outputs <span><span class="math inline">\(M(x)\)</span></span> if <span><span class="math inline">\(M\)</span></span> halts on <span><span class="math inline">\(x\)</span></span> within at most <span><span class="math inline">\(T\)</span></span> steps.</figcaption>
</figure>
<div id="nand-compiler" class="theorem" title="Turing-machine to circuit compiler" name="Theorem 12.14 (Turing-machine to circuit compiler) ">
<p>There is algorithm <span><span class="math inline">\(\ensuremath{\mathit{UNROLL}}\)</span></span> such that for every Turing Machine <span><span class="math inline">\(M\)</span></span> and numbers <span><span class="math inline">\(n,T\)</span></span>, <span><span class="math inline">\(\ensuremath{\mathit{UNROLL}}(M,1^T,1^n)\)</span></span> runs for <span><span class="math inline">\(poly(|M|,T,n)\)</span></span> steps and outputs a NAND circuit <span><span class="math inline">\(C\)</span></span> with <span><span class="math inline">\(n\)</span></span> inputs, <span><span class="math inline">\(O(T^2)\)</span></span> gates, and one output, such that <span>
<div class='myequationbox'><span class="math display">\[
C(x) = \begin{cases}y &amp; M \text{ halts in $\leq T$ steps and outputs $y$} \\ 0 &amp; \text{otherwise} \end{cases}\;.
\]</span></div></span></p>
</div>
<div class="proof" data-ref="nand-compiler" name="Solution 12.6.2">
<p>We only sketch the proof since it follows by directly translating the proof of <a href="">non-uniform-thm</a>{.ref into an algorithm together with the simulation of Turing machines by NAND-TM programs (see also <a href='#unrolldescriptionfig'>Figure 12.13</a>). Specifically, <span><span class="math inline">\(\ensuremath{\mathit{UNROLL}}\)</span></span> does the following:</p>
<ol type="1">
<li><p>Transform the Turing Machine <span><span class="math inline">\(M\)</span></span> into an equivalent NAND-TM program <span><span class="math inline">\(P\)</span></span>.</p></li>
<li><p>Transform the NAND-TM program <span><span class="math inline">\(P\)</span></span> into an equivalent oblivious program <span><span class="math inline">\(P&#39;\)</span></span> following the proof of <a href='#obliviousnandtmthm'>Theorem 12.13</a>. The program <span><span class="math inline">\(P&#39;\)</span></span> takes <span><span class="math inline">\(T&#39; = O(T^2)\)</span></span> steps to simulate <span><span class="math inline">\(T\)</span></span> steps of <span><span class="math inline">\(P\)</span></span>.</p></li>
<li><p>“Unroll the loop” of <span><span class="math inline">\(P&#39;\)</span></span> by obtaining a NAND-CIRC program of <span><span class="math inline">\(O(T&#39;)\)</span></span> lines (or equivalently a NAND circuit with <span><span class="math inline">\(O(T^2)\)</span></span> gates) corresponding to the execution of <span><span class="math inline">\(T&#39;\)</span></span> iterations of <span><span class="math inline">\(P&#39;\)</span></span>.</p></li>
</ol>
</div>
<figure>
<img src="../figure/unrolldescription.png" alt="12.13: We can transform a Turing Machine M, input length parameter n, and time bound T into an O(T^2) sized NAND circuit that agrees with M on all inputs x\in \{0,1\}^n on which M halts in at most T steps. The transformation is obtained by first using the equivalence of Turing Machines and NAND-TM programs P, then turning P into an equivalent oblivious NAND-TM program P&#39; via , then “unrolling” O(T^2) iterations of the loop of P&#39; to obtain an O(T^2) line NAND-CIRC program that agrees with P&#39; on length n inputs, and finally translating this program into an equivalent circuit." id="unrolldescriptionfig" /><figcaption>12.13: We can transform a Turing Machine <span><span class="math inline">\(M\)</span></span>, input length parameter <span><span class="math inline">\(n\)</span></span>, and time bound <span><span class="math inline">\(T\)</span></span> into an <span><span class="math inline">\(O(T^2)\)</span></span> sized NAND circuit that agrees with <span><span class="math inline">\(M\)</span></span> on all inputs <span><span class="math inline">\(x\in \{0,1\}^n\)</span></span> on which <span><span class="math inline">\(M\)</span></span> halts in at most <span><span class="math inline">\(T\)</span></span> steps. The transformation is obtained by first using the equivalence of Turing Machines and NAND-TM programs <span><span class="math inline">\(P\)</span></span>, then turning <span><span class="math inline">\(P\)</span></span> into an equivalent <em>oblivious</em> NAND-TM program <span><span class="math inline">\(P&#39;\)</span></span> via <a href='#obliviousnandtmthm'>Theorem 12.13</a>, then “unrolling” <span><span class="math inline">\(O(T^2)\)</span></span> iterations of the loop of <span><span class="math inline">\(P&#39;\)</span></span> to obtain an <span><span class="math inline">\(O(T^2)\)</span></span> line NAND-CIRC program that agrees with <span><span class="math inline">\(P&#39;\)</span></span> on length <span><span class="math inline">\(n\)</span></span> inputs, and finally translating this program into an equivalent circuit.</figcaption>
</figure>
<div id="unrollloop" class="bigidea" name="Bigidea 19">
<p>By “unrolling the loop” we can transform an algorithm that takes <span><span class="math inline">\(T(n)\)</span></span> steps to compute <span><span class="math inline">\(F\)</span></span> into a circuit that uses <span><span class="math inline">\(poly(T(n))\)</span></span> gates to compute the restriction of <span><span class="math inline">\(F\)</span></span> to <span><span class="math inline">\(\{0,1\}^n\)</span></span>.</p>
</div>
<div class="pause" name="Pause 12.6.2">
<p>Reviewing the transformations described in <a href='#unrolldescriptionfig'>Figure 12.13</a>, as well as solving the following two exercises is a great way to get more comfort with non-uniform complexity and in particular with <span><span class="math inline">\(\mathbf{P_{/poly}}\)</span></span> and its relation to <span><span class="math inline">\(\mathbf{P}\)</span></span>.</p>
</div>
<div id="characterizationofp" class="solvedexercise" title="Alternative characterization of $\mathbf{P}$" name="Solvedexercise 12.4 (Alternative characterization of $\mathbf{P}$) ">
<p>Prove that for every <span><span class="math inline">\(F:\{0,1\}^* \rightarrow \{0,1\}\)</span></span>, <span><span class="math inline">\(F\in \mathbf{P}\)</span></span> if and only if there is a polynomial-time Turing Machine <span><span class="math inline">\(M\)</span></span> such that for every <span><span class="math inline">\(n\in \N\)</span></span>, <span><span class="math inline">\(M(1^n)\)</span></span> outputs a description of an <span><span class="math inline">\(n\)</span></span> input circuit <span><span class="math inline">\(C_n\)</span></span> that computes the restriction <span><span class="math inline">\(F_{\upharpoonright n}\)</span></span> of <span><span class="math inline">\(F\)</span></span> to inputs in <span><span class="math inline">\(\{0,1\}^n\)</span></span>.</p>
</div>
<div class="solution" data-ref="characterizationofp" name="Solution 12.6.2">
<p>We start with the “if” direction. Suppose that there is a polynomial-time Turing Machine <span><span class="math inline">\(M\)</span></span> that on input <span><span class="math inline">\(1^n\)</span></span> outputs a circuit <span><span class="math inline">\(C_n\)</span></span> that computes <span><span class="math inline">\(F_{\upharpoonright n}\)</span></span>. Then the following is a polynomial-time Turing Machine <span><span class="math inline">\(M&#39;\)</span></span> to compute <span><span class="math inline">\(F\)</span></span>. On input <span><span class="math inline">\(x\in \{0,1\}^*\)</span></span>, <span><span class="math inline">\(M&#39;\)</span></span> will:</p>
<ol type="1">
<li><p>Let <span><span class="math inline">\(n=|x|\)</span></span> and compute <span><span class="math inline">\(C_n = M(1^n)\)</span></span>.</p></li>
<li><p>Return the evaluation of <span><span class="math inline">\(C_n\)</span></span> on <span><span class="math inline">\(x\)</span></span>.</p></li>
</ol>
<p>Since we can evaluate a Boolean circuit on an input in polynomial time, <span><span class="math inline">\(M&#39;\)</span></span> runs in polynomial time and computes <span><span class="math inline">\(F(x)\)</span></span> on every input <span><span class="math inline">\(x\)</span></span>.</p>
<p>For the “only if” direction, if <span><span class="math inline">\(M&#39;\)</span></span> is a Turing Machine that computes <span><span class="math inline">\(F\)</span></span> in polynomial-time, then (applying the equivalence of Turing Machines and NAND-TM as well as <a href='#obliviousnandtmthm'>Theorem 12.13</a>) there is also an oblivious NAND-TM program <span><span class="math inline">\(P\)</span></span> that computes <span><span class="math inline">\(F\)</span></span> in time <span><span class="math inline">\(p(n)\)</span></span> for some polynomial <span><span class="math inline">\(p\)</span></span>. We can now define <span><span class="math inline">\(M\)</span></span> to be the Turing Machine that on input <span><span class="math inline">\(1^n\)</span></span> outputs the NAND circuit obtained by “unrolling the loop” of <span><span class="math inline">\(P\)</span></span> for <span><span class="math inline">\(p(n)\)</span></span> iterations. The resulting NAND circuit computes <span><span class="math inline">\(F_{\upharpoonright n}\)</span></span> and has <span><span class="math inline">\(O(p(n))\)</span></span> gates. It can also be transformed to a Boolean circuit with <span><span class="math inline">\(O(p(n))\)</span></span> AND/OR/NOT gates.</p>
</div>
<div id="adviceppoly" class="solvedexercise" title="$\mathbf{P_{/poly}}$ characterization by advice" name="Solvedexercise 12.5 ($\mathbf{P_{/poly}}$ characterization by advice) ">
<p>Let <span><span class="math inline">\(F:\{0,1\}^* \rightarrow \{0,1\}\)</span></span>. Then <span><span class="math inline">\(F\in\mathbf{P_{/poly}}\)</span></span> if and only if there exists a polynomial <span><span class="math inline">\(p:\N \rightarrow \N\)</span></span>, a polynomial-time Turing Machine <span><span class="math inline">\(M\)</span></span> and a sequence <span><span class="math inline">\(\{ a_n \}_{n\in \N}\)</span></span> of strings, such that for every <span><span class="math inline">\(n\in \N\)</span></span>:</p>
<ul>
<li><span><span class="math inline">\(|a_n| \leq p(n)\)</span></span><br />
</li>
<li>For every <span><span class="math inline">\(x\in \{0,1\}^n\)</span></span>, <span><span class="math inline">\(M(a_n,x)=F(x)\)</span></span>.</li>
</ul>
</div>
<div class="solution" data-ref="adviceppoly" name="Solution 12.6.2">
<p>We only sketch the proof. For the “only if” direction, if <span><span class="math inline">\(F\in \mathbf{P_{/poly}}\)</span></span> then we can use for <span><span class="math inline">\(a_n\)</span></span> simply the description of the corresponding circuit <span><span class="math inline">\(C_n\)</span></span> and for <span><span class="math inline">\(M\)</span></span> the program that computes in polynomial time the evaluation of a circuit on its input.</p>
<p>For the “if” direction, we can use the same “unrolling the loop” technique of <a href='#non-uniform-thm'>Theorem 12.12</a> to show that if <span><span class="math inline">\(P\)</span></span> is a polynomial-time NAND-TM program, then for every <span><span class="math inline">\(n\in \N\)</span></span>, the map <span><span class="math inline">\(x \mapsto P(a_n,x)\)</span></span> can be computed by a polynomial size NAND-CIRC program <span><span class="math inline">\(Q_n\)</span></span>.</p>
</div>
<h3 id="can-uniform-algorithms-simulate-non-uniform-ones" data-number="12.6.3">Can uniform algorithms simulate non uniform ones?</h3>
<p><a href='#non-uniform-thm'>Theorem 12.12</a> shows that every function in <span><span class="math inline">\(\ensuremath{\mathit{TIME}}(T(n))\)</span></span> is in <span><span class="math inline">\(\ensuremath{\mathit{SIZE}}(poly(T(n)))\)</span></span>. One can ask if there is an inverse relation. Suppose that <span><span class="math inline">\(F\)</span></span> is such that <span><span class="math inline">\(F_{\upharpoonright n}\)</span></span> has a “short” NAND-CIRC program for every <span><span class="math inline">\(n\)</span></span>. Can we say that it must be in <span><span class="math inline">\(\ensuremath{\mathit{TIME}}(T(n))\)</span></span> for some “small” <span><span class="math inline">\(T\)</span></span>? The answer is an emphatic <strong>no</strong>. Not only is <span><span class="math inline">\(\mathbf{P_{/poly}}\)</span></span> not contained in <span><span class="math inline">\(\mathbf{P}\)</span></span>, in fact <span><span class="math inline">\(\mathbf{P_{/poly}}\)</span></span> contains functions that are <em>uncomputable</em>!</p>
<div id="Ppolyuncomputable" class="theorem" title="$\mathbf{P_{/poly}}$ contains uncomputable functions" name="Theorem 12.15 ($\mathbf{P_{/poly}}$ contains uncomputable functions) ">
<p>There exists an <em>uncomputable</em> function <span><span class="math inline">\(F:\{0,1\}^* \rightarrow \{0,1\}\)</span></span> such that <span><span class="math inline">\(F \in \mathbf{P_{/poly}}\)</span></span>.</p>
</div>
<div id="section-5" class="proofidea" data-ref="Ppolyuncomputable" name="Proofidea">
<p>Since <span><span class="math inline">\(\mathbf{P_{/poly}}\)</span></span> corresponds to non uniform computation, a function <span><span class="math inline">\(F\)</span></span> is in <span><span class="math inline">\(\mathbf{P_{/poly}}\)</span></span> if for every <span><span class="math inline">\(n\in \N\)</span></span>, the restriction <span><span class="math inline">\(F_{\upharpoonright n}\)</span></span> to inputs of length <span><span class="math inline">\(n\)</span></span> has a small circuit/program, even if the circuits for different values of <span><span class="math inline">\(n\)</span></span> are completely different from one another. In particular, if <span><span class="math inline">\(F\)</span></span> has the property that for every equal-length inputs <span><span class="math inline">\(x\)</span></span> and <span><span class="math inline">\(x&#39;\)</span></span>, <span><span class="math inline">\(F(x)=F(x&#39;)\)</span></span> then this means that <span><span class="math inline">\(F_{\upharpoonright n}\)</span></span> is either the constant function zero or the constant function one for every <span><span class="math inline">\(n\in \N\)</span></span>. Since the constant function has a (very!) small circuit, such a function <span><span class="math inline">\(F\)</span></span> will always be in <span><span class="math inline">\(\mathbf{P_{/poly}}\)</span></span> (indeed even in smaller classes). Yet by a reduction from the Halting problem, we can obtain a function with this property that is uncomputable.</p>
</div>
<div class="proof" data-ref="Ppolyuncomputable" name="Proof 12.6.3">
<p>Consider the following “unary halting function” <span><span class="math inline">\(\ensuremath{\mathit{UH}}:\{0,1\}^* \rightarrow \{0,1\}\)</span></span> defined as follows. We let <span><span class="math inline">\(S:\N \rightarrow \{0,1\}^*\)</span></span> be the function that on input <span><span class="math inline">\(n\in \N\)</span></span>, outputs the string that corresponds to the binary representation of the number <span><span class="math inline">\(n\)</span></span> without the most significant <span><span class="math inline">\(1\)</span></span> digit. Note that <span><span class="math inline">\(S\)</span></span> is <em>onto</em>. For every <span><span class="math inline">\(x\in \{0,1\}\)</span></span>, we define <span><span class="math inline">\(\ensuremath{\mathit{UH}}(x)=\ensuremath{\mathit{HALTONZERO}}(S(|x|))\)</span></span>. That is, if <span><span class="math inline">\(n\)</span></span> is the length of <span><span class="math inline">\(x\)</span></span>, then <span><span class="math inline">\(\ensuremath{\mathit{UH}}(x)=1\)</span></span> if and only if the string <span><span class="math inline">\(S(n)\)</span></span> encodes a NAND-TM program that halts on the input <span><span class="math inline">\(0\)</span></span>.</p>
<p><span><span class="math inline">\(\ensuremath{\mathit{UH}}\)</span></span> is uncomputable, since otherwise we could compute <span><span class="math inline">\(\ensuremath{\mathit{HALTONZERO}}\)</span></span> by transforming the input program <span><span class="math inline">\(P\)</span></span> into the integer <span><span class="math inline">\(n\)</span></span> such that <span><span class="math inline">\(P=S(n)\)</span></span> and then running <span><span class="math inline">\(\ensuremath{\mathit{UH}}(1^n)\)</span></span> (i.e., <span><span class="math inline">\(\ensuremath{\mathit{UH}}\)</span></span> on the string of <span><span class="math inline">\(n\)</span></span> ones). On the other hand, for every <span><span class="math inline">\(n\)</span></span>, <span><span class="math inline">\(\ensuremath{\mathit{UH}}_n(x)\)</span></span> is either equal to <span><span class="math inline">\(0\)</span></span> for all inputs <span><span class="math inline">\(x\)</span></span> or equal to <span><span class="math inline">\(1\)</span></span> on all inputs <span><span class="math inline">\(x\)</span></span>, and hence can be computed by a NAND-CIRC program of a <em>constant</em> number of lines.</p>
</div>
<p>The issue here is of course <em>uniformity</em>. For a function <span><span class="math inline">\(F:\{0,1\}^* \rightarrow \{0,1\}\)</span></span>, if <span><span class="math inline">\(F\)</span></span> is in <span><span class="math inline">\(\ensuremath{\mathit{TIME}}(T(n))\)</span></span> then we have a <em>single</em> algorithm that can compute <span><span class="math inline">\(F_{\upharpoonright n}\)</span></span> for every <span><span class="math inline">\(n\)</span></span>. On the other hand, <span><span class="math inline">\(F_{\upharpoonright n}\)</span></span> might be in <span><span class="math inline">\(\ensuremath{\mathit{SIZE}}(T(n))\)</span></span> for every <span><span class="math inline">\(n\)</span></span> using a completely different algorithm for every input length. For this reason we typically use <span><span class="math inline">\(\mathbf{P_{/poly}}\)</span></span> not as a model of <em>efficient</em> computation but rather as a way to model <em>inefficient computation</em>. For example, in cryptography people often define an encryption scheme to be secure if breaking it for a key of length <span><span class="math inline">\(n\)</span></span> requires more than a polynomial number of NAND lines. Since <span><span class="math inline">\(\mathbf{P} \subseteq \mathbf{P_{/poly}}\)</span></span>, this in particular precludes a polynomial time algorithm for doing so, but there are technical reasons why working in a non uniform model makes more sense in cryptography. It also allows to talk about security in non asymptotic terms such as a scheme having “<span><span class="math inline">\(128\)</span></span> bits of security”.</p>
<p>While it can sometimes be a real issue, in many natural settings the difference between uniform and non-uniform computation does not seem to so important. In particular, in all the examples of problems not known to be in <span><span class="math inline">\(\mathbf{P}\)</span></span> we discussed before: longest path, 3SAT, factoring, etc., these problems are also not known to be in <span><span class="math inline">\(\mathbf{P_{/poly}}\)</span></span> either. Thus, for “natural” functions, if you pretend that <span><span class="math inline">\(\ensuremath{\mathit{TIME}}(T(n))\)</span></span> is roughly the same as <span><span class="math inline">\(\ensuremath{\mathit{SIZE}}(T(n))\)</span></span>, you will be right more often than wrong.</p>
<figure>
<img src="../figure/PEXPPpolyrelations.png" alt="12.14: Relations between \mathbf{P}, \mathbf{EXP}, and \mathbf{P_{/poly}}. It is known that \mathbf{P} \subseteq \mathbf{EXP}, \mathbf{P} \subseteq \mathbf{P_{/poly}} and that \mathbf{P_{/poly}} contains uncomputable functions (which in particular are outside of \mathbf{EXP}). It is not known whether or not \mathbf{EXP} \subseteq \mathbf{P_{/poly}} though it is believed that \mathbf{EXP} \not\subseteq \mathbf{P_{/poly}}." id="PEXPPpolyrelationsfig" /><figcaption>12.14: Relations between <span><span class="math inline">\(\mathbf{P}\)</span></span>, <span><span class="math inline">\(\mathbf{EXP}\)</span></span>, and <span><span class="math inline">\(\mathbf{P_{/poly}}\)</span></span>. It is known that <span><span class="math inline">\(\mathbf{P} \subseteq \mathbf{EXP}\)</span></span>, <span><span class="math inline">\(\mathbf{P} \subseteq \mathbf{P_{/poly}}\)</span></span> and that <span><span class="math inline">\(\mathbf{P_{/poly}}\)</span></span> contains uncomputable functions (which in particular are outside of <span><span class="math inline">\(\mathbf{EXP}\)</span></span>). It is not known whether or not <span><span class="math inline">\(\mathbf{EXP} \subseteq \mathbf{P_{/poly}}\)</span></span> though it is believed that <span><span class="math inline">\(\mathbf{EXP} \not\subseteq \mathbf{P_{/poly}}\)</span></span>.</figcaption>
</figure>
<h3 id="uniform-vs.-nonuniform-computation-a-recap" data-number="12.6.4">Uniform vs. Nonuniform computation: A recap</h3>
<p>To summarize, the two models of computation we have described so far are:</p>
<ul>
<li><p><strong>Uniform models:</strong> <em>Turing machines</em>, <em>NAND-TM programs</em>, <em>RAM machines</em>, <em>NAND-RAM programs</em>, <em>C/JavaScript/Python</em>, etc. These model include loops and unbounded memory hence a single program can compute a function with unbounded input length.</p></li>
<li><p><strong>Non-uniform models:</strong> <em>Boolean Circuits</em> or <em>straightline programs</em> have no loops and can only compute finite functions. The time to execute them is exactly the number of lines or gates they contain.</p></li>
</ul>
<p>For a function <span><span class="math inline">\(F:\{0,1\}^* \rightarrow \{0,1\}\)</span></span> and some nice time bound <span><span class="math inline">\(T:\N \rightarrow \N\)</span></span>, we know that:</p>
<ul>
<li><p>If <span><span class="math inline">\(F\)</span></span> is uniformly computable in time <span><span class="math inline">\(T(n)\)</span></span> then there is a sequence of circuits <span><span class="math inline">\(C_1,C_2,\ldots\)</span></span> where <span><span class="math inline">\(C_n\)</span></span> has <span><span class="math inline">\(poly(T(n))\)</span></span> gates and computes <span><span class="math inline">\(F_{\upharpoonright n}\)</span></span> (i.e., restriction of <span><span class="math inline">\(F\)</span></span> to <span><span class="math inline">\(\{0,1\}^n\)</span></span>) for every <span><span class="math inline">\(n\)</span></span>.</p></li>
<li><p>The reverse direction is not necessarily true - there are examples of functions <span><span class="math inline">\(F:\{0,1\}^n \rightarrow \{0,1\}\)</span></span> such that <span><span class="math inline">\(F_{\upharpoonright n}\)</span></span> can be computed by even a constant size circuit but <span><span class="math inline">\(F\)</span></span> is uncomputable.</p></li>
</ul>
<p>This means that non uniform complexity is more useful to establish <em>hardness</em> of a function than its <em>easiness</em>.</p>
<div class="recap" name="Recap 12.6.4">
<ul>
<li><p>We can define the time complexity of a function using NAND-TM programs, and similarly to the notion of computability, this appears to capture the inherent complexity of the function.</p></li>
<li><p>There are many natural problems that have polynomial-time algorithms, and other natural problems that we’d love to solve, but for which the best known algorithms are exponential.</p></li>
<li><p>The definition of polynomial time, and hence the class <span><span class="math inline">\(\mathbf{P}\)</span></span>, is robust to the choice of model, whether it is Turing machines, NAND-TM, NAND-RAM, modern programming languages, and many other models.</p></li>
<li><p>The time hierarchy theorem shows that there are <em>some</em> problems that can be solved in exponential, but not in polynomial time. However, we do not know if that is the case for the natural examples that we described in this lecture.</p></li>
<li><p>By “unrolling the loop” we can show that every function computable in time <span><span class="math inline">\(T(n)\)</span></span> can be computed by a sequence of NAND-CIRC programs (one for every input length) each of size at most <span><span class="math inline">\(poly(T(n))\)</span></span></p></li>
</ul>
</div>
<h2 id="exercises" data-number="12.7">Exercises</h2>
<div id="definitionofP" class="exercise" title="Equivalence of different definitions of $\mathbf{P}$ and $\mathbf{EXP}$." name="Exercise 12.1 (Equivalence of different definitions of $\mathbf{P}$ and $\mathbf{EXP}$.) ">
<p>Prove that the classes <span><span class="math inline">\(\mathbf{P}\)</span></span> and <span><span class="math inline">\(\mathbf{EXP}\)</span></span> defined in <a href='#PandEXPdef'>Definition 12.2</a> are equal to <span><span class="math inline">\(\cup_{c\in \{1,2,3,\ldots \}} \ensuremath{\mathit{TIME}}(n^c)\)</span></span> and <span><span class="math inline">\(\cup_{c\in \{1,2,3,\ldots \}} \ensuremath{\mathit{TIME}}(2^{n^c})\)</span></span> respectively. (If <span><span class="math inline">\(S_1,S_2,S_3,\ldots\)</span></span> is a collection of sets then the set <span><span class="math inline">\(S = \cup_{c\in \{1,2,3,\ldots \}} S_c\)</span></span> is the set of all elements <span><span class="math inline">\(e\)</span></span> such that there exists some <span><span class="math inline">\(c\in \{ 1,2,3,\ldots \}\)</span></span> where <span><span class="math inline">\(e\in S_c\)</span></span>.)</p>
</div>
<div id="robsutrepresex" class="exercise" title="Robustness to representation" name="Exercise 12.2 (Robustness to representation) ">
<p><a href='#polyRAMTM-thm'>Theorem 12.5</a> shows that the classes <span><span class="math inline">\(\mathbf{P}\)</span></span> and <span><span class="math inline">\(\mathbf{EXP}\)</span></span> are <em>robust</em> with respect to variations in the choice of the computational model. This exercise shows that these classes are also robust with respect to our choice of the representation of the input.</p>
<p>Specifically, let <span><span class="math inline">\(F\)</span></span> be a function mapping graphs to <span><span class="math inline">\(\{0,1\}\)</span></span>, and let <span><span class="math inline">\(F&#39;, F&#39;&#39;:\{0,1\}^* \rightarrow \{0,1\}\)</span></span> be the functions defined as follows. For every <span><span class="math inline">\(x\in \{0,1\}^*\)</span></span>:</p>
<ul>
<li><p><span><span class="math inline">\(F&#39;(x)=1\)</span></span> iff <span><span class="math inline">\(x\)</span></span> represents a graph <span><span class="math inline">\(G\)</span></span> via the adjacency matrix representation such that <span><span class="math inline">\(F(G)=1\)</span></span>.</p></li>
<li><p><span><span class="math inline">\(F&#39;&#39;(x)=1\)</span></span> iff <span><span class="math inline">\(x\)</span></span> represents a graph <span><span class="math inline">\(G\)</span></span> via the adjacency list representation such that <span><span class="math inline">\(F(G)=1\)</span></span>.</p></li>
</ul>
<p>Prove that <span><span class="math inline">\(F&#39; \in \mathbf{P}\)</span></span> iff <span><span class="math inline">\(F&#39;&#39; \in \mathbf{P}\)</span></span>.</p>
<p>More generally, for every function <span><span class="math inline">\(F:\{0,1\}^* \rightarrow \{0,1\}\)</span></span>, the answer to the question of whether <span><span class="math inline">\(F\in \mathbf{P}\)</span></span> (or whether <span><span class="math inline">\(F\in \mathbf{EXP}\)</span></span>) is unchanged by switching representations, as long as transforming one representation to the other can be done in polynomial time (which essentially holds for all reasonable representations).</p>
</div>
<div id="boolex" class="exercise" title="Boolean functions" name="Exercise 12.3 (Boolean functions) ">
<p>For every function <span><span class="math inline">\(F:\{0,1\}^* \rightarrow \{0,1\}^*\)</span></span>, define <span><span class="math inline">\(Bool(F)\)</span></span> to be the function mapping <span><span class="math inline">\(\{0,1\}^*\)</span></span> to <span><span class="math inline">\(\{0,1\}\)</span></span> such that on input a (string representation of a) triple <span><span class="math inline">\((x,i,\sigma)\)</span></span> with <span><span class="math inline">\(x\in \{0,1\}^*\)</span></span>, <span><span class="math inline">\(i \in \N\)</span></span> and <span><span class="math inline">\(\sigma \in \{0,1\}\)</span></span>,</p>
<p><span>
<div class='myequationbox'><span class="math display">\[
Bool(F)(x,i,\sigma) = \begin{cases} F(x)_i &amp; \sigma =0, i&lt;|F(x)| \\
                                    1      &amp; \sigma = 1,i&lt;|F(x)| \\
                                 0   &amp; \text{otherwise} \end{cases}
\]</span></div></span> where <span><span class="math inline">\(F(x)_i\)</span></span> is the <span><span class="math inline">\(i\)</span></span>-th bit of the string <span><span class="math inline">\(F(x)\)</span></span>.</p>
<p>Prove that <span><span class="math inline">\(F \in \overline{\mathbf{P}}\)</span></span> if and only if <span><span class="math inline">\(Bool(F) \in \mathbf{P}\)</span></span>.</p>
</div>
<div id="poly-time-comp-ex" class="exercise" title="Composition of polynomial time" name="Exercise 12.4 (Composition of polynomial time) ">
<p>Prove that if <span><span class="math inline">\(F,G:\{0,1\}^* \rightarrow \{0,1\}^*\)</span></span> are in <span><span class="math inline">\(\overline{\mathbf{P}}\)</span></span> then their <em>composition</em> <span><span class="math inline">\(F\circ G\)</span></span>, which is the function <span><span class="math inline">\(H\)</span></span> s.t. <span><span class="math inline">\(H(x)=F(G(x))\)</span></span>, is also in <span><span class="math inline">\(\overline{\mathbf{P}}\)</span></span>.</p>
</div>
<div id="exp-time-comp-ex" class="exercise" title="Non composition of exponential time" name="Exercise 12.5 (Non composition of exponential time) ">
<p>Prove that there is some <span><span class="math inline">\(F,G:\{0,1\}^* \rightarrow \{0,1\}^*\)</span></span> s.t. <span><span class="math inline">\(F,G \in \overline{\mathbf{EXP}}\)</span></span> but <span><span class="math inline">\(F\circ G\)</span></span> is not in <span><span class="math inline">\(\mathbf{EXP}\)</span></span>.</p>
</div>
<div id="oblivious-ex" class="exercise" title="Oblivious Turing Machines" name="Exercise 12.6 (Oblivious Turing Machines) ">
<p>We say that a Turing machine <span><span class="math inline">\(M\)</span></span> is <em>oblivious</em> if there is some function <span><span class="math inline">\(T:\N\times \N \rightarrow \Z\)</span></span> such that for every input <span><span class="math inline">\(x\)</span></span> of length <span><span class="math inline">\(n\)</span></span>, and <span><span class="math inline">\(t\in \N\)</span></span> it holds that:</p>
<ul>
<li><p>If <span><span class="math inline">\(M\)</span></span> takes more than <span><span class="math inline">\(t\)</span></span> steps to halt on the input <span><span class="math inline">\(x\)</span></span>, then in the <span><span class="math inline">\(t\)</span></span>-th step <span><span class="math inline">\(M\)</span></span>’s head will be in the position <span><span class="math inline">\(T(n,t)\)</span></span>. (Note that this position depends only on the <em>length</em> of <span><span class="math inline">\(x\)</span></span> and not its contents.)</p></li>
<li><p>If <span><span class="math inline">\(M\)</span></span> halts before the <span><span class="math inline">\(t\)</span></span>-th step then <span><span class="math inline">\(T(n,t) = -1\)</span></span>.</p></li>
</ul>
<p>Prove that if <span><span class="math inline">\(F\in \mathbf{P}\)</span></span> then there exists an <em>oblivious</em> Turing machine <span><span class="math inline">\(M\)</span></span> that computes <span><span class="math inline">\(F\)</span></span> in polynomial time. See footnote for hint.<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup></p>
</div>
<div id="graphedgeex" class="exercise" name="Exercise 12.7">
<p>Let <span><span class="math inline">\(\ensuremath{\mathit{EDGE}}:\{0,1\}^* \rightarrow \{0,1\}\)</span></span> be the function such that on input a string representing a triple <span><span class="math inline">\((L,i,j)\)</span></span>, where <span><span class="math inline">\(L\)</span></span> is the adjacency list representation of an <span><span class="math inline">\(n\)</span></span> vertex graph <span><span class="math inline">\(G\)</span></span>, and <span><span class="math inline">\(i\)</span></span> and <span><span class="math inline">\(j\)</span></span> are numbers in <span><span class="math inline">\([n]\)</span></span>, <span><span class="math inline">\(\ensuremath{\mathit{EDGE}}(L,i,j)=1\)</span></span> if the edge <span><span class="math inline">\(\{i,j \}\)</span></span> is present in the graph. <span><span class="math inline">\(\ensuremath{\mathit{EDGE}}\)</span></span> outputs <span><span class="math inline">\(0\)</span></span> on all other inputs.</p>
<ol type="1">
<li><p>Prove that <span><span class="math inline">\(\ensuremath{\mathit{EDGE}} \in \mathbf{P}\)</span></span>.</p></li>
<li><p>Let <span><span class="math inline">\(\ensuremath{\mathit{PLANARMATRIX}}:\{0,1\}^* \rightarrow \{0,1\}\)</span></span> be the function that on input an adjacency matrix <span><span class="math inline">\(A\)</span></span> outputs <span><span class="math inline">\(1\)</span></span> if and only if the graph represented by <span><span class="math inline">\(A\)</span></span> is <em>planar</em> (that is, can be drawn on the plane without edges crossing one another). For this question, you can use without proof the fact that <span><span class="math inline">\(\ensuremath{\mathit{PLANARMATRIX}} \in \mathbf{P}\)</span></span>. Prove that <span><span class="math inline">\(\ensuremath{\mathit{PLANARLIST}} \in \mathbf{P}\)</span></span> where <span><span class="math inline">\(\ensuremath{\mathit{PLANARLIST}}:\{0,1\}^* \rightarrow \{0,1\}\)</span></span> is the function that on input an adjacency list <span><span class="math inline">\(L\)</span></span> outputs <span><span class="math inline">\(1\)</span></span> if and only if <span><span class="math inline">\(L\)</span></span> represents a planar graph.</p></li>
</ol>
</div>
<div id="evalnandcircuit" class="exercise" title="Evaluate NAND circuits" name="Exercise 12.8 (Evaluate NAND circuits) ">
<p>Let <span><span class="math inline">\(\ensuremath{\mathit{NANDEVAL}}:\{0,1\}^* \rightarrow \{0,1\}\)</span></span> be the function such that for every string representing a pair <span><span class="math inline">\((Q,x)\)</span></span> where <span><span class="math inline">\(Q\)</span></span> is an <span><span class="math inline">\(n\)</span></span>-input <span><span class="math inline">\(1\)</span></span>-output NAND-CIRC (not NAND-TM!) program and <span><span class="math inline">\(x\in \{0,1\}^n\)</span></span>, <span><span class="math inline">\(\ensuremath{\mathit{NANDEVAL}}(Q,x)=Q(x)\)</span></span>. On all other inputs <span><span class="math inline">\(\ensuremath{\mathit{NANDEVAL}}\)</span></span> outputs <span><span class="math inline">\(0\)</span></span>.</p>
<p>Prove that <span><span class="math inline">\(\ensuremath{\mathit{NANDEVAL}} \in \mathbf{P}\)</span></span>.</p>
</div>
<div id="hardfunc" class="exercise" title="Find hard function" name="Exercise 12.9 (Find hard function) ">
<p>Let <span><span class="math inline">\(\ensuremath{\mathit{NANDHARD}}:\{0,1\}^* \rightarrow \{0,1\}\)</span></span> be the function such that on input a string representing a pair <span><span class="math inline">\((f,s)\)</span></span> where</p>
<ul>
<li><span><span class="math inline">\(f \in \{0,1\}^{2^n}\)</span></span> for some <span><span class="math inline">\(n\in \mathbb{N}\)</span></span></li>
<li><span><span class="math inline">\(s\in \mathbb{N}\)</span></span></li>
</ul>
<p><span><span class="math inline">\(\ensuremath{\mathit{NANDHARD}}(f,s)=1\)</span></span> if there is no NAND-CIRC program <span><span class="math inline">\(Q\)</span></span> of at most <span><span class="math inline">\(s\)</span></span> lines that computes the function <span><span class="math inline">\(F:\{0,1\}^n \rightarrow \{0,1\}\)</span></span> whose truth table is the string <span><span class="math inline">\(f\)</span></span>. That is, <span><span class="math inline">\(\ensuremath{\mathit{NANDHARD}}(f,s)=1\)</span></span> if for every NAND-CIRC program <span><span class="math inline">\(Q\)</span></span> of at most <span><span class="math inline">\(s\)</span></span> lines, there exists some <span><span class="math inline">\(x\in \{0,1\}^{n}\)</span></span> such that <span><span class="math inline">\(Q(x) \neq f_x\)</span></span> where <span><span class="math inline">\(f_x\)</span></span> denote the <span><span class="math inline">\(x\)</span></span>-the coordinate of <span><span class="math inline">\(f\)</span></span>, using the binary representation to identify <span><span class="math inline">\(\{0,1\}^n\)</span></span> with the numbers <span><span class="math inline">\(\{0,\ldots,2^n -1 \}\)</span></span>.</p>
<ol type="1">
<li><p>Prove that <span><span class="math inline">\(\ensuremath{\mathit{NANDHARD}} \in \mathbf{EXP}\)</span></span>.</p></li>
<li><p>(Challenge) Prove that there is an algorithm <span><span class="math inline">\(\ensuremath{\mathit{FINDHARD}}\)</span></span> such that if <span><span class="math inline">\(n\)</span></span> is sufficiently large, then <span><span class="math inline">\(\ensuremath{\mathit{FINDHARD}}(1^n)\)</span></span> runs in time <span><span class="math inline">\(2^{2^{O(n)}}\)</span></span> and outputs a string <span><span class="math inline">\(f \in \{0,1\}^{2^n}\)</span></span> that is the truth table of a function that is not contained in <span><span class="math inline">\(\ensuremath{\mathit{SIZE}}(2^n/(1000n))\)</span></span>. (In other words, if <span><span class="math inline">\(f\)</span></span> is the string output by <span><span class="math inline">\(\ensuremath{\mathit{FINDHARD}}(1^n)\)</span></span> then if we let <span><span class="math inline">\(F:\{0,1\}^n \rightarrow \{0,1\}\)</span></span> be the function such that <span><span class="math inline">\(F(x)\)</span></span> outputs the <span><span class="math inline">\(x\)</span></span>-th coordinate of <span><span class="math inline">\(f\)</span></span>, then <span><span class="math inline">\(F\not\in \ensuremath{\mathit{SIZE}}(2^n/(1000n))\)</span></span>.<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup></p></li>
</ol>
</div>
<div id="scheduleprogex" class="exercise" name="Exercise 12.10">
<p>Suppose that you are in charge of scheduling courses in computer science in University X. In University X, computer science students wake up late, and have to work on their startups in the afternoon, and take long weekends with their investors. So you only have two possible slots: you can schedule a course either Monday-Wednesday 11am-1pm or Tuesday-Thursday 11am-1pm.</p>
<p>Let <span><span class="math inline">\(\ensuremath{\mathit{SCHEDULE}}:\{0,1\}^* \rightarrow \{0,1\}\)</span></span> be the function that takes as input a list of courses <span><span class="math inline">\(L\)</span></span> and a list of <em>conflicts</em> <span><span class="math inline">\(C\)</span></span> (i.e., list of pairs of courses that cannot share the same time slot) and outputs <span><span class="math inline">\(1\)</span></span> if and only if there is a “conflict free” scheduling of the courses in <span><span class="math inline">\(L\)</span></span>, where no pair in <span><span class="math inline">\(C\)</span></span> is scheduled in the same time slot.</p>
<p>More precisely, the list <span><span class="math inline">\(L\)</span></span> is a list of strings <span><span class="math inline">\((c_0,\ldots,c_{n-1})\)</span></span> and the list <span><span class="math inline">\(C\)</span></span> is a list of pairs of the form <span><span class="math inline">\((c_i,c_j)\)</span></span>. <span><span class="math inline">\(\ensuremath{\mathit{SCHEDULE}}(L,C)=1\)</span></span> if and only if there exists a partition of <span><span class="math inline">\(c_0,\ldots,c_{n-1}\)</span></span> into two parts so that there is no pair <span><span class="math inline">\((c_i,c_j) \in C\)</span></span> such that both <span><span class="math inline">\(c_i\)</span></span> and <span><span class="math inline">\(c_j\)</span></span> are in the same part.</p>
<p>Prove that <span><span class="math inline">\(\ensuremath{\mathit{SCHEDULE}} \in \mathbf{P}\)</span></span>. As usual, you do not have to provide the full code to show that this is the case, and can describe operations as a high level, as well as appeal to any data structures or other results mentioned in the book or in lecture. Note that to show that a function <span><span class="math inline">\(F\)</span></span> is in <span><span class="math inline">\(\mathbf{P}\)</span></span> you need to both <strong>(1)</strong> present an algorithm <span><span class="math inline">\(A\)</span></span> that computes <span><span class="math inline">\(F\)</span></span> in polynomial time, <strong>(2)</strong> <em>prove</em> that <span><span class="math inline">\(A\)</span></span> does indeed run in polynomial time, and does indeed compute the correct answer.</p>
<p>Try to think whether or not your algorithm extends to the case where there are <em>three</em> possible time slots.</p>
</div>
<h2 id="bibnotesrunningtime" data-number="12.8">Bibliographical notes</h2>
<p>Because we are interested in the <em>maximum</em> number of steps for inputs of a given length, running-time as we defined it is often known as <em>worst case complexity</em>. The <em>minimum</em> number of steps (or “best case” complexity) to compute a function on length <span><span class="math inline">\(n\)</span></span> inputs is typically not a meaningful quantity since essentially every natural problem will have some trivially easy instances. However, the <em>average case complexity</em> (i.e., complexity on a “typical” or “random” input) is an interesting concept which we’ll return to when we discuss <em>cryptography</em>. That said, worst-case complexity is the most standard and basic of the complexity measures, and will be our focus in most of this book.</p>
<p>Some lower bounds for single-tape Turing machines are given in  (<a href="https://scholar.google.com/scholar?hl=en&q=Maass+Combinatorial+lower+bound+arguments+for+deterministic+and+nondeterministic+Turing+machines" target="_blank">Maass, 1985</a>) .</p>
<p>For defining efficiency in the <span><span class="math inline">\(\lambda\)</span></span> calculus, one needs to be careful about the order of application of the reduction steps, which can matter for computational efficiency, see for example <a href="https://lmcs.episciences.org/1627">this paper</a>.</p>
<p>The notation <span><span class="math inline">\(\mathbf{P_{/poly}}\)</span></span> is used for historical reasons. It was introduced by Karp and Lipton, who considered this class as corresponding to functions that can be computed by polynomial-time Turing Machines that are given for any input length <span><span class="math inline">\(n\)</span></span> an <em>advice string</em> of length polynomial in <span><span class="math inline">\(n\)</span></span>.</p>
<div id="footnotediv" class="footnotes">
<ol>
<li class="footnote" id="fn:1"><p>
<div>
<p><em>Hint:</em> This is the Turing Machine analog of <a href='#obliviousnandtmthm'>Theorem 12.13</a>. We replace one step of the original TM <span><span class="math inline">\(M&#39;\)</span></span> computing <span><span class="math inline">\(F\)</span></span> with a “sweep” of the obliviouss TM <span><span class="math inline">\(M\)</span></span> in which it goes <span><span class="math inline">\(T\)</span></span> steps to the right and then <span><span class="math inline">\(T\)</span></span> steps to the left.</p>
</div>
<a href="#fnref:1" title="return to article"> ↩</a><p></li>
<li class="footnote" id="fn:2"><p>
<div>
<p><strong>Hint:</strong> Use Item 1, the existence of functions requiring exponentially hard NAND programs, and the fact that there are only finitely many functions mapping <span><span class="math inline">\(\{0,1\}^n\)</span></span> to <span><span class="math inline">\(\{0,1\}\)</span></span>.</p>
</div>
<a href="#fnref:2" title="return to article"> ↩</a><p></li>
</ol>
</div>
<!--bookdown:body:end-->


<!-- end of  actual content -->

<!-- start of comments -->


<a name="commentform"></a>
<h2 id="comments" class="nocount">Comments</h2>

<p>Comments are posted on the <a href="https://github.com/boazbk/tcs/issues">GitHub repository</a> using the <a href="https://utteranc.es">utteranc.es</a> app.
A GitHub login is required to comment.
If you don't want to authorize the app to post on your behalf, you can also comment directly on the <a href="https://github.com/boazbk/tcs/issues?q=Defining Computation+in%3Atitle">GitHub issue for this page</a>.


<p>


<script src="https://utteranc.es/client.js" 
repo="boazbk/tcs" 
issue-term="title" 
label="comments"
theme="github-light" 
crossorigin="anonymous" async>
  </script>


<!-- end of comments -->

<p>Compiled on 12/02/2019 21:39:05</p>

<p>Copyright 2019, Boaz Barak.


<a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/"><img alt="Creative Commons License"
    style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png" /></a><br />This work is
licensed under a <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/">Creative Commons
  Attribution-NonCommercial-NoDerivatives 4.0 International License</a>.

<p>Produced using <a href="https://pandoc.org/">pandoc</a> and <a href="http://scorreia.com/software/panflute/">panflute</a> with templates derived from <a href="https://www.gitbook.com/">gitbook</a> and <a href="https://bookdown.org/">bookdown</a>.</p>



</div>


            </section>

          </div>
        </div>
      </div>
<!--bookdown:link_prev-->
<!--bookdown:link_next-->



    </div>
  </div>
<!--bookdown:config-->
<script src="js/app.min.js"></script>
<script src="js/lunr.js"></script>
<script src="js/plugin-search.js"></script>
<script src="js/plugin-sharing.js"></script>
<script src="js/plugin-fontsettings.js"></script>
<script src="js/fullscreen.js"></script>
<script src="js/plugin-bookdown.js"></script>
<script src="js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"history": {
"link": null,
"text": null
},
"download": ["https://files.boazbarak.org/introtcs/lec_11_running_time.pdf"],
"toc": {
"collapse": "section"
}
});
});
</script>


</body>

</html>
