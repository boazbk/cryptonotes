<!DOCTYPE html>
<html  lang="en">

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Introduction to Theoretical Computer Science: Modeling randomized computation</title>
  <meta name="description" content="Textbook on Theoretical Computer Science by Boaz Barak">

  <meta property="og:title" content="Introduction to Theoretical Computer Science: Modeling randomized computation" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://introtcs.org/" />
  <meta property="og:image" content="icons/cover.png" />
  <meta property="og:description" content="Textbook on Theoretical Computer Science by Boaz Barak" />
  <meta name="github-repo" content="boazbk/tcs" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Introduction to Theoretical Computer Science" />
  <meta name="twitter:description" content="Textbook on Theoretical Computer Science by Boaz Barak" />
  <meta name="twitter:image" content="https://introtcs.org/icons/cover.png" />

<meta name="author" content="Boaz Barak">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <link rel="apple-touch-icon-precomposed" sizes="120x120" href="icons/apple-touch-icon-120x120.png">
  <link rel="shortcut icon" href="icons/favicon.ico" type="image/x-icon">

<!-- Boaz: resources -->

<!-- <script src="https://kit.fontawesome.com/ab08ce82a8.js"></script> -->

<link rel="stylesheet" src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.9.0/css/all.min.css">


<!-- KaTeX -->


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css"
  integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">

<!-- The loading of KaTeX is deferred to speed up page rendering -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js"
  integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>

<!-- To automatically render math in text elements, include the auto-render extension: -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js"
  integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
  onload='renderMathInElement(document.body, {  throwOnError: false, macros: { "\\N": "\\mathbb{N}", "\\R": "\\mathbb{R}", "\\Z": "\\mathbb{Z}","\\E": "\\mathbb{E}","\\val": "\\mathrm{val}", "\\label": "\\;\\;\\;\\;\\;\\;\\;\\;","\\floor": "\\lfloor #1 \\rfloor","\\ceil": "\\lceil #1 \\rceil", "\\ensuremath": "#1"}});'>
</script>




<!-- KaTeX -->
<!-- pseudocode -->
<link rel="stylesheet" href="css/pseudocode.css">
<!-- <script src="js/pseudocode.min.js"></script> -->


<!-- Gitbook resources -->

  <script src="js/jquery.min.js"></script>
  <link href="css/style.css" rel="stylesheet" />
  
  <link href="css/plugin-table.css" rel="stylesheet" />
  <link href="css/plugin-bookdown.css" rel="stylesheet" />
  <link href="css/plugin-highlight.css" rel="stylesheet" />
  <link href="css/plugin-search.css" rel="stylesheet" />
  <link href="css/plugin-fontsettings.css" rel="stylesheet" />
  <link href="css/moregitbook.css" rel="stylesheet" />

  <link href="css/resmisc.css" rel="stylesheet" />





<!-- Boaz: end resources -->



<!--bookdown:link_prev-->
<!--bookdown:link_next-->



<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<!-- bigfoot-->

<link href="css/bigfoot-default.css" rel="stylesheet" />
<script type="text/javascript" src="js/bigfoot.js"></script>

<script type="text/javascript">
    var bigfoot = jQuery.bigfoot(
        {
            deleteOnUnhover: false,
            preventPageScroll: false,
            hoverDelay: 250
        }
    );
</script>

<!-- end bigfoot -->


</head>

<body>



<!--bookdown:title:start-->
<!--bookdown:title:end-->


<!--bookdown:toc:start-->
  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">
<!--bookdown:toc2:start-->
<ul class="summary">
<li><a href="./">Introduction to Theoretical Computer Science</a></li>
<li class="divider"></li><li class="chapter" data-level="p" data-path="lec_00_0_preface.html"><a href="lec_00_0_preface.html"><i class="fa fa-check"></i><b>p</b> Preface</a><ul><li class="chapter" data-level="p.1" data-path="lec_00_0_preface.html"><a href="lec_00_0_preface.html#to-the-student"><i class="fa fa-check"></i><b>p.1</b> To the student</a><ul><li class="chapter" data-level="p.1.1" data-path="lec_00_0_preface.html"><a href="lec_00_0_preface.html#is-the-effort-worth-it"><i class="fa fa-check"></i><b>p.1.1</b> Is the effort worth it?</a></li></ul></li><li class="chapter" data-level="p.2" data-path="lec_00_0_preface.html"><a href="lec_00_0_preface.html#to-potential-instructors"><i class="fa fa-check"></i><b>p.2</b> To potential instructors</a></li><li class="chapter" data-level="p.3" data-path="lec_00_0_preface.html"><a href="lec_00_0_preface.html#acknowledgements"><i class="fa fa-check"></i><b>p.3</b> Acknowledgements</a></li></ul></li><li class="chapter" data-level="0" data-path="lec_01_introduction.html"><a href="lec_01_introduction.html"><i class="fa fa-check"></i><b>0</b> Introduction</a><ul><li class="chapter" data-level="0.1" data-path="lec_01_introduction.html"><a href="lec_01_introduction.html#integer-multiplication-an-example-of-an-algorithm"><i class="fa fa-check"></i><b>0.1</b> Integer multiplication: an example of an algorithm</a></li><li class="chapter" data-level="0.2" data-path="lec_01_introduction.html"><a href="lec_01_introduction.html#karatsubasec"><i class="fa fa-check"></i><b>0.2</b> Extended Example: A faster way to multiply (optional)</a></li><li class="chapter" data-level="0.3" data-path="lec_01_introduction.html"><a href="lec_01_introduction.html#algsbeyondarithmetic"><i class="fa fa-check"></i><b>0.3</b> Algorithms beyond arithmetic</a></li><li class="chapter" data-level="0.4" data-path="lec_01_introduction.html"><a href="lec_01_introduction.html#on-the-importance-of-negative-results."><i class="fa fa-check"></i><b>0.4</b> On the importance of negative results.</a></li><li class="chapter" data-level="0.5" data-path="lec_01_introduction.html"><a href="lec_01_introduction.html#roadmapsec"><i class="fa fa-check"></i><b>0.5</b> Roadmap to the rest of this book</a><ul><li class="chapter" data-level="0.5.1" data-path="lec_01_introduction.html"><a href="lec_01_introduction.html#dependencies-between-chapters"><i class="fa fa-check"></i><b>0.5.1</b> Dependencies between chapters</a></li></ul></li><li class="chapter" data-level="0.6" data-path="lec_01_introduction.html"><a href="lec_01_introduction.html#exercises"><i class="fa fa-check"></i><b>0.6</b> Exercises</a></li><li class="chapter" data-level="0.7" data-path="lec_01_introduction.html"><a href="lec_01_introduction.html#bnotesintrosec"><i class="fa fa-check"></i><b>0.7</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="1" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html"><i class="fa fa-check"></i><b>1</b> Mathematical Background</a><ul><li class="chapter" data-level="1.1" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#manualbackground"><i class="fa fa-check"></i><b>1.1</b> This chapter: a reader’s manual</a></li><li class="chapter" data-level="1.2" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#secmathoverview"><i class="fa fa-check"></i><b>1.2</b> A quick overview of mathematical prerequisites</a></li><li class="chapter" data-level="1.3" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#reading-mathematical-texts"><i class="fa fa-check"></i><b>1.3</b> Reading mathematical texts</a><ul><li class="chapter" data-level="1.3.1" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#definitions"><i class="fa fa-check"></i><b>1.3.1</b> Definitions</a></li><li class="chapter" data-level="1.3.2" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#assertions-theorems-lemmas-claims"><i class="fa fa-check"></i><b>1.3.2</b> Assertions: Theorems, lemmas, claims</a></li><li class="chapter" data-level="1.3.3" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#proofs"><i class="fa fa-check"></i><b>1.3.3</b> Proofs</a></li></ul></li><li class="chapter" data-level="1.4" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#basic-discrete-math-objects"><i class="fa fa-check"></i><b>1.4</b> Basic discrete math objects</a><ul><li class="chapter" data-level="1.4.1" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#sets"><i class="fa fa-check"></i><b>1.4.1</b> Sets</a></li><li class="chapter" data-level="1.4.2" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#specialsets"><i class="fa fa-check"></i><b>1.4.2</b> Special sets</a></li><li class="chapter" data-level="1.4.3" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#functionsec"><i class="fa fa-check"></i><b>1.4.3</b> Functions</a></li><li class="chapter" data-level="1.4.4" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#graphsec"><i class="fa fa-check"></i><b>1.4.4</b> Graphs</a></li><li class="chapter" data-level="1.4.5" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#secquantifiers"><i class="fa fa-check"></i><b>1.4.5</b> Logic operators and quantifiers</a></li><li class="chapter" data-level="1.4.6" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#secquantifierssums"><i class="fa fa-check"></i><b>1.4.6</b> Quantifiers for summations and products</a></li><li class="chapter" data-level="1.4.7" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#boundvarsec"><i class="fa fa-check"></i><b>1.4.7</b> Parsing formulas: bound and free variables</a></li><li class="chapter" data-level="1.4.8" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#secbigohnotation"><i class="fa fa-check"></i><b>1.4.8</b> Asymptotics and Big-O notation</a></li><li class="chapter" data-level="1.4.9" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#some-rules-of-thumb-for-big-o-notation"><i class="fa fa-check"></i><b>1.4.9</b> Some rules of thumb for Big-O notation</a></li></ul></li><li class="chapter" data-level="1.5" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#proofsbackgroundsec"><i class="fa fa-check"></i><b>1.5</b> Proofs</a><ul><li class="chapter" data-level="1.5.1" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#proofs-and-programs"><i class="fa fa-check"></i><b>1.5.1</b> Proofs and programs</a></li><li class="chapter" data-level="1.5.2" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#proof-writing-style"><i class="fa fa-check"></i><b>1.5.2</b> Proof writing style</a></li><li class="chapter" data-level="1.5.3" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#patterns-in-proofs"><i class="fa fa-check"></i><b>1.5.3</b> Patterns in proofs</a></li></ul></li><li class="chapter" data-level="1.6" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#topsortsec"><i class="fa fa-check"></i><b>1.6</b> Extended example: Topological Sorting</a><ul><li class="chapter" data-level="1.6.1" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#inductionsec"><i class="fa fa-check"></i><b>1.6.1</b> Mathematical induction</a></li><li class="chapter" data-level="1.6.2" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#proving-the-result-by-induction"><i class="fa fa-check"></i><b>1.6.2</b> Proving the result by induction</a></li><li class="chapter" data-level="1.6.3" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#minimality-and-uniqueness"><i class="fa fa-check"></i><b>1.6.3</b> Minimality and uniqueness</a></li></ul></li><li class="chapter" data-level="1.7" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#notationsec"><i class="fa fa-check"></i><b>1.7</b> This book: notation and conventions</a><ul><li class="chapter" data-level="1.7.1" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#conventionsec"><i class="fa fa-check"></i><b>1.7.1</b> Variable name conventions</a></li><li class="chapter" data-level="1.7.2" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#some-idioms"><i class="fa fa-check"></i><b>1.7.2</b> Some idioms</a></li></ul></li><li class="chapter" data-level="1.8" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#exercises"><i class="fa fa-check"></i><b>1.8</b> Exercises</a></li><li class="chapter" data-level="1.9" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#notesmathchap"><i class="fa fa-check"></i><b>1.9</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="2" data-path="lec_02_representation.html"><a href="lec_02_representation.html"><i class="fa fa-check"></i><b>2</b> Computation and Representation</a><ul><li class="chapter" data-level="2.1" data-path="lec_02_representation.html"><a href="lec_02_representation.html#defining-representations"><i class="fa fa-check"></i><b>2.1</b> Defining representations</a><ul><li class="chapter" data-level="2.1.1" data-path="lec_02_representation.html"><a href="lec_02_representation.html#representing-natural-numbers"><i class="fa fa-check"></i><b>2.1.1</b> Representing natural numbers</a></li><li class="chapter" data-level="2.1.2" data-path="lec_02_representation.html"><a href="lec_02_representation.html#meaning-of-representations-discussion"><i class="fa fa-check"></i><b>2.1.2</b> Meaning of representations (discussion)</a></li></ul></li><li class="chapter" data-level="2.2" data-path="lec_02_representation.html"><a href="lec_02_representation.html#representations-beyond-natural-numbers"><i class="fa fa-check"></i><b>2.2</b> Representations beyond natural numbers</a><ul><li class="chapter" data-level="2.2.1" data-path="lec_02_representation.html"><a href="lec_02_representation.html#repnegativeintegerssec"><i class="fa fa-check"></i><b>2.2.1</b> Representing (potentially negative) integers</a></li><li class="chapter" data-level="2.2.2" data-path="lec_02_representation.html"><a href="lec_02_representation.html#twoscomplement"><i class="fa fa-check"></i><b>2.2.2</b> Two’s complement representation (optional)</a></li><li class="chapter" data-level="2.2.3" data-path="lec_02_representation.html"><a href="lec_02_representation.html#rational-numbers-and-representing-pairs-of-strings"><i class="fa fa-check"></i><b>2.2.3</b> Rational numbers, and representing pairs of strings</a></li></ul></li><li class="chapter" data-level="2.3" data-path="lec_02_representation.html"><a href="lec_02_representation.html#representing-real-numbers"><i class="fa fa-check"></i><b>2.3</b> Representing real numbers</a><ul><li class="chapter" data-level="2.3.1" data-path="lec_02_representation.html"><a href="lec_02_representation.html#cantorsec"><i class="fa fa-check"></i><b>2.3.1</b> Can we represent reals exactly?</a></li></ul></li><li class="chapter" data-level="2.4" data-path="lec_02_representation.html"><a href="lec_02_representation.html#representing-objects-beyond-numbers"><i class="fa fa-check"></i><b>2.4</b> Representing objects beyond numbers</a><ul><li class="chapter" data-level="2.4.1" data-path="lec_02_representation.html"><a href="lec_02_representation.html#finite-representations"><i class="fa fa-check"></i><b>2.4.1</b> Finite representations</a></li><li class="chapter" data-level="2.4.2" data-path="lec_02_representation.html"><a href="lec_02_representation.html#prefixfreesec"><i class="fa fa-check"></i><b>2.4.2</b> Prefix-free encoding</a></li><li class="chapter" data-level="2.4.3" data-path="lec_02_representation.html"><a href="lec_02_representation.html#making-representations-prefix-free"><i class="fa fa-check"></i><b>2.4.3</b> Making representations prefix-free</a></li><li class="chapter" data-level="2.4.4" data-path="lec_02_representation.html"><a href="lec_02_representation.html#proof-by-python-optional"><i class="fa fa-check"></i><b>2.4.4</b> Proof by Python (optional)</a></li><li class="chapter" data-level="2.4.5" data-path="lec_02_representation.html"><a href="lec_02_representation.html#representing-letters-and-text"><i class="fa fa-check"></i><b>2.4.5</b> Representing letters and text</a></li><li class="chapter" data-level="2.4.6" data-path="lec_02_representation.html"><a href="lec_02_representation.html#representing-vectors-matrices-images"><i class="fa fa-check"></i><b>2.4.6</b> Representing vectors, matrices, images</a></li><li class="chapter" data-level="2.4.7" data-path="lec_02_representation.html"><a href="lec_02_representation.html#representing-graphs"><i class="fa fa-check"></i><b>2.4.7</b> Representing graphs</a></li><li class="chapter" data-level="2.4.8" data-path="lec_02_representation.html"><a href="lec_02_representation.html#representing-lists-and-nested-lists"><i class="fa fa-check"></i><b>2.4.8</b> Representing lists and nested lists</a></li><li class="chapter" data-level="2.4.9" data-path="lec_02_representation.html"><a href="lec_02_representation.html#notation"><i class="fa fa-check"></i><b>2.4.9</b> Notation</a></li></ul></li><li class="chapter" data-level="2.5" data-path="lec_02_representation.html"><a href="lec_02_representation.html#defining-computational-tasks-as-mathematical-functions"><i class="fa fa-check"></i><b>2.5</b> Defining computational tasks as mathematical functions</a><ul><li class="chapter" data-level="2.5.1" data-path="lec_02_representation.html"><a href="lec_02_representation.html#secimplvsspec"><i class="fa fa-check"></i><b>2.5.1</b> Distinguish functions from programs!</a></li></ul></li><li class="chapter" data-level="2.6" data-path="lec_02_representation.html"><a href="lec_02_representation.html#exercises"><i class="fa fa-check"></i><b>2.6</b> Exercises</a></li><li class="chapter" data-level="2.7" data-path="lec_02_representation.html"><a href="lec_02_representation.html#bibnotesrepres"><i class="fa fa-check"></i><b>2.7</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="3" data-path="lec_03_computation.html"><a href="lec_03_computation.html"><i class="fa fa-check"></i><b>3</b> Defining computation</a><ul><li class="chapter" data-level="3.1" data-path="lec_03_computation.html"><a href="lec_03_computation.html#defining-computation"><i class="fa fa-check"></i><b>3.1</b> Defining computation</a></li><li class="chapter" data-level="3.2" data-path="lec_03_computation.html"><a href="lec_03_computation.html#computing-using-and-or-and-not."><i class="fa fa-check"></i><b>3.2</b> Computing using AND, OR, and NOT.</a><ul><li class="chapter" data-level="3.2.1" data-path="lec_03_computation.html"><a href="lec_03_computation.html#some-properties-of-and-and-or"><i class="fa fa-check"></i><b>3.2.1</b> Some properties of AND and OR</a></li><li class="chapter" data-level="3.2.2" data-path="lec_03_computation.html"><a href="lec_03_computation.html#xoraonexample"><i class="fa fa-check"></i><b>3.2.2</b> Extended example: Computing \ensuremath{\mathit{XOR}} from \ensuremath{\mathit{AND}}, \ensuremath{\mathit{OR}}, and \ensuremath{\mathit{NOT}}</a></li><li class="chapter" data-level="3.2.3" data-path="lec_03_computation.html"><a href="lec_03_computation.html#informally-defining-basic-operations-and-algorithms"><i class="fa fa-check"></i><b>3.2.3</b> Informally defining basic operations and algorithms</a></li></ul></li><li class="chapter" data-level="3.3" data-path="lec_03_computation.html"><a href="lec_03_computation.html#booleancircuitfig"><i class="fa fa-check"></i><b>3.3</b> Boolean Circuits</a><ul><li class="chapter" data-level="3.3.1" data-path="lec_03_computation.html"><a href="lec_03_computation.html#boolean-circuits-a-formal-definition"><i class="fa fa-check"></i><b>3.3.1</b> Boolean circuits: a formal definition</a></li><li class="chapter" data-level="3.3.2" data-path="lec_03_computation.html"><a href="lec_03_computation.html#equivalence-of-circuits-and-straight-line-programs"><i class="fa fa-check"></i><b>3.3.2</b> Equivalence of circuits and straight-line programs</a></li></ul></li><li class="chapter" data-level="3.4" data-path="lec_03_computation.html"><a href="lec_03_computation.html#physicalimplementationsec"><i class="fa fa-check"></i><b>3.4</b> Physical implementations of computing devices (digression)</a><ul><li class="chapter" data-level="3.4.1" data-path="lec_03_computation.html"><a href="lec_03_computation.html#transistors"><i class="fa fa-check"></i><b>3.4.1</b> Transistors</a></li><li class="chapter" data-level="3.4.2" data-path="lec_03_computation.html"><a href="lec_03_computation.html#logical-gates-from-transistors"><i class="fa fa-check"></i><b>3.4.2</b> Logical gates from transistors</a></li><li class="chapter" data-level="3.4.3" data-path="lec_03_computation.html"><a href="lec_03_computation.html#biological-computing"><i class="fa fa-check"></i><b>3.4.3</b> Biological computing</a></li><li class="chapter" data-level="3.4.4" data-path="lec_03_computation.html"><a href="lec_03_computation.html#cellular-automata-and-the-game-of-life"><i class="fa fa-check"></i><b>3.4.4</b> Cellular automata and the game of life</a></li><li class="chapter" data-level="3.4.5" data-path="lec_03_computation.html"><a href="lec_03_computation.html#neural-networks"><i class="fa fa-check"></i><b>3.4.5</b> Neural networks</a></li><li class="chapter" data-level="3.4.6" data-path="lec_03_computation.html"><a href="lec_03_computation.html#a-computer-made-from-marbles-and-pipes"><i class="fa fa-check"></i><b>3.4.6</b> A computer made from marbles and pipes</a></li></ul></li><li class="chapter" data-level="3.5" data-path="lec_03_computation.html"><a href="lec_03_computation.html#nandsec"><i class="fa fa-check"></i><b>3.5</b> The NAND function</a><ul><li class="chapter" data-level="3.5.1" data-path="lec_03_computation.html"><a href="lec_03_computation.html#nand-circuits"><i class="fa fa-check"></i><b>3.5.1</b> NAND Circuits</a></li><li class="chapter" data-level="3.5.2" data-path="lec_03_computation.html"><a href="lec_03_computation.html#more-examples-of-nand-circuits-optional"><i class="fa fa-check"></i><b>3.5.2</b> More examples of NAND circuits (optional)</a></li><li class="chapter" data-level="3.5.3" data-path="lec_03_computation.html"><a href="lec_03_computation.html#nandcircsec"><i class="fa fa-check"></i><b>3.5.3</b> The NAND-CIRC Programming language</a></li></ul></li><li class="chapter" data-level="3.6" data-path="lec_03_computation.html"><a href="lec_03_computation.html#equivalence-of-all-these-models"><i class="fa fa-check"></i><b>3.6</b> Equivalence of all these models</a><ul><li class="chapter" data-level="3.6.1" data-path="lec_03_computation.html"><a href="lec_03_computation.html#othergatessec"><i class="fa fa-check"></i><b>3.6.1</b> Circuits with other gate sets</a></li><li class="chapter" data-level="3.6.2" data-path="lec_03_computation.html"><a href="lec_03_computation.html#specvsimplrem"><i class="fa fa-check"></i><b>3.6.2</b> Specification vs. implementation (again)</a></li></ul></li><li class="chapter" data-level="3.7" data-path="lec_03_computation.html"><a href="lec_03_computation.html#exercises"><i class="fa fa-check"></i><b>3.7</b> Exercises</a></li><li class="chapter" data-level="3.8" data-path="lec_03_computation.html"><a href="lec_03_computation.html#biographical-notes"><i class="fa fa-check"></i><b>3.8</b> Biographical notes</a></li></ul></li><li class="chapter" data-level="4" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html"><i class="fa fa-check"></i><b>4</b> Syntactic sugar, and computing every function</a><ul><li class="chapter" data-level="4.1" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#secsyntacticsugar"><i class="fa fa-check"></i><b>4.1</b> Some examples of syntactic sugar</a><ul><li class="chapter" data-level="4.1.1" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#user-defined-procedures"><i class="fa fa-check"></i><b>4.1.1</b> User-defined procedures</a></li><li class="chapter" data-level="4.1.2" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#functionsynsugarthmpython"><i class="fa fa-check"></i><b>4.1.2</b> Proof by Python (optional)</a></li><li class="chapter" data-level="4.1.3" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#ifstatementsec"><i class="fa fa-check"></i><b>4.1.3</b> Conditional statements</a></li></ul></li><li class="chapter" data-level="4.2" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#addexample"><i class="fa fa-check"></i><b>4.2</b> Extended example: Addition and Multiplication (optional)</a></li><li class="chapter" data-level="4.3" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#seclookupfunc"><i class="fa fa-check"></i><b>4.3</b> The LOOKUP function</a><ul><li class="chapter" data-level="4.3.1" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#constructing-a-nand-circ-program-for-lookup"><i class="fa fa-check"></i><b>4.3.1</b> Constructing a NAND-CIRC program for \ensuremath{\mathit{LOOKUP}}</a></li></ul></li><li class="chapter" data-level="4.4" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#seccomputeallfunctions"><i class="fa fa-check"></i><b>4.4</b> Computing every function</a><ul><li class="chapter" data-level="4.4.1" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#proof-of-nands-universality"><i class="fa fa-check"></i><b>4.4.1</b> Proof of NAND’s Universality</a></li><li class="chapter" data-level="4.4.2" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#tight-upper-bound"><i class="fa fa-check"></i><b>4.4.2</b> Improving by a factor of n (optional)</a></li></ul></li><li class="chapter" data-level="4.5" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#seccomputalternative"><i class="fa fa-check"></i><b>4.5</b> Computing every function: An alternative proof</a></li><li class="chapter" data-level="4.6" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#secdefinesizeclasses"><i class="fa fa-check"></i><b>4.6</b> The class \ensuremath{\mathit{SIZE}}(T)</a></li><li class="chapter" data-level="4.7" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#exercises"><i class="fa fa-check"></i><b>4.7</b> Exercises</a></li><li class="chapter" data-level="4.8" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#computeeveryfunctionbibnotes"><i class="fa fa-check"></i><b>4.8</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="5" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html"><i class="fa fa-check"></i><b>5</b> Code as data, data as code</a><ul><li class="chapter" data-level="5.1" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#representprogramsec"><i class="fa fa-check"></i><b>5.1</b> Representing programs as strings</a></li><li class="chapter" data-level="5.2" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#countingcircuitsec"><i class="fa fa-check"></i><b>5.2</b> Counting programs, and lower bounds on the size of NAND-CIRC programs</a><ul><li class="chapter" data-level="5.2.1" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#size-hierarchy-theorem-optional"><i class="fa fa-check"></i><b>5.2.1</b> Size hierarchy theorem (optional)</a></li></ul></li><li class="chapter" data-level="5.3" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#listoftuplesrepsec"><i class="fa fa-check"></i><b>5.3</b> The tuples representation</a><ul><li class="chapter" data-level="5.3.1" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#stringrepresentationrpgoramsec"><i class="fa fa-check"></i><b>5.3.1</b> From tuples to strings</a></li></ul></li><li class="chapter" data-level="5.4" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#a-nand-circ-interpreter-in-nand-circ"><i class="fa fa-check"></i><b>5.4</b> A NAND-CIRC interpreter in NAND-CIRC</a><ul><li class="chapter" data-level="5.4.1" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#efficient-universal-programs"><i class="fa fa-check"></i><b>5.4.1</b> Efficient universal programs</a></li><li class="chapter" data-level="5.4.2" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#a-nand-circ-interpeter-in-pseudocode"><i class="fa fa-check"></i><b>5.4.2</b> A NAND-CIRC interpeter in pseudocode</a></li><li class="chapter" data-level="5.4.3" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#nandevalpythonsec"><i class="fa fa-check"></i><b>5.4.3</b> A NAND interpreter in Python</a></li><li class="chapter" data-level="5.4.4" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#constructing-the-nand-circ-interpreter-in-nand-circ"><i class="fa fa-check"></i><b>5.4.4</b> Constructing the NAND-CIRC interpreter in NAND-CIRC</a></li></ul></li><li class="chapter" data-level="5.5" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#a-python-interpreter-in-nand-circ-discussion"><i class="fa fa-check"></i><b>5.5</b> A Python interpreter in NAND-CIRC (discussion)</a></li><li class="chapter" data-level="5.6" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#PECTTsec"><i class="fa fa-check"></i><b>5.6</b> The physical extended Church-Turing thesis (discussion)</a><ul><li class="chapter" data-level="5.6.1" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#attempts-at-refuting-the-pectt"><i class="fa fa-check"></i><b>5.6.1</b> Attempts at refuting the PECTT</a></li></ul></li><li class="chapter" data-level="5.7" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#recap-of-part-i-finite-computation"><i class="fa fa-check"></i><b>5.7</b> Recap of Part I: Finite Computation</a></li><li class="chapter" data-level="5.8" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#exercises"><i class="fa fa-check"></i><b>5.8</b> Exercises</a></li><li class="chapter" data-level="5.9" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#bibnotescodeasdata"><i class="fa fa-check"></i><b>5.9</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="6" data-path="lec_06_loops.html"><a href="lec_06_loops.html"><i class="fa fa-check"></i><b>6</b> Loops and infinity</a><ul><li class="chapter" data-level="6.1" data-path="lec_06_loops.html"><a href="lec_06_loops.html#turing-machines"><i class="fa fa-check"></i><b>6.1</b> Turing Machines</a><ul><li class="chapter" data-level="6.1.1" data-path="lec_06_loops.html"><a href="lec_06_loops.html#turingmachinepalindrome"><i class="fa fa-check"></i><b>6.1.1</b> Extended example: A Turing machine for palindromes</a></li><li class="chapter" data-level="6.1.2" data-path="lec_06_loops.html"><a href="lec_06_loops.html#turing-machines-a-formal-definition"><i class="fa fa-check"></i><b>6.1.2</b> Turing machines: a formal definition</a></li><li class="chapter" data-level="6.1.3" data-path="lec_06_loops.html"><a href="lec_06_loops.html#computable-functions"><i class="fa fa-check"></i><b>6.1.3</b> Computable functions</a></li><li class="chapter" data-level="6.1.4" data-path="lec_06_loops.html"><a href="lec_06_loops.html#infinite-loops-and-partial-functions"><i class="fa fa-check"></i><b>6.1.4</b> Infinite loops and partial functions</a></li></ul></li><li class="chapter" data-level="6.2" data-path="lec_06_loops.html"><a href="lec_06_loops.html#turing-machines-as-programming-languages"><i class="fa fa-check"></i><b>6.2</b> Turing machines as programming languages</a><ul><li class="chapter" data-level="6.2.1" data-path="lec_06_loops.html"><a href="lec_06_loops.html#the-nand-tm-programming-language"><i class="fa fa-check"></i><b>6.2.1</b> The NAND-TM Programming language</a></li><li class="chapter" data-level="6.2.2" data-path="lec_06_loops.html"><a href="lec_06_loops.html#sneak-peak-nand-tm-vs-turing-machines"><i class="fa fa-check"></i><b>6.2.2</b> Sneak peak: NAND-TM vs Turing machines</a></li><li class="chapter" data-level="6.2.3" data-path="lec_06_loops.html"><a href="lec_06_loops.html#examples"><i class="fa fa-check"></i><b>6.2.3</b> Examples</a></li></ul></li><li class="chapter" data-level="6.3" data-path="lec_06_loops.html"><a href="lec_06_loops.html#equivalence-of-turing-machines-and-nand-tm-programs"><i class="fa fa-check"></i><b>6.3</b> Equivalence of Turing machines and NAND-TM programs</a><ul><li class="chapter" data-level="6.3.1" data-path="lec_06_loops.html"><a href="lec_06_loops.html#specification-vs-implementation-again"><i class="fa fa-check"></i><b>6.3.1</b> Specification vs implementation (again)</a></li></ul></li><li class="chapter" data-level="6.4" data-path="lec_06_loops.html"><a href="lec_06_loops.html#nand-tm-syntactic-sugar"><i class="fa fa-check"></i><b>6.4</b> NAND-TM syntactic sugar</a><ul><li class="chapter" data-level="6.4.1" data-path="lec_06_loops.html"><a href="lec_06_loops.html#nandtminnerloopssec"><i class="fa fa-check"></i><b>6.4.1</b> GOTO and inner loops</a></li></ul></li><li class="chapter" data-level="6.5" data-path="lec_06_loops.html"><a href="lec_06_loops.html#uniformity-and-nand-vs-nand-tm-discussion"><i class="fa fa-check"></i><b>6.5</b> Uniformity, and NAND vs NAND-TM (discussion)</a></li><li class="chapter" data-level="6.6" data-path="lec_06_loops.html"><a href="lec_06_loops.html#exercises"><i class="fa fa-check"></i><b>6.6</b> Exercises</a></li><li class="chapter" data-level="6.7" data-path="lec_06_loops.html"><a href="lec_06_loops.html#chaploopnotes"><i class="fa fa-check"></i><b>6.7</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="7" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html"><i class="fa fa-check"></i><b>7</b> Equivalent models of computation</a><ul><li class="chapter" data-level="7.1" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#ram-machines-and-nand-ram"><i class="fa fa-check"></i><b>7.1</b> RAM machines and NAND-RAM</a></li><li class="chapter" data-level="7.2" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#nandtmgorydetailssec"><i class="fa fa-check"></i><b>7.2</b> The gory details (optional)</a><ul><li class="chapter" data-level="7.2.1" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#indexed-access-in-nand-tm"><i class="fa fa-check"></i><b>7.2.1</b> Indexed access in NAND-TM</a></li><li class="chapter" data-level="7.2.2" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#two-dimensional-arrays-in-nand-tm"><i class="fa fa-check"></i><b>7.2.2</b> Two dimensional arrays in NAND-TM</a></li><li class="chapter" data-level="7.2.3" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#all-the-rest"><i class="fa fa-check"></i><b>7.2.3</b> All the rest</a></li></ul></li><li class="chapter" data-level="7.3" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#turing-equivalence-discussion"><i class="fa fa-check"></i><b>7.3</b> Turing equivalence (discussion)</a><ul><li class="chapter" data-level="7.3.1" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#the-best-of-both-worlds-paradigm"><i class="fa fa-check"></i><b>7.3.1</b> The Best of both worlds paradigm</a></li><li class="chapter" data-level="7.3.2" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#lets-talk-about-abstractions."><i class="fa fa-check"></i><b>7.3.2</b> Let’s talk about abstractions.</a></li><li class="chapter" data-level="7.3.3" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#turingcompletesec"><i class="fa fa-check"></i><b>7.3.3</b> Turing completeness and equivalence, a formal definition (optional)</a></li></ul></li><li class="chapter" data-level="7.4" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#cellularautomatasec"><i class="fa fa-check"></i><b>7.4</b> Cellular automata</a><ul><li class="chapter" data-level="7.4.1" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#one-dimensional-cellular-automata-are-turing-complete"><i class="fa fa-check"></i><b>7.4.1</b> One dimensional cellular automata are Turing complete</a></li><li class="chapter" data-level="7.4.2" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#turingmachinesconfigsec"><i class="fa fa-check"></i><b>7.4.2</b> Configurations of Turing machines and the next-step function</a></li></ul></li><li class="chapter" data-level="7.5" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#lambdacalculussec"><i class="fa fa-check"></i><b>7.5</b> Lambda calculus and functional programming languages</a><ul><li class="chapter" data-level="7.5.1" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#applying-functions-to-functions"><i class="fa fa-check"></i><b>7.5.1</b> Applying functions to functions</a></li><li class="chapter" data-level="7.5.2" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#curryingsec"><i class="fa fa-check"></i><b>7.5.2</b> Obtaining multi-argument functions via Currying</a></li><li class="chapter" data-level="7.5.3" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#formal-description-of-the-λ-calculus."><i class="fa fa-check"></i><b>7.5.3</b> Formal description of the λ calculus.</a></li><li class="chapter" data-level="7.5.4" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#infiniteloopslambda"><i class="fa fa-check"></i><b>7.5.4</b> Infinite loops in the λ calculus</a></li></ul></li><li class="chapter" data-level="7.6" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#the-enhanced-λ-calculus"><i class="fa fa-check"></i><b>7.6</b> The Enhanced λ calculus</a><ul><li class="chapter" data-level="7.6.1" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#computing-a-function-in-the-enhanced-λ-calculus"><i class="fa fa-check"></i><b>7.6.1</b> Computing a function in the enhanced λ calculus</a></li><li class="chapter" data-level="7.6.2" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#enhanced-λ-calculus-is-turing-complete"><i class="fa fa-check"></i><b>7.6.2</b> Enhanced λ calculus is Turing-complete</a></li></ul></li><li class="chapter" data-level="7.7" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#lambdacacluluspuresec"><i class="fa fa-check"></i><b>7.7</b> From enhanced to pure λ calculus</a><ul><li class="chapter" data-level="7.7.1" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#list-processing"><i class="fa fa-check"></i><b>7.7.1</b> List processing</a></li><li class="chapter" data-level="7.7.2" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#ycombinatorsec"><i class="fa fa-check"></i><b>7.7.2</b> The Y combinator, or recursion without recursion</a></li></ul></li><li class="chapter" data-level="7.8" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#churchturingdiscussionsec"><i class="fa fa-check"></i><b>7.8</b> The Church-Turing Thesis (discussion)</a><ul><li class="chapter" data-level="7.8.1" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#different-models-of-computation"><i class="fa fa-check"></i><b>7.8.1</b> Different models of computation</a></li></ul></li><li class="chapter" data-level="7.9" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#exercises"><i class="fa fa-check"></i><b>7.9</b> Exercises</a></li><li class="chapter" data-level="7.10" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#othermodelsbibnotes"><i class="fa fa-check"></i><b>7.10</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="8" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html"><i class="fa fa-check"></i><b>8</b> Universality and uncomputability</a><ul><li class="chapter" data-level="8.1" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#universality-or-a-meta-circular-evaluator"><i class="fa fa-check"></i><b>8.1</b> Universality or a meta-circular evaluator</a><ul><li class="chapter" data-level="8.1.1" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#representtmsec"><i class="fa fa-check"></i><b>8.1.1</b> Proving the existence of a universal Turing Machine</a></li><li class="chapter" data-level="8.1.2" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#implications-of-universality-discussion"><i class="fa fa-check"></i><b>8.1.2</b> Implications of universality (discussion)</a></li></ul></li><li class="chapter" data-level="8.2" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#is-every-function-computable"><i class="fa fa-check"></i><b>8.2</b> Is every function computable?</a></li><li class="chapter" data-level="8.3" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#haltingsec"><i class="fa fa-check"></i><b>8.3</b> The Halting problem</a><ul><li class="chapter" data-level="8.3.1" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#is-the-halting-problem-really-hard-discussion"><i class="fa fa-check"></i><b>8.3.1</b> Is the Halting problem really hard? (discussion)</a></li><li class="chapter" data-level="8.3.2" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#haltalternativesec"><i class="fa fa-check"></i><b>8.3.2</b> A direct proof of the uncomputability of \ensuremath{\mathit{HALT}} (optional)</a></li></ul></li><li class="chapter" data-level="8.4" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#reductionsuncompsec"><i class="fa fa-check"></i><b>8.4</b> Reductions</a><ul><li class="chapter" data-level="8.4.1" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#example-halting-on-the-zero-problem"><i class="fa fa-check"></i><b>8.4.1</b> Example: Halting on the zero problem</a></li></ul></li><li class="chapter" data-level="8.5" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#rices-theorem-and-the-impossibility-of-general-software-verification"><i class="fa fa-check"></i><b>8.5</b> Rice’s Theorem and the impossibility of general software verification</a><ul><li class="chapter" data-level="8.5.1" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#ricethmsec"><i class="fa fa-check"></i><b>8.5.1</b> Rice’s Theorem</a></li><li class="chapter" data-level="8.5.2" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#halting-and-rices-theorem-for-other-turing-complete-models"><i class="fa fa-check"></i><b>8.5.2</b> Halting and Rice’s Theorem for other Turing-complete models</a></li><li class="chapter" data-level="8.5.3" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#is-software-verification-doomed-discussion"><i class="fa fa-check"></i><b>8.5.3</b> Is software verification doomed? (discussion)</a></li></ul></li><li class="chapter" data-level="8.6" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#exercises"><i class="fa fa-check"></i><b>8.6</b> Exercises</a></li><li class="chapter" data-level="8.7" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#uncomputablebibnotes"><i class="fa fa-check"></i><b>8.7</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="9" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html"><i class="fa fa-check"></i><b>9</b> Restricted computational models</a><ul><li class="chapter" data-level="9.1" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#turing-completeness-as-a-bug"><i class="fa fa-check"></i><b>9.1</b> Turing completeness as a bug</a></li><li class="chapter" data-level="9.2" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#regular-expressions"><i class="fa fa-check"></i><b>9.2</b> Regular expressions</a></li><li class="chapter" data-level="9.3" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#deterministic-finite-automata-and-efficient-matching-of-regular-expressions-optional"><i class="fa fa-check"></i><b>9.3</b> Deterministic finite automata, and efficient matching of regular expressions (optional)</a><ul><li class="chapter" data-level="9.3.1" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#matching-regular-expressions-using-constant-memory"><i class="fa fa-check"></i><b>9.3.1</b> Matching regular expressions using constant memory</a></li><li class="chapter" data-level="9.3.2" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#secdfa"><i class="fa fa-check"></i><b>9.3.2</b> Deterministic Finite Automata</a></li><li class="chapter" data-level="9.3.3" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#regular-functions-are-closed-under-complement"><i class="fa fa-check"></i><b>9.3.3</b> Regular functions are closed under complement</a></li></ul></li><li class="chapter" data-level="9.4" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#limitations-of-regular-expressions"><i class="fa fa-check"></i><b>9.4</b> Limitations of regular expressions</a></li><li class="chapter" data-level="9.5" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#other-semantic-properties-of-regular-expressions"><i class="fa fa-check"></i><b>9.5</b> Other semantic properties of regular expressions</a></li><li class="chapter" data-level="9.6" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#seccfg"><i class="fa fa-check"></i><b>9.6</b> Context free grammars</a><ul><li class="chapter" data-level="9.6.1" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#context-free-grammars-as-a-computational-model"><i class="fa fa-check"></i><b>9.6.1</b> Context-free grammars as a computational model</a></li><li class="chapter" data-level="9.6.2" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#the-power-of-context-free-grammars"><i class="fa fa-check"></i><b>9.6.2</b> The power of context free grammars</a></li><li class="chapter" data-level="9.6.3" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#limitations-of-context-free-grammars-optional"><i class="fa fa-check"></i><b>9.6.3</b> Limitations of context-free grammars (optional)</a></li></ul></li><li class="chapter" data-level="9.7" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#semantic-properties-of-context-free-languages"><i class="fa fa-check"></i><b>9.7</b> Semantic properties of context free languages</a><ul><li class="chapter" data-level="9.7.1" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#uncomputability-of-context-free-grammar-equivalence-optional"><i class="fa fa-check"></i><b>9.7.1</b> Uncomputability of context-free grammar equivalence (optional)</a></li></ul></li><li class="chapter" data-level="9.8" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#summary-of-semantic-properties-for-regular-expressions-and-context-free-grammars"><i class="fa fa-check"></i><b>9.8</b> Summary of semantic properties for regular expressions and context-free grammars</a></li><li class="chapter" data-level="9.9" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#exercises"><i class="fa fa-check"></i><b>9.9</b> Exercises</a></li><li class="chapter" data-level="9.10" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#bibliographical-notes"><i class="fa fa-check"></i><b>9.10</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="10" data-path="lec_09_godel.html"><a href="lec_09_godel.html"><i class="fa fa-check"></i><b>10</b> Is every theorem provable?</a><ul><li class="chapter" data-level="10.1" data-path="lec_09_godel.html"><a href="lec_09_godel.html#godelproofdef"><i class="fa fa-check"></i><b>10.1</b> Hilbert’s Program and Gödel’s Incompleteness Theorem</a><ul><li class="chapter" data-level="10.1.1" data-path="lec_09_godel.html"><a href="lec_09_godel.html#godelproofsystemssec"><i class="fa fa-check"></i><b>10.1.1</b> Defining Proof Systems</a></li></ul></li><li class="chapter" data-level="10.2" data-path="lec_09_godel.html"><a href="lec_09_godel.html#gödels-incompleteness-theorem-computational-variant"><i class="fa fa-check"></i><b>10.2</b> Gödel’s Incompleteness Theorem: Computational variant</a></li><li class="chapter" data-level="10.3" data-path="lec_09_godel.html"><a href="lec_09_godel.html#quantified-integer-statements"><i class="fa fa-check"></i><b>10.3</b> Quantified integer statements</a></li><li class="chapter" data-level="10.4" data-path="lec_09_godel.html"><a href="lec_09_godel.html#diophantine-equations-and-the-mrdp-theorem"><i class="fa fa-check"></i><b>10.4</b> Diophantine equations and the MRDP Theorem</a></li><li class="chapter" data-level="10.5" data-path="lec_09_godel.html"><a href="lec_09_godel.html#hardness-of-quantified-integer-statements"><i class="fa fa-check"></i><b>10.5</b> Hardness of quantified integer statements</a><ul><li class="chapter" data-level="10.5.1" data-path="lec_09_godel.html"><a href="lec_09_godel.html#step-1-quantified-mixed-statements-and-computation-histories"><i class="fa fa-check"></i><b>10.5.1</b> Step 1: Quantified mixed statements and computation histories</a></li><li class="chapter" data-level="10.5.2" data-path="lec_09_godel.html"><a href="lec_09_godel.html#step-2-reducing-mixed-statements-to-integer-statements"><i class="fa fa-check"></i><b>10.5.2</b> Step 2: Reducing mixed statements to integer statements</a></li></ul></li><li class="chapter" data-level="10.6" data-path="lec_09_godel.html"><a href="lec_09_godel.html#exercises"><i class="fa fa-check"></i><b>10.6</b> Exercises</a></li><li class="chapter" data-level="10.7" data-path="lec_09_godel.html"><a href="lec_09_godel.html#bibliographical-notes"><i class="fa fa-check"></i><b>10.7</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="11" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html"><i class="fa fa-check"></i><b>11</b> Efficient computation</a><ul><li class="chapter" data-level="11.1" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#problems-on-graphs"><i class="fa fa-check"></i><b>11.1</b> Problems on graphs</a><ul><li class="chapter" data-level="11.1.1" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#finding-the-shortest-path-in-a-graph"><i class="fa fa-check"></i><b>11.1.1</b> Finding the shortest path in a graph</a></li><li class="chapter" data-level="11.1.2" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#finding-the-longest-path-in-a-graph"><i class="fa fa-check"></i><b>11.1.2</b> Finding the longest path in a graph</a></li><li class="chapter" data-level="11.1.3" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#mincutsec"><i class="fa fa-check"></i><b>11.1.3</b> Finding the minimum cut in a graph</a></li><li class="chapter" data-level="11.1.4" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#linerprogsec"><i class="fa fa-check"></i><b>11.1.4</b> Min-Cut Max-Flow and Linear programming</a></li><li class="chapter" data-level="11.1.5" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#finding-the-maximum-cut-in-a-graph"><i class="fa fa-check"></i><b>11.1.5</b> Finding the maximum cut in a graph</a></li><li class="chapter" data-level="11.1.6" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#a-note-on-convexity"><i class="fa fa-check"></i><b>11.1.6</b> A note on convexity</a></li></ul></li><li class="chapter" data-level="11.2" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#beyond-graphs"><i class="fa fa-check"></i><b>11.2</b> Beyond graphs</a><ul><li class="chapter" data-level="11.2.1" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#sat"><i class="fa fa-check"></i><b>11.2.1</b> SAT</a></li><li class="chapter" data-level="11.2.2" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#solving-linear-equations"><i class="fa fa-check"></i><b>11.2.2</b> Solving linear equations</a></li><li class="chapter" data-level="11.2.3" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#solving-quadratic-equations"><i class="fa fa-check"></i><b>11.2.3</b> Solving quadratic equations</a></li></ul></li><li class="chapter" data-level="11.3" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#more-advanced-examples"><i class="fa fa-check"></i><b>11.3</b> More advanced examples</a><ul><li class="chapter" data-level="11.3.1" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#determinant-of-a-matrix"><i class="fa fa-check"></i><b>11.3.1</b> Determinant of a matrix</a></li><li class="chapter" data-level="11.3.2" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#permanent-of-a-matrix"><i class="fa fa-check"></i><b>11.3.2</b> Permanent of a matrix</a></li><li class="chapter" data-level="11.3.3" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#finding-a-zero-sum-equilibrium"><i class="fa fa-check"></i><b>11.3.3</b> Finding a zero-sum equilibrium</a></li><li class="chapter" data-level="11.3.4" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#finding-a-nash-equilibrium"><i class="fa fa-check"></i><b>11.3.4</b> Finding a Nash equilibrium</a></li><li class="chapter" data-level="11.3.5" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#primality-testing"><i class="fa fa-check"></i><b>11.3.5</b> Primality testing</a></li><li class="chapter" data-level="11.3.6" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#integer-factoring"><i class="fa fa-check"></i><b>11.3.6</b> Integer factoring</a></li></ul></li><li class="chapter" data-level="11.4" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#our-current-knowledge"><i class="fa fa-check"></i><b>11.4</b> Our current knowledge</a></li><li class="chapter" data-level="11.5" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#exercises"><i class="fa fa-check"></i><b>11.5</b> Exercises</a></li><li class="chapter" data-level="11.6" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#effalgnotes"><i class="fa fa-check"></i><b>11.6</b> Bibliographical notes</a></li><li class="chapter" data-level="11.7" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#further-explorations"><i class="fa fa-check"></i><b>11.7</b> Further explorations</a></li></ul></li><li class="chapter" data-level="12" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html"><i class="fa fa-check"></i><b>12</b> Modeling running time</a><ul><li class="chapter" data-level="12.1" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#formally-defining-running-time"><i class="fa fa-check"></i><b>12.1</b> Formally defining running time</a><ul><li class="chapter" data-level="12.1.1" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#polynomial-and-exponential-time"><i class="fa fa-check"></i><b>12.1.1</b> Polynomial and Exponential Time</a></li></ul></li><li class="chapter" data-level="12.2" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#modeling-running-time-using-ram-machines-nand-ram"><i class="fa fa-check"></i><b>12.2</b> Modeling running time using RAM Machines / NAND-RAM</a></li><li class="chapter" data-level="12.3" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#ECTTsec"><i class="fa fa-check"></i><b>12.3</b> Extended Church-Turing Thesis (discussion)</a></li><li class="chapter" data-level="12.4" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#efficient-universal-machine-a-nand-ram-interpreter-in-nand-ram"><i class="fa fa-check"></i><b>12.4</b> Efficient universal machine: a NAND-RAM interpreter in NAND-RAM</a><ul><li class="chapter" data-level="12.4.1" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#timed-universal-turing-machine"><i class="fa fa-check"></i><b>12.4.1</b> Timed Universal Turing Machine</a></li></ul></li><li class="chapter" data-level="12.5" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#the-time-hierarchy-theorem"><i class="fa fa-check"></i><b>12.5</b> The time hierarchy theorem</a></li><li class="chapter" data-level="12.6" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#nonuniformcompsec"><i class="fa fa-check"></i><b>12.6</b> Non uniform computation</a><ul><li class="chapter" data-level="12.6.1" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#obliviousnandtm"><i class="fa fa-check"></i><b>12.6.1</b> Oblivious NAND-TM programs</a></li><li class="chapter" data-level="12.6.2" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#unrollloopsec"><i class="fa fa-check"></i><b>12.6.2</b> Unrolling the loop: algorithmic transformation of Turing Machines to circuits</a></li><li class="chapter" data-level="12.6.3" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#can-uniform-algorithms-simulate-non-uniform-ones"><i class="fa fa-check"></i><b>12.6.3</b> Can uniform algorithms simulate non uniform ones?</a></li><li class="chapter" data-level="12.6.4" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#uniform-vs.-nonuniform-computation-a-recap"><i class="fa fa-check"></i><b>12.6.4</b> Uniform vs. Nonuniform computation: A recap</a></li></ul></li><li class="chapter" data-level="12.7" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#exercises"><i class="fa fa-check"></i><b>12.7</b> Exercises</a></li><li class="chapter" data-level="12.8" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#bibnotesrunningtime"><i class="fa fa-check"></i><b>12.8</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="13" data-path="lec_12_NP.html"><a href="lec_12_NP.html"><i class="fa fa-check"></i><b>13</b> Polynomial-time reductions</a><ul><li class="chapter" data-level="13.1" data-path="lec_12_NP.html"><a href="lec_12_NP.html#formaldefdecisionexamplessec"><i class="fa fa-check"></i><b>13.1</b> Formal definitions of problems</a></li><li class="chapter" data-level="13.2" data-path="lec_12_NP.html"><a href="lec_12_NP.html#polytimeredsec"><i class="fa fa-check"></i><b>13.2</b> Polynomial-time reductions</a></li><li class="chapter" data-level="13.3" data-path="lec_12_NP.html"><a href="lec_12_NP.html#reducing-3sat-to-zero-one-equations"><i class="fa fa-check"></i><b>13.3</b> Reducing 3SAT to zero one equations</a><ul><li class="chapter" data-level="13.3.1" data-path="lec_12_NP.html"><a href="lec_12_NP.html#quadratic-equations"><i class="fa fa-check"></i><b>13.3.1</b> Quadratic equations</a></li></ul></li><li class="chapter" data-level="13.4" data-path="lec_12_NP.html"><a href="lec_12_NP.html#the-independent-set-problem"><i class="fa fa-check"></i><b>13.4</b> The independent set problem</a></li><li class="chapter" data-level="13.5" data-path="lec_12_NP.html"><a href="lec_12_NP.html#reducing-independent-set-to-maximum-cut"><i class="fa fa-check"></i><b>13.5</b> Reducing Independent Set to Maximum Cut</a></li><li class="chapter" data-level="13.6" data-path="lec_12_NP.html"><a href="lec_12_NP.html#reducing-3sat-to-longest-path"><i class="fa fa-check"></i><b>13.6</b> Reducing 3SAT to Longest Path</a><ul><li class="chapter" data-level="13.6.1" data-path="lec_12_NP.html"><a href="lec_12_NP.html#summary-of-relations"><i class="fa fa-check"></i><b>13.6.1</b> Summary of relations</a></li></ul></li><li class="chapter" data-level="13.7" data-path="lec_12_NP.html"><a href="lec_12_NP.html#exercises"><i class="fa fa-check"></i><b>13.7</b> Exercises</a></li><li class="chapter" data-level="13.8" data-path="lec_12_NP.html"><a href="lec_12_NP.html#reductionsbibnotes"><i class="fa fa-check"></i><b>13.8</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="14" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html"><i class="fa fa-check"></i><b>14</b> NP, NP completeness, and the Cook-Levin Theorem</a><ul><li class="chapter" data-level="14.1" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#the-class-mathbfnp"><i class="fa fa-check"></i><b>14.1</b> The class \mathbf{NP}</a><ul><li class="chapter" data-level="14.1.1" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#examples-of-functions-in-mathbfnp"><i class="fa fa-check"></i><b>14.1.1</b> Examples of functions in \mathbf{NP}</a></li><li class="chapter" data-level="14.1.2" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#basic-facts-about-mathbfnp"><i class="fa fa-check"></i><b>14.1.2</b> Basic facts about \mathbf{NP}</a></li></ul></li><li class="chapter" data-level="14.2" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#from-mathbfnp-to-3sat-the-cook-levin-theorem"><i class="fa fa-check"></i><b>14.2</b> From \mathbf{NP} to 3SAT: The Cook-Levin Theorem</a><ul><li class="chapter" data-level="14.2.1" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#what-does-this-mean"><i class="fa fa-check"></i><b>14.2.1</b> What does this mean?</a></li><li class="chapter" data-level="14.2.2" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#the-cook-levin-theorem-proof-outline"><i class="fa fa-check"></i><b>14.2.2</b> The Cook-Levin Theorem: Proof outline</a></li></ul></li><li class="chapter" data-level="14.3" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#the-nandsat-problem-and-why-it-is-mathbfnp-hard."><i class="fa fa-check"></i><b>14.3</b> The \ensuremath{\mathit{NANDSAT}} Problem, and why it is \mathbf{NP} hard.</a></li><li class="chapter" data-level="14.4" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#the-3nand-problem"><i class="fa fa-check"></i><b>14.4</b> The 3\ensuremath{\mathit{NAND}} problem</a></li><li class="chapter" data-level="14.5" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#from-3nand-to-3sat"><i class="fa fa-check"></i><b>14.5</b> From 3\ensuremath{\mathit{NAND}} to 3\ensuremath{\mathit{SAT}}</a></li><li class="chapter" data-level="14.6" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#wrapping-up"><i class="fa fa-check"></i><b>14.6</b> Wrapping up</a></li><li class="chapter" data-level="14.7" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#exercises"><i class="fa fa-check"></i><b>14.7</b> Exercises</a></li><li class="chapter" data-level="14.8" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#bibliographical-notes"><i class="fa fa-check"></i><b>14.8</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="15" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html"><i class="fa fa-check"></i><b>15</b> What if P equals NP?</a><ul><li class="chapter" data-level="15.1" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#search-to-decision-reduction"><i class="fa fa-check"></i><b>15.1</b> Search-to-decision reduction</a></li><li class="chapter" data-level="15.2" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#optimizationsection"><i class="fa fa-check"></i><b>15.2</b> Optimization</a><ul><li class="chapter" data-level="15.2.1" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#example-supervised-learning"><i class="fa fa-check"></i><b>15.2.1</b> Example: Supervised learning</a></li><li class="chapter" data-level="15.2.2" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#example-breaking-cryptosystems"><i class="fa fa-check"></i><b>15.2.2</b> Example: Breaking cryptosystems</a></li></ul></li><li class="chapter" data-level="15.3" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#finding-mathematical-proofs"><i class="fa fa-check"></i><b>15.3</b> Finding mathematical proofs</a></li><li class="chapter" data-level="15.4" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#quantifier-elimination-advanced"><i class="fa fa-check"></i><b>15.4</b> Quantifier elimination (advanced)</a><ul><li class="chapter" data-level="15.4.1" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#selfimprovingsat"><i class="fa fa-check"></i><b>15.4.1</b> Application: self improving algorithm for 3\ensuremath{\mathit{SAT}}</a></li></ul></li><li class="chapter" data-level="15.5" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#approximating-counting-problems-and-posterior-sampling-advanced-optional"><i class="fa fa-check"></i><b>15.5</b> Approximating counting problems and posterior sampling (advanced, optional)</a></li><li class="chapter" data-level="15.6" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#what-does-all-of-this-imply"><i class="fa fa-check"></i><b>15.6</b> What does all of this imply?</a></li><li class="chapter" data-level="15.7" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#can-mathbfp-neq-mathbfnp-be-neither-true-nor-false"><i class="fa fa-check"></i><b>15.7</b> Can \mathbf{P} \neq \mathbf{NP} be neither true nor false?</a></li><li class="chapter" data-level="15.8" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#is-mathbfpmathbfnp-in-practice"><i class="fa fa-check"></i><b>15.8</b> Is \mathbf{P}=\mathbf{NP} in practice?</a></li><li class="chapter" data-level="15.9" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#what-if-mathbfp-neq-mathbfnp"><i class="fa fa-check"></i><b>15.9</b> What if \mathbf{P} \neq \mathbf{NP}?</a></li><li class="chapter" data-level="15.10" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#exercises"><i class="fa fa-check"></i><b>15.10</b> Exercises</a></li><li class="chapter" data-level="15.11" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#bibliographical-notes"><i class="fa fa-check"></i><b>15.11</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="16" data-path="lec_14a_space_complexity.html"><a href="lec_14a_space_complexity.html"><i class="fa fa-check"></i><b>16</b> Space bounded computation</a><ul><li class="chapter" data-level="16.1" data-path="lec_14a_space_complexity.html"><a href="lec_14a_space_complexity.html#exercises"><i class="fa fa-check"></i><b>16.1</b> Exercises</a></li><li class="chapter" data-level="16.2" data-path="lec_14a_space_complexity.html"><a href="lec_14a_space_complexity.html#bibliographical-notes"><i class="fa fa-check"></i><b>16.2</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="17" data-path="lec_15_probability.html"><a href="lec_15_probability.html"><i class="fa fa-check"></i><b>17</b> Probability Theory 101</a><ul><li class="chapter" data-level="17.1" data-path="lec_15_probability.html"><a href="lec_15_probability.html#random-coins"><i class="fa fa-check"></i><b>17.1</b> Random coins</a><ul><li class="chapter" data-level="17.1.1" data-path="lec_15_probability.html"><a href="lec_15_probability.html#random-variables"><i class="fa fa-check"></i><b>17.1.1</b> Random variables</a></li><li class="chapter" data-level="17.1.2" data-path="lec_15_probability.html"><a href="lec_15_probability.html#distributions-over-strings"><i class="fa fa-check"></i><b>17.1.2</b> Distributions over strings</a></li><li class="chapter" data-level="17.1.3" data-path="lec_15_probability.html"><a href="lec_15_probability.html#more-general-sample-spaces."><i class="fa fa-check"></i><b>17.1.3</b> More general sample spaces.</a></li></ul></li><li class="chapter" data-level="17.2" data-path="lec_15_probability.html"><a href="lec_15_probability.html#correlations-and-independence"><i class="fa fa-check"></i><b>17.2</b> Correlations and independence</a><ul><li class="chapter" data-level="17.2.1" data-path="lec_15_probability.html"><a href="lec_15_probability.html#independent-random-variables"><i class="fa fa-check"></i><b>17.2.1</b> Independent random variables</a></li><li class="chapter" data-level="17.2.2" data-path="lec_15_probability.html"><a href="lec_15_probability.html#collections-of-independent-random-variables."><i class="fa fa-check"></i><b>17.2.2</b> Collections of independent random variables.</a></li></ul></li><li class="chapter" data-level="17.3" data-path="lec_15_probability.html"><a href="lec_15_probability.html#concentration-and-tail-bounds"><i class="fa fa-check"></i><b>17.3</b> Concentration and tail bounds</a><ul><li class="chapter" data-level="17.3.1" data-path="lec_15_probability.html"><a href="lec_15_probability.html#chebyshevs-inequality"><i class="fa fa-check"></i><b>17.3.1</b> Chebyshev’s Inequality</a></li><li class="chapter" data-level="17.3.2" data-path="lec_15_probability.html"><a href="lec_15_probability.html#the-chernoff-bound"><i class="fa fa-check"></i><b>17.3.2</b> The Chernoff bound</a></li></ul></li><li class="chapter" data-level="17.4" data-path="lec_15_probability.html"><a href="lec_15_probability.html#exercises"><i class="fa fa-check"></i><b>17.4</b> Exercises</a></li><li class="chapter" data-level="17.5" data-path="lec_15_probability.html"><a href="lec_15_probability.html#bibliographical-notes"><i class="fa fa-check"></i><b>17.5</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="18" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html"><i class="fa fa-check"></i><b>18</b> Probabilistic computation</a><ul><li class="chapter" data-level="18.1" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html#finding-approximately-good-maximum-cuts."><i class="fa fa-check"></i><b>18.1</b> Finding approximately good maximum cuts.</a><ul><li class="chapter" data-level="18.1.1" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html#amplifying-the-success-of-randomized-algorithms"><i class="fa fa-check"></i><b>18.1.1</b> Amplifying the success of randomized algorithms</a></li><li class="chapter" data-level="18.1.2" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html#success-amplification"><i class="fa fa-check"></i><b>18.1.2</b> Success amplification</a></li><li class="chapter" data-level="18.1.3" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html#two-sided-amplification"><i class="fa fa-check"></i><b>18.1.3</b> Two-sided amplification</a></li><li class="chapter" data-level="18.1.4" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html#what-does-this-mean"><i class="fa fa-check"></i><b>18.1.4</b> What does this mean?</a></li><li class="chapter" data-level="18.1.5" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html#solving-sat-through-randomization"><i class="fa fa-check"></i><b>18.1.5</b> Solving SAT through randomization</a></li><li class="chapter" data-level="18.1.6" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html#bipartite-matching."><i class="fa fa-check"></i><b>18.1.6</b> Bipartite matching.</a></li></ul></li><li class="chapter" data-level="18.2" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html#exercises"><i class="fa fa-check"></i><b>18.2</b> Exercises</a></li><li class="chapter" data-level="18.3" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html#bibliographical-notes"><i class="fa fa-check"></i><b>18.3</b> Bibliographical notes</a></li><li class="chapter" data-level="18.4" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html#acknowledgements"><i class="fa fa-check"></i><b>18.4</b> Acknowledgements</a></li></ul></li><li class="chapter" data-level="19" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html"><i class="fa fa-check"></i><b>19</b> Modeling randomized computation</a><ul><li class="chapter" data-level="19.1" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#modeling-randomized-computation"><i class="fa fa-check"></i><b>19.1</b> Modeling randomized computation</a><ul><li class="chapter" data-level="19.1.1" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#an-alternative-view-random-coins-as-an-extra-input"><i class="fa fa-check"></i><b>19.1.1</b> An alternative view: random coins as an extra input</a></li><li class="chapter" data-level="19.1.2" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#successamptwosided"><i class="fa fa-check"></i><b>19.1.2</b> Success amplification of two-sided error algorithms</a></li></ul></li><li class="chapter" data-level="19.2" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#mathbfbpp-and-mathbfnp-completeness"><i class="fa fa-check"></i><b>19.2</b> \mathbf{BPP} and \mathbf{NP} completeness</a></li><li class="chapter" data-level="19.3" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#the-power-of-randomization"><i class="fa fa-check"></i><b>19.3</b> The power of randomization</a><ul><li class="chapter" data-level="19.3.1" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#solving-mathbfbpp-in-exponential-time"><i class="fa fa-check"></i><b>19.3.1</b> Solving \mathbf{BPP} in exponential time</a></li><li class="chapter" data-level="19.3.2" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#simulating-randomized-algorithms-by-circuits"><i class="fa fa-check"></i><b>19.3.2</b> Simulating randomized algorithms by circuits</a></li></ul></li><li class="chapter" data-level="19.4" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#derandomization"><i class="fa fa-check"></i><b>19.4</b> Derandomization</a><ul><li class="chapter" data-level="19.4.1" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#pseudorandom-generators"><i class="fa fa-check"></i><b>19.4.1</b> Pseudorandom generators</a></li><li class="chapter" data-level="19.4.2" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#optimalprgconj"><i class="fa fa-check"></i><b>19.4.2</b> From existence to constructivity</a></li><li class="chapter" data-level="19.4.3" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#usefulness-of-pseudorandom-generators"><i class="fa fa-check"></i><b>19.4.3</b> Usefulness of pseudorandom generators</a></li></ul></li><li class="chapter" data-level="19.5" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#mathbfpmathbfnp-and-mathbfbpp-vs-mathbfp"><i class="fa fa-check"></i><b>19.5</b> \mathbf{P}=\mathbf{NP} and \mathbf{BPP} vs \mathbf{P}</a></li><li class="chapter" data-level="19.6" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#non-constructive-existence-of-pseudorandom-generators-advanced-optional"><i class="fa fa-check"></i><b>19.6</b> Non-constructive existence of pseudorandom generators (advanced, optional)</a></li><li class="chapter" data-level="19.7" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#exercises"><i class="fa fa-check"></i><b>19.7</b> Exercises</a></li><li class="chapter" data-level="19.8" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#modelrandbibnotes"><i class="fa fa-check"></i><b>19.8</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="20" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html"><i class="fa fa-check"></i><b>20</b> Cryptography</a><ul><li class="chapter" data-level="20.1" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#classical-cryptosystems"><i class="fa fa-check"></i><b>20.1</b> Classical cryptosystems</a></li><li class="chapter" data-level="20.2" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#defining-encryption"><i class="fa fa-check"></i><b>20.2</b> Defining encryption</a></li><li class="chapter" data-level="20.3" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#defining-security-of-encryption"><i class="fa fa-check"></i><b>20.3</b> Defining security of encryption</a></li><li class="chapter" data-level="20.4" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#perfect-secrecy"><i class="fa fa-check"></i><b>20.4</b> Perfect secrecy</a><ul><li class="chapter" data-level="20.4.1" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#example-perfect-secrecy-in-the-battlefield"><i class="fa fa-check"></i><b>20.4.1</b> Example: Perfect secrecy in the battlefield</a></li><li class="chapter" data-level="20.4.2" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#constructing-perfectly-secret-encryption"><i class="fa fa-check"></i><b>20.4.2</b> Constructing perfectly secret encryption</a></li></ul></li><li class="chapter" data-level="20.5" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#necessity-of-long-keys"><i class="fa fa-check"></i><b>20.5</b> Necessity of long keys</a></li><li class="chapter" data-level="20.6" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#computational-secrecy"><i class="fa fa-check"></i><b>20.6</b> Computational secrecy</a><ul><li class="chapter" data-level="20.6.1" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#stream-ciphers-or-the-derandomized-one-time-pad"><i class="fa fa-check"></i><b>20.6.1</b> Stream ciphers or the derandomized one-time pad</a></li></ul></li><li class="chapter" data-level="20.7" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#computational-secrecy-and-mathbfnp"><i class="fa fa-check"></i><b>20.7</b> Computational secrecy and \mathbf{NP}</a></li><li class="chapter" data-level="20.8" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#public-key-cryptography"><i class="fa fa-check"></i><b>20.8</b> Public key cryptography</a><ul><li class="chapter" data-level="20.8.1" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#defining-public-key-encryption"><i class="fa fa-check"></i><b>20.8.1</b> Defining public key encryption</a></li><li class="chapter" data-level="20.8.2" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#diffie-hellman-key-exchange"><i class="fa fa-check"></i><b>20.8.2</b> Diffie-Hellman key exchange</a></li></ul></li><li class="chapter" data-level="20.9" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#other-security-notions"><i class="fa fa-check"></i><b>20.9</b> Other security notions</a></li><li class="chapter" data-level="20.10" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#magic"><i class="fa fa-check"></i><b>20.10</b> Magic</a><ul><li class="chapter" data-level="20.10.1" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#zero-knowledge-proofs"><i class="fa fa-check"></i><b>20.10.1</b> Zero knowledge proofs</a></li><li class="chapter" data-level="20.10.2" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#fully-homomorphic-encryption"><i class="fa fa-check"></i><b>20.10.2</b> Fully homomorphic encryption</a></li><li class="chapter" data-level="20.10.3" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#multiparty-secure-computation"><i class="fa fa-check"></i><b>20.10.3</b> Multiparty secure computation</a></li></ul></li><li class="chapter" data-level="20.11" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#exercises"><i class="fa fa-check"></i><b>20.11</b> Exercises</a></li><li class="chapter" data-level="20.12" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#bibliographical-notes"><i class="fa fa-check"></i><b>20.12</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="21" data-path="lec_24_proofs.html"><a href="lec_24_proofs.html"><i class="fa fa-check"></i><b>21</b> Proofs and algorithms</a><ul><li class="chapter" data-level="21.1" data-path="lec_24_proofs.html"><a href="lec_24_proofs.html#exercises"><i class="fa fa-check"></i><b>21.1</b> Exercises</a></li><li class="chapter" data-level="21.2" data-path="lec_24_proofs.html"><a href="lec_24_proofs.html#bibliographical-notes"><i class="fa fa-check"></i><b>21.2</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="22" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html"><i class="fa fa-check"></i><b>22</b> Quantum computing</a><ul><li class="chapter" data-level="22.1" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#the-double-slit-experiment"><i class="fa fa-check"></i><b>22.1</b> The double slit experiment</a></li><li class="chapter" data-level="22.2" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantum-amplitudes"><i class="fa fa-check"></i><b>22.2</b> Quantum amplitudes</a><ul><li class="chapter" data-level="22.2.1" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#linear-algebra-quick-review"><i class="fa fa-check"></i><b>22.2.1</b> Linear algebra quick review</a></li></ul></li><li class="chapter" data-level="22.3" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#bellineqsec"><i class="fa fa-check"></i><b>22.3</b> Bell’s Inequality</a></li><li class="chapter" data-level="22.4" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantum-weirdness"><i class="fa fa-check"></i><b>22.4</b> Quantum weirdness</a></li><li class="chapter" data-level="22.5" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantum-computing-and-computation---an-executive-summary."><i class="fa fa-check"></i><b>22.5</b> Quantum computing and computation - an executive summary.</a></li><li class="chapter" data-level="22.6" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantum-systems"><i class="fa fa-check"></i><b>22.6</b> Quantum systems</a><ul><li class="chapter" data-level="22.6.1" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantum-amplitudes-1"><i class="fa fa-check"></i><b>22.6.1</b> Quantum amplitudes</a></li><li class="chapter" data-level="22.6.2" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantum-systems-an-executive-summary"><i class="fa fa-check"></i><b>22.6.2</b> Quantum systems: an executive summary</a></li></ul></li><li class="chapter" data-level="22.7" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#analysis-of-bells-inequality-optional"><i class="fa fa-check"></i><b>22.7</b> Analysis of Bell’s Inequality (optional)</a></li><li class="chapter" data-level="22.8" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantum-computation"><i class="fa fa-check"></i><b>22.8</b> Quantum computation</a><ul><li class="chapter" data-level="22.8.1" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantum-circuits"><i class="fa fa-check"></i><b>22.8.1</b> Quantum circuits</a></li><li class="chapter" data-level="22.8.2" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#qnand-circ-programs-optional"><i class="fa fa-check"></i><b>22.8.2</b> QNAND-CIRC programs (optional)</a></li><li class="chapter" data-level="22.8.3" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#uniform-computation"><i class="fa fa-check"></i><b>22.8.3</b> Uniform computation</a></li></ul></li><li class="chapter" data-level="22.9" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#physically-realizing-quantum-computation"><i class="fa fa-check"></i><b>22.9</b> Physically realizing quantum computation</a></li><li class="chapter" data-level="22.10" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#shors-algorithm-hearing-the-shape-of-prime-factors"><i class="fa fa-check"></i><b>22.10</b> Shor’s Algorithm: Hearing the shape of prime factors</a><ul><li class="chapter" data-level="22.10.1" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#period-finding"><i class="fa fa-check"></i><b>22.10.1</b> Period finding</a></li><li class="chapter" data-level="22.10.2" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#shors-algorithm-a-birds-eye-view"><i class="fa fa-check"></i><b>22.10.2</b> Shor’s Algorithm: A bird’s eye view</a></li></ul></li><li class="chapter" data-level="22.11" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantum-fourier-transform-advanced-optional"><i class="fa fa-check"></i><b>22.11</b> Quantum Fourier Transform (advanced, optional)</a><ul><li class="chapter" data-level="22.11.1" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantum-fourier-transform-over-the-boolean-cube-simons-algorithm"><i class="fa fa-check"></i><b>22.11.1</b> Quantum Fourier Transform over the Boolean Cube: Simon’s Algorithm</a></li><li class="chapter" data-level="22.11.2" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#from-fourier-to-period-finding-simons-algorithm-advanced-optional"><i class="fa fa-check"></i><b>22.11.2</b> From Fourier to Period finding: Simon’s Algorithm (advanced, optional)</a></li><li class="chapter" data-level="22.11.3" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#from-simon-to-shor-advanced-optional"><i class="fa fa-check"></i><b>22.11.3</b> From Simon to Shor (advanced, optional)</a></li></ul></li><li class="chapter" data-level="22.12" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#exercises"><i class="fa fa-check"></i><b>22.12</b> Exercises</a></li><li class="chapter" data-level="22.13" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantumbibnotessec"><i class="fa fa-check"></i><b>22.13</b> Bibliographical notes</a></li></ul></li><li class="divider"></li>
</ul>
<!--bookdown:toc2:end-->
      </nav>
    </div>

    <div class="book-header" role="navigation">
      <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Modeling randomized computation</a>
      </h1>
    </div>

    <div class="book-body">
      <div class="body-inner">


        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<!--bookdown:toc:end-->
<!--bookdown:body:start-->

<div  class="section level2">

<!-- link to pdf version -->


<!-- start of header referring to comments -->
<div><p></p><p style="color:#871640;"><i class="fas fa-wrench"></i> See any bugs/typos/confusing explanations? <a href="https://github.com/boazbk/tcs/issues/new">Open a GitHub issue</a>. You can also <a href="#commentform">comment below</a> <i class="fas fa-wrench"></i></p></div>



<div><p style="color:#871640;">&#x2605; See also the <a id="pdflink" href='https://files.boazbarak.org/introtcs/lec_17_model_rand.pdf'><b>PDF version of this chapter</b></a> (better formatting/references) &#x2605;</p></div>

<!-- end of header referring to comments -->

<!--- start of actual content -->

<h1 id="chapmodelrand" data-number="19">Modeling randomized computation</h1>
<div id="section" class="objectives" name="Objectives">
<ul>
<li>Formal definition of probabilistic polynomial time: the class <span><span class="math inline">\(\mathbf{BPP}\)</span></span>.<br />
</li>
<li>Proof that that every function in <span><span class="math inline">\(\mathbf{BPP}\)</span></span> can be computed by <span><span class="math inline">\(poly(n)\)</span></span>-sized NAND-CIRC programs/circuits.<br />
</li>
<li>Relations between <span><span class="math inline">\(\mathbf{BPP}\)</span></span> and <span><span class="math inline">\(\mathbf{NP}\)</span></span>.<br />
</li>
<li>Pseudorandom generators</li>
</ul>
</div>
<blockquote>
<p><em>“Any one who considers arithmetical methods of producing random digits is, of course, in a state of sin.”</em> John von Neumann, 1951.</p>
</blockquote>
<p>So far we have described randomized algorithms in an informal way, assuming that an operation such as “pick a string <span><span class="math inline">\(x\in \{0,1\}^n\)</span></span>” can be done efficiently. We have neglected to address two questions:</p>
<ol type="1">
<li><p>How do we actually efficiently obtain random strings in the physical world?</p></li>
<li><p>What is the mathematical model for randomized computations, and is it more powerful than deterministic computation?</p></li>
</ol>
<p>The first question is of both practical and theoretical importance, but for now let’s just say that there are various physical sources of “random” or “unpredictable” data. A user’s mouse movements and typing pattern, (non solid state) hard drive and network latency, thermal noise, and radioactive decay have all been used as sources for randomness (see discussion in <a href='#modelrandbibnotes'>Section 19.8</a>). For example, many Intel chips come with a random number generator <a href="http://spectrum.ieee.org/computing/hardware/behind-intels-new-randomnumber-generator">built in</a>. One can even build mechanical coin tossing machines (see <a href='#coinfig'>Figure 19.1</a>).</p>
<figure>
<img src="../figure/coin_tosser.jpg" alt="19.1: A mechanical coin tosser built for Percy Diaconis by Harvard technicians Steve Sansone and Rick Haggerty" id="coinfig" class="margin" /><figcaption>19.1: A mechanical coin tosser built for Percy Diaconis by Harvard technicians Steve Sansone and Rick Haggerty</figcaption>
</figure>
<p>In this chapter we focus on the second question: formally modeling probabilistic computation and studying its power. We will show that:</p>
<ol type="1">
<li><p>We can define the class <span><span class="math inline">\(\mathbf{BPP}\)</span></span> that captures all Boolean functions that can be computed in polynomial time by a randomized algorithm. Crucially <span><span class="math inline">\(\mathbf{BPP}\)</span></span> is still very much a <em>worst case</em> class of computation: the probability is only over the choice of the random coins of the algorithm, as opposed to the choice of the input.</p></li>
<li><p>We can <em>amplify</em> the success probability of randomized algorithms, and as a result the class <span><span class="math inline">\(\mathbf{BPP}\)</span></span> would be identical if we changed the required success probability to any number <span><span class="math inline">\(p\)</span></span> that lies strictly between <span><span class="math inline">\(1/2\)</span></span> and <span><span class="math inline">\(1\)</span></span> (and in fact any number in the range <span><span class="math inline">\(1/2 + 1/q(n)\)</span></span> to <span><span class="math inline">\(1-2^{-q(n)}\)</span></span> for any polynomial <span><span class="math inline">\(q(n)\)</span></span>).</p></li>
<li><p>Though, as is the case for <span><span class="math inline">\(\mathbf{P}\)</span></span> and <span><span class="math inline">\(\mathbf{NP}\)</span></span>, there is much we do not know about the class <span><span class="math inline">\(\mathbf{BPP}\)</span></span>, we can establish some relations between <span><span class="math inline">\(\mathbf{BPP}\)</span></span> and the other complexity classes we saw before. In particular we will show that <span><span class="math inline">\(\mathbf{P} \subseteq \mathbf{BPP} \subseteq \mathbf{EXP}\)</span></span> and <span><span class="math inline">\(\mathbf{BPP} \subseteq \mathbf{P_{/poly}}\)</span></span>.</p></li>
<li><p>While the relation between <span><span class="math inline">\(\mathbf{BPP}\)</span></span> and <span><span class="math inline">\(\mathbf{NP}\)</span></span> is not known, we can show that if <span><span class="math inline">\(\mathbf{P}=\mathbf{NP}\)</span></span> then <span><span class="math inline">\(\mathbf{BPP}=\mathbf{P}\)</span></span>.</p></li>
<li><p>We also show that the concept of <span><span class="math inline">\(\mathbf{NP}\)</span></span> completeness applies equally well if we use randomized algorithms as our model of “efficient computation”. That is, if a single <span><span class="math inline">\(\mathbf{NP}\)</span></span> complete problem has a randomized polynomial-time algorithm, then all of <span><span class="math inline">\(\mathbf{NP}\)</span></span> can be computed in polynomial-time by randomized algorithms.</p></li>
<li><p>Finally we will discuss the question of whether <span><span class="math inline">\(\mathbf{BPP} = \mathbf{P}\)</span></span> and show some of the intriguing evidence that the answer might actually be <em>“Yes”</em> using the concept of <em>pseudorandom generators</em>.</p></li>
</ol>
<h2 id="modeling-randomized-computation" data-number="19.1">Modeling randomized computation</h2>
<p>Modeling randomized computation is actually quite easy. We can add the following operations to any programming language such as NAND-TM, NAND-RAM, NAND-CIRC etc..:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" title="1">foo <span class="op">=</span> RAND()</a></code></pre></div>
<p>where <code>foo</code> is a variable. The result of applying this operation is that <code>foo</code> is assigned a random bit in <span><span class="math inline">\(\{0,1\}\)</span></span>. (Every time the <code>RAND</code> operation is invoked it returns a fresh independent random bit.) We call the programming languages that are augmented with this extra operation RNAND-TM, RNAND-RAM, and RNAND-CIRC respectively.</p>
<p>Similarly, we can easily define randomized Turing machines as Turing machines in which the transition function <span><span class="math inline">\(\delta\)</span></span> gets as an extra input (in addition to the current state and symbol read from the tape) a bit <span><span class="math inline">\(b\)</span></span> that in each step is chosen at random <span><span class="math inline">\(\{0,1\}\)</span></span>. Of course the function can ignore this bit (and have the same output regardless of whether <span><span class="math inline">\(b=0\)</span></span> or <span><span class="math inline">\(b=1\)</span></span>) and hence randomized Turing machines generalize deterministic Turing machines.</p>
<p>We can use the <code>RAND()</code> operation to define the notion of a function being computed by a randomized <span><span class="math inline">\(T(n)\)</span></span> time algorithm for every nice time bound <span><span class="math inline">\(T:\N \rightarrow \N\)</span></span>, as well as the notion of a finite function being computed by a size <span><span class="math inline">\(S\)</span></span> randomized NAND-CIRC program (or, equivalently, a randomized circuit with <span><span class="math inline">\(S\)</span></span> gates that correspond to either the NAND or coin-tossing operations). However, for simplicity we will not define randomized computation in full generality, but simply focus on the class of functions that are computable by randomized algorithms <em>running in polynomial time</em>, which by historical convention is known as <span><span class="math inline">\(\mathbf{BPP}\)</span></span>:</p>
<div id="BPPdef" class="definition" title="The class $\mathbf{BPP}$" name="Definition 19.1 (The class $\mathbf{BPP}$) ">
<p>Let <span><span class="math inline">\(F: \{0,1\}^*\rightarrow \{0,1\}\)</span></span>. We say that <span><span class="math inline">\(F\in \mathbf{BPP}\)</span></span> if there exist constants <span><span class="math inline">\(a,b\in \N\)</span></span> and an RNAND-TM program <span><span class="math inline">\(P\)</span></span> such that for every <span><span class="math inline">\(x\in \{0,1\}^*\)</span></span>, on input <span><span class="math inline">\(x\)</span></span>, the program <span><span class="math inline">\(P\)</span></span> halts within at most <span><span class="math inline">\(a|x|^b\)</span></span> steps and <span>
<div class='myequationbox'><span class="math display">\[
\Pr[ P(x)= F(x)] \geq \tfrac{2}{3} \;\;(19.1)
\]</span><a id='BPPdefinitioneq'></a></div></span> where this probability is taken over the result of the RAND operations of <span><span class="math inline">\(P\)</span></span>.</p>
</div>
<p>Note that the probability in <a href='#BPPdefinitioneq'>Equation 19.1</a> is taken only over the random choices in the execution of <span><span class="math inline">\(P\)</span></span> and <em>not</em> over the choice of the input <span><span class="math inline">\(x\)</span></span>. In particular, as discussed in <a href='lec_16_randomized_alg.html#randomworstcaseidea'>Bigidea 23</a>, <span><span class="math inline">\(\mathbf{BPP}\)</span></span> is still a <em>worst case</em> complexity class, in the sense that if <span><span class="math inline">\(F\)</span></span> is in <span><span class="math inline">\(\mathbf{BPP}\)</span></span> then there is a polynomial-time randomized algorithm that computes <span><span class="math inline">\(F\)</span></span> with probability at least <span><span class="math inline">\(2/3\)</span></span> <em>on every possible</em> (and not just random) input.</p>
<p>The same polynomial-overhead simulation of NAND-RAM programs by NAND-TM programs we saw in <a href='lec_11_running_time.html#polyRAMTM-thm'>Theorem 12.5</a> extends to <em>randomized</em> programs as well. Hence the class <span><span class="math inline">\(\mathbf{BPP}\)</span></span> is the same regardless of whether it is defined via RNAND-TM or RNAND-RAM programs. Similarly, we could have just as well defined <span><span class="math inline">\(\mathbf{BPP}\)</span></span> using randomized Turing machines.</p>
<p>Because of these equivalences, below we will use the name <em>“polynomial time randomized algorithm”</em> to denote a computation that can be modeled by a polynomial-time RNAND-TM program, RNAND-RAM program, or a randomized Turing machine (or any programming language that includes a coin tossing operation). Since all these models are equivalent up to polynomial factors, you can use your favorite model to capture polynomial-time randomized algorithms without any loss in generality.</p>
<div id="choosingfromsetex" class="solvedexercise" title="Choosing from a set" name="Solvedexercise 19.1 (Choosing from a set) ">
<p>Modern programming languages often involve not just the ability to toss a random coin in <span><span class="math inline">\(\{0,1\}\)</span></span> but also to choose an element at random from a set <span><span class="math inline">\(S\)</span></span>. Show that you can emulate this primitive using coin tossing. Specifically, show that there is randomized algorithm <span><span class="math inline">\(A\)</span></span> that on input a set <span><span class="math inline">\(S\)</span></span> of <span><span class="math inline">\(m\)</span></span> strings of length <span><span class="math inline">\(n\)</span></span>, runs in time <span><span class="math inline">\(poly(n,m)\)</span></span> and outputs either an element <span><span class="math inline">\(x\in S\)</span></span> or “fail” such that</p>
<ol type="1">
<li><p>Let <span><span class="math inline">\(p\)</span></span> be the probability that <span><span class="math inline">\(A\)</span></span> outputs “fail”, then <span><span class="math inline">\(p &lt; 2^{-n}\)</span></span> (a number small enough that it can be ignored).</p></li>
<li><p>For every <span><span class="math inline">\(x \in S\)</span></span>, the probability that <span><span class="math inline">\(A\)</span></span> outputs <span><span class="math inline">\(x\)</span></span> is exactly <span><span class="math inline">\(\tfrac{1-p}{m}\)</span></span> (and so the output is uniform over <span><span class="math inline">\(S\)</span></span> if we ignore the tiny probability of failure)</p></li>
</ol>
</div>
<div class="solution" data-ref="choosingfromsetex" name="Solution 19.1">
<p>If the size of <span><span class="math inline">\(S\)</span></span> is a power of two, that is <span><span class="math inline">\(m=2^\ell\)</span></span> for some <span><span class="math inline">\(\ell\in N\)</span></span>, then we can choose a random element in <span><span class="math inline">\(S\)</span></span> by tossing <span><span class="math inline">\(\ell\)</span></span> coins to obtain a string <span><span class="math inline">\(w \in \{0,1\}^\ell\)</span></span> and then output the <span><span class="math inline">\(i\)</span></span>-th element of <span><span class="math inline">\(S\)</span></span> where <span><span class="math inline">\(i\)</span></span> is the number whose binary representation is <span><span class="math inline">\(w\)</span></span>.</p>
<p>If <span><span class="math inline">\(S\)</span></span> is not a power of two, then our first attempt will be to let <span><span class="math inline">\(\ell = \ceil{\log m}\)</span></span> and do the same, but then output the <span><span class="math inline">\(i\)</span></span>-th element of <span><span class="math inline">\(S\)</span></span> if <span><span class="math inline">\(i \in [m]\)</span></span> and output “fail” otherwise. Conditioned on not outputting “fail”, this element is distributed uniformly in <span><span class="math inline">\(S\)</span></span>. However, in the worst case, <span><span class="math inline">\(2^\ell\)</span></span> can be almost <span><span class="math inline">\(2m\)</span></span> and so the probability of fail might be close to half. To reduce the failure probability, we can repeat the experiment above <span><span class="math inline">\(n\)</span></span> times. Specifically, we will use the following algorithm</p>
<div  class="pseudocodeoutput">
<div class="ps-root">
<div class="ps-algorithm with-caption" id = samplefromsetalg>
<p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;">
<span class="ps-keyword">Algorithm 2 </span>Sample from set</p>
<div class="ps-algorithmic"><p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"><span class="ps-keyword">Input:</span>  Set \(S = \{ x_0,\ldots, x_{m-1} \}\) with \(x_i\in \{0,1\}^n\) for all \(i\in [m]\).<p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"><span class="ps-keyword">Output:</span>  Either \(x\in S\) or "fail"<p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"> Let \(\ell \leftarrow \lceil \log m \rceil\)<p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"><span class="ps-keyword">for</span>{\(j = 0,1,\ldots,n-1\)} 
                <div class="ps-block" style="margin-left:1.2em;">
                <p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;">    Pick \(w \sim \{0,1\}^\ell\)<p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;">    Let \(i\in [2^\ell]\) be number whose binary representation is \(w\).<p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;">   <span class="ps-keyword">if</span>{\(i&lt;m\)} 
                <div class="ps-block" style="margin-left:1.2em;">
                <p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;">     <span class="ps-keyword">return</span> \(x_i\)
</div><p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;">   <span class="ps-keyword">endif</span>
</div><p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"><span class="ps-keyword">endfor</span><p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"><span class="ps-keyword">return</span> "fail"<p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"></div>
</div>
</div>
</div>
<p>Conditioned on not failing, the output of <a href='#samplefromsetalg'>Algorithm 19.2</a> is uniformly distributed in <span><span class="math inline">\(S\)</span></span>. However, since <span><span class="math inline">\(2^\ell &lt; 2m\)</span></span>, the probability of failure in each iteration is less than <span><span class="math inline">\(1/2\)</span></span> and so the probability of failure in all of them is at most <span><span class="math inline">\((1/2)^{n}= 2^{-n}\)</span></span>.</p>
</div>
<h3 id="an-alternative-view-random-coins-as-an-extra-input" data-number="19.1.1">An alternative view: random coins as an “extra input”</h3>
<p>While we presented randomized computation as adding an extra “coin tossing” operation to our programs, we can also model this as being given an additional extra input. That is, we can think of a randomized algorithm <span><span class="math inline">\(A\)</span></span> as a <em>deterministic</em> algorithm <span><span class="math inline">\(A&#39;\)</span></span> that takes <em>two inputs</em> <span><span class="math inline">\(x\)</span></span> and <span><span class="math inline">\(r\)</span></span> where the second input <span><span class="math inline">\(r\)</span></span> is chosen at random from <span><span class="math inline">\(\{0,1\}^m\)</span></span> for some <span><span class="math inline">\(m\in \N\)</span></span> (see <a href='#randomalgsviewsfig'>Figure 19.2</a>). The equivalence to the <a href='#BPPdef'>Definition 19.1</a> is shown in the following theorem:</p>
<figure>
<img src="../figure/randomalgstwoviews.png" alt="19.2: The two equivalent views of randomized algorithms. We can think of such an algorithm as having access to an internal RAND() operation that outputs a random independent value in \{0,1\} whenever it is invoked, or we can think of it as a deterministic algorithm that in addition to the standard input x \in \{0,1\}^n obtains an additional auxiliary input r \in \{0,1\}^m that is chosen uniformly at random." id="randomalgsviewsfig" class="margin" /><figcaption>19.2: The two equivalent views of randomized algorithms. We can think of such an algorithm as having access to an internal <code>RAND()</code> operation that outputs a random independent value in <span><span class="math inline">\(\{0,1\}\)</span></span> whenever it is invoked, or we can think of it as a deterministic algorithm that in addition to the standard input <span><span class="math inline">\(x \in \{0,1\}^n\)</span></span> obtains an additional auxiliary input <span><span class="math inline">\(r \in \{0,1\}^m\)</span></span> that is chosen uniformly at random.</figcaption>
</figure>
<div id="randextrainput" class="theorem" title="Alternative characterization of $\mathbf{BPP}$" name="Theorem 19.3 (Alternative characterization of $\mathbf{BPP}$) ">
<p>Let <span><span class="math inline">\(F:\{0,1\}^* \rightarrow \{0,1\}\)</span></span>. Then <span><span class="math inline">\(F\in \mathbf{BPP}\)</span></span> if and only if there exists <span><span class="math inline">\(a,b\in \N\)</span></span> and <span><span class="math inline">\(G:\{0,1\}^* \rightarrow \{0,1\}\)</span></span> such that <span><span class="math inline">\(G\)</span></span> is in <span><span class="math inline">\(\mathbf{P}\)</span></span> and for every <span><span class="math inline">\(x\in \{0,1\}^*\)</span></span>, <span>
<div class='myequationbox'><span class="math display">\[
\Pr_{r\sim \{0,1\}^{a|x|^b}} [ G(xr)=F(x)] \geq \tfrac{2}{3}  \;\;(19.2)\;.
\]</span><a id='eqBPPauxiliary'></a></div></span></p>
</div>
<div id="section-1" class="proofidea" data-ref="randextrainput" name="Proofidea">
<p>The idea behind the proof is that, as illustrated in <a href='#randomalgsviewsfig'>Figure 19.2</a>, we can simply replace sampling a random coin with reading a bit from the extra “random input” <span><span class="math inline">\(r\)</span></span> and vice versa. To prove this rigorously we need to work through some slightly cumbersome formal notation. This might be one of those proofs that is easier to work out on your own than to read.</p>
</div>
<div class="proof" data-ref="randextrainput" name="Proof 19.1.1">
<p>We start by showing the “only if” direction. Let <span><span class="math inline">\(F\in \mathbf{BPP}\)</span></span> and let <span><span class="math inline">\(P\)</span></span> be an RNAND-TM program that computes <span><span class="math inline">\(F\)</span></span> as per <a href='#BPPdef'>Definition 19.1</a>, and let <span><span class="math inline">\(a,b\in \N\)</span></span> be such that on every input of length <span><span class="math inline">\(n\)</span></span>, the program <span><span class="math inline">\(P\)</span></span> halts within at most <span><span class="math inline">\(an^b\)</span></span> steps. We will construct a polynomial-time algorithm that <span><span class="math inline">\(P&#39;\)</span></span> such that for every <span><span class="math inline">\(x\in \{0,1\}^n\)</span></span>, if we set <span><span class="math inline">\(m=an^b\)</span></span>, then <span>
<div class='myequationbox'><span class="math display">\[
\Pr_{r \sim \{0,1\}^{m}}[ P&#39;(xr) = 1] = \Pr[ P(x) = 1 ] \;,
\]</span></div></span> where the probability in the righthand side is taken over the <code>RAND()</code> operations in <span><span class="math inline">\(P\)</span></span>. In particular this means that if we define <span><span class="math inline">\(G(xr) = P&#39;(xr)\)</span></span> then the function <span><span class="math inline">\(G\)</span></span> satisfies the conditions of <a href='#eqBPPauxiliary'>Equation 19.2</a>.</p>
<p>The algorithm <span><span class="math inline">\(P&#39;\)</span></span> will be very simple: it simulates the program <span><span class="math inline">\(P\)</span></span>, maintaining a counter <span><span class="math inline">\(i\)</span></span> initialized to <span><span class="math inline">\(0\)</span></span>. Every time that <span><span class="math inline">\(P\)</span></span> makes a <code>RAND()</code> operation, the program <span><span class="math inline">\(P&#39;\)</span></span> will supply the result from <span><span class="math inline">\(r_i\)</span></span> and increment <span><span class="math inline">\(i\)</span></span> by one. We will never “run out” of bits, since the running time of <span><span class="math inline">\(P\)</span></span> is at most <span><span class="math inline">\(an^b\)</span></span> and hence it can make at most this number of <code>RNAND()</code> calls. The output of <span><span class="math inline">\(P&#39;(xr)\)</span></span> for a random <span><span class="math inline">\(r\sim \{0,1\}^m\)</span></span> will be distributed identically to the output of <span><span class="math inline">\(P(x)\)</span></span>.</p>
<p>For the other direction, given a function <span><span class="math inline">\(G\in \mathbf{P}\)</span></span> satisfying the condition <a href='#eqBPPauxiliary'>Equation 19.2</a> and a NAND-TM <span><span class="math inline">\(P&#39;\)</span></span> that computes <span><span class="math inline">\(G\)</span></span> in polynomial time, we can construct an RNAND-TM program <span><span class="math inline">\(P\)</span></span> that computes <span><span class="math inline">\(F\)</span></span> in polynomial time. On input <span><span class="math inline">\(x\in \{0,1\}^n\)</span></span>, the program <span><span class="math inline">\(P\)</span></span> will simply use the <code>RNAND()</code> instruction <span><span class="math inline">\(an^b\)</span></span> times to fill an array <code>R[</code><span><span class="math inline">\(0\)</span></span><code>]</code> , <span><span class="math inline">\(\ldots\)</span></span>, <code>R[</code><span><span class="math inline">\(an^b-1\)</span></span><code>]</code> and then execute the original program <span><span class="math inline">\(P&#39;\)</span></span> on input <span><span class="math inline">\(xr\)</span></span> where <span><span class="math inline">\(r_i\)</span></span> is the <span><span class="math inline">\(i\)</span></span>-th element of the array <code>R</code>. Once again, it is clear that if <span><span class="math inline">\(P&#39;\)</span></span> runs in polynomial time then so will <span><span class="math inline">\(P\)</span></span>, and for every input <span><span class="math inline">\(x\)</span></span> and <span><span class="math inline">\(r\in \{0,1\}^{an^b}\)</span></span>, the output of <span><span class="math inline">\(P\)</span></span> on input <span><span class="math inline">\(x\)</span></span> and where the coin tosses outcome is <span><span class="math inline">\(r\)</span></span> is equal to <span><span class="math inline">\(P&#39;(xr)\)</span></span>.</p>
</div>
<div id="BPPandNP" class="remark" title="Definitions of $\mathbf{BPP}$ and $\mathbf{NP}$" name="Remark 19.4 (Definitions of $\mathbf{BPP}$ and $\mathbf{NP}$) ">
<p>The characterization of <span><span class="math inline">\(\mathbf{BPP}\)</span></span> <a href='#randextrainput'>Theorem 19.3</a> is reminiscent of the characterization of <span><span class="math inline">\(\mathbf{NP}\)</span></span> in <a href='lec_13_Cook_Levin.html#NP-def'>Definition 14.1</a>, with the randomness in the case of <span><span class="math inline">\(\mathbf{BPP}\)</span></span> playing the role of the solution in the case of <span><span class="math inline">\(\mathbf{NP}\)</span></span>. However, there are important differences between the two:</p>
<ul>
<li><p>The definition of <span><span class="math inline">\(\mathbf{NP}\)</span></span> is “one sided”: <span><span class="math inline">\(F(x)=1\)</span></span> if <em>there exists</em> a solution <span><span class="math inline">\(w\)</span></span> such that <span><span class="math inline">\(G(xw)=1\)</span></span> and <span><span class="math inline">\(F(x)=0\)</span></span> if <em>for every</em> string <span><span class="math inline">\(w\)</span></span> of the appropriate length, <span><span class="math inline">\(G(xw)=0\)</span></span>. In contrast, the characterization of <span><span class="math inline">\(\mathbf{BPP}\)</span></span> is symmetric with respect to the cases <span><span class="math inline">\(F(x)=0\)</span></span> and <span><span class="math inline">\(F(x)=1\)</span></span>.</p></li>
<li><p>The relation between <span><span class="math inline">\(\mathbf{NP}\)</span></span> and <span><span class="math inline">\(\mathbf{BPP}\)</span></span> is not immediately clear. It is not known whether <span><span class="math inline">\(\mathbf{BPP} \subseteq \mathbf{NP}\)</span></span>, <span><span class="math inline">\(\mathbf{NP} \subseteq \mathbf{BPP}\)</span></span>, or these two classes are incomparable. It is however known (with a non-trivial proof) that if <span><span class="math inline">\(\mathbf{P}=\mathbf{NP}\)</span></span> then <span><span class="math inline">\(\mathbf{BPP}=\mathbf{P}\)</span></span> (see <a href='#BPPvsNP'>Theorem 19.11</a>).</p></li>
<li><p>Most importantly, the definition of <span><span class="math inline">\(\mathbf{NP}\)</span></span> is “ineffective,” since it does not yield a way of actually finding whether there exists a solution among the exponentially many possibilities. By contrast, the definition of <span><span class="math inline">\(\mathbf{BPP}\)</span></span> gives us a way to compute the function in practice by simply choosing the second input at random.</p></li>
</ul>
</div>
<p><strong>“Random tapes”.</strong> <a href='#randextrainput'>Theorem 19.3</a> motivates sometimes considering the randomness of an RNAND-TM (or RNAND-RAM) program as an extra input. As such, if <span><span class="math inline">\(A\)</span></span> is a randomized algorithm that on inputs of length <span><span class="math inline">\(n\)</span></span> makes at most <span><span class="math inline">\(m\)</span></span> coin tosses, we will often use the notation <span><span class="math inline">\(A(x;r)\)</span></span> (where <span><span class="math inline">\(x\in \{0,1\}^n\)</span></span> and <span><span class="math inline">\(r\in \{0,1\}^{m}\)</span></span>) to refer to the result of executing <span><span class="math inline">\(x\)</span></span> when the coin tosses of <span><span class="math inline">\(A\)</span></span> correspond to the coordinates of <span><span class="math inline">\(r\)</span></span>. This second, or “auxiliary,” input is sometimes referred to as a “random tape.” This terminology originates from the model of randomized Turing machines.</p>
<h3 id="successamptwosided" data-number="19.1.2">Success amplification of two-sided error algorithms</h3>
<p>The number <span><span class="math inline">\(2/3\)</span></span> might seem arbitrary, but as we’ve seen in <a href='lec_16_randomized_alg.html#randomizedalgchap'>Chapter 18</a> it can be amplified to our liking:</p>
<div id="amplificationthm" class="theorem" title="Amplification" name="Theorem 19.5 (Amplification) ">
<p>Let <span><span class="math inline">\(F:\{0,1\}^* \rightarrow \{0,1\}\)</span></span> be a Boolean function such that there is a polynomial <span><span class="math inline">\(p:\N \rightarrow \N\)</span></span> and a polynomial-time randomized algorithm <span><span class="math inline">\(A\)</span></span> satisfying that for every <span><span class="math inline">\(x\in \{0,1\}^n\)</span></span>, <span>
<div class='myequationbox'><span class="math display">\[
\Pr[A(x) = F(x)] \geq \frac{1}{2} + \frac{1}{p(n)} \;\;(19.4) \;.
\]</span><a id='eqbppampassumption'></a></div></span> Then for every polynomial <span><span class="math inline">\(q:\N \rightarrow \N\)</span></span> there is a polynomial-time randomized algorithm <span><span class="math inline">\(B\)</span></span> satisfying for every <span><span class="math inline">\(x\in \{0,1\}^n\)</span></span>, <span>
<div class='myequationbox'><span class="math display">\[
\Pr[B(x) = F(x)] \geq  1 - 2^{-q(n)} \;.
\]</span></div></span></p>
</div>
<div id="amplificationidea" class="bigidea" name="Bigidea 24">
<p>We can <em>amplify</em> the success of randomized algorithms to a value that is arbitrarily close to <span><span class="math inline">\(1\)</span></span>.</p>
</div>
<div id="section-2" class="proofidea" data-ref="amplificationthm" name="Proofidea">
<p>The proof is the same as we’ve seen before in the case of maximum cut and other examples. We use the Chernoff bound to argue that if <span><span class="math inline">\(A\)</span></span> computes <span><span class="math inline">\(F\)</span></span> with probability at least <span><span class="math inline">\(\tfrac{1}{2} + \epsilon\)</span></span> and we run it <span><span class="math inline">\(O(k/\epsilon^2)\)</span></span> times, each time using fresh and independent random coins, then the probability that the majority of the answers will not be correct will be less than <span><span class="math inline">\(2^{-k}\)</span></span>. Amplification can be thought of as a “polling” of the choices for randomness for the algorithm (see <a href='#amplificationfig'>Figure 19.3</a>).</p>
</div>
<div class="proof" data-ref="amplificationthm" name="Proof 19.1.2">
<p>Let <span><span class="math inline">\(A\)</span></span> be an algorithm satisfying <a href='#eqbppampassumption'>Equation 19.4</a>. Set <span><span class="math inline">\(\epsilon = \tfrac{1}{p(n)}\)</span></span> and <span><span class="math inline">\(k = q(n)\)</span></span> where <span><span class="math inline">\(p,q\)</span></span> are the polynomials in the theorem statement. We can run <span><span class="math inline">\(P\)</span></span> on input <span><span class="math inline">\(x\)</span></span> for <span><span class="math inline">\(t=10k/\epsilon^2\)</span></span> times, using fresh randomness in each execution, and compute the outputs <span><span class="math inline">\(y_0,\ldots,y_{t-1}\)</span></span>. We output the value <span><span class="math inline">\(y\)</span></span> that appeared the largest number of times. Let <span><span class="math inline">\(X_i\)</span></span> be the random variable that is equal to <span><span class="math inline">\(1\)</span></span> if <span><span class="math inline">\(y_i = F(x)\)</span></span> and equal to <span><span class="math inline">\(0\)</span></span> otherwise. The random variables <span><span class="math inline">\(X_0,\ldots,X_{t-1}\)</span></span> are i.i.d. and satisfy <span><span class="math inline">\(\E [X_i] = \Pr[ X_i = 1] \geq 1/2 + \epsilon\)</span></span>, and hence by linearity of expectation <span><span class="math inline">\(\mathbb{E}[\sum_{i=0}^{t-1} X_i] \geq t(1/2 + \epsilon)\)</span></span>. For the plurality value to be <em>incorrect</em>, it must hold that <span><span class="math inline">\(\sum_{i=0}^{t-1} X_i \leq t/2\)</span></span>, which means that <span><span class="math inline">\(\sum_{i=0}^{t-1}X_i\)</span></span> is at least <span><span class="math inline">\(\epsilon t\)</span></span> far from its expectation. Hence by the Chernoff bound (<a href='lec_15_probability.html#chernoffthm'>Theorem 17.12</a>), the probability that the plurality value is not correct is at most <span><span class="math inline">\(2e^{-\epsilon^2 t}\)</span></span>, which is smaller than <span><span class="math inline">\(2^{-k}\)</span></span> for our choice of <span><span class="math inline">\(t\)</span></span>.</p>
</div>
<figure>
<img src="../figure/BPPamplification.png" alt="19.3: If F\in\mathbf{BPP} then there is randomized polynomial-time algorithm P with the following property: In the case F(x)=0 two thirds of the “population” of random choices satisfy P(x;r)=0 and in the case F(x)=1 two thirds of the population satisfy P(x;r)=1. We can think of amplification as a form of “polling” of the choices of randomness. By the Chernoff bound, if we poll a sample of O(\tfrac{\log(1/\delta)}{\epsilon^2}) random choices r, then with probability at least 1-\delta, the fraction of r’s in the sample satisfying P(x;r)=1 will give us an estimate of the fraction of the population within an \epsilon margin of error. This is the same calculation used by pollsters to determine the needed sample size in their polls." id="amplificationfig" class="margin" /><figcaption>19.3: If <span><span class="math inline">\(F\in\mathbf{BPP}\)</span></span> then there is randomized polynomial-time algorithm <span><span class="math inline">\(P\)</span></span> with the following property: In the case <span><span class="math inline">\(F(x)=0\)</span></span> two thirds of the “population” of random choices satisfy <span><span class="math inline">\(P(x;r)=0\)</span></span> and in the case <span><span class="math inline">\(F(x)=1\)</span></span> two thirds of the population satisfy <span><span class="math inline">\(P(x;r)=1\)</span></span>. We can think of amplification as a form of “polling” of the choices of randomness. By the Chernoff bound, if we poll a sample of <span><span class="math inline">\(O(\tfrac{\log(1/\delta)}{\epsilon^2})\)</span></span> random choices <span><span class="math inline">\(r\)</span></span>, then with probability at least <span><span class="math inline">\(1-\delta\)</span></span>, the fraction of <span><span class="math inline">\(r\)</span></span>’s in the sample satisfying <span><span class="math inline">\(P(x;r)=1\)</span></span> will give us an estimate of the fraction of the population within an <span><span class="math inline">\(\epsilon\)</span></span> margin of error. This is the same calculation used by pollsters to determine the needed sample size in their polls.</figcaption>
</figure>
<h2 id="mathbfbpp-and-mathbfnp-completeness" data-number="19.2"><span><span class="math inline">\(\mathbf{BPP}\)</span></span> and <span><span class="math inline">\(\mathbf{NP}\)</span></span> completeness</h2>
<p>Since “noisy processes” abound in nature, randomized algorithms can be realized physically, and so it is reasonable to propose <span><span class="math inline">\(\mathbf{BPP}\)</span></span> rather than <span><span class="math inline">\(\mathbf{P}\)</span></span> as our mathematical model for “feasible” or “tractable” computation. One might wonder if this makes all the previous chapters irrelevant, and in particular if the theory of <span><span class="math inline">\(\mathbf{NP}\)</span></span> completeness still applies to probabilistic algorithms. Fortunately, the answer is <em>Yes</em>:</p>
<div id="NPCandBPP" class="theorem" title="NP hardness and BPP" name="Theorem 19.6 (NP hardness and BPP) ">
<p>Suppose that <span><span class="math inline">\(F\)</span></span> is <span><span class="math inline">\(\mathbf{NP}\)</span></span>-hard and <span><span class="math inline">\(F\in \mathbf{BPP}\)</span></span>. Then <span><span class="math inline">\(\mathbf{NP} \subseteq \mathbf{BPP}\)</span></span>.</p>
</div>
<p>Before seeing the proof, note that <a href='#NPCandBPP'>Theorem 19.6</a> implies that if there was a randomized polynomial time algorithm for any <span><span class="math inline">\(\mathbf{NP}\)</span></span>-complete problem such as <span><span class="math inline">\(3\ensuremath{\mathit{SAT}}\)</span></span>, <span><span class="math inline">\(\ensuremath{\mathit{ISET}}\)</span></span> etc., then there would be such an algorithm for <em>every</em> problem in <span><span class="math inline">\(\mathbf{NP}\)</span></span>. Thus, regardless of whether our model of computation is deterministic or randomized algorithms, <span><span class="math inline">\(\mathbf{NP}\)</span></span> complete problems retain their status as the “hardest problems in <span><span class="math inline">\(\mathbf{NP}\)</span></span>.”</p>
<div id="section-3" class="proofidea" data-ref="NPCandBPP" name="Proofidea">
<p>The idea is to simply run the reduction as usual, and plug it into the randomized algorithm instead of a deterministic one. It would be an excellent exercise, and a way to reinforce the definitions of <span><span class="math inline">\(\mathbf{NP}\)</span></span>-hardness and randomized algorithms, for you to work out the proof for yourself. However for the sake of completeness, we include this proof below.</p>
</div>
<div id="section-4" class="proof" data-ref="NPCandBPP" name="Proof">
<p>Suppose that <span><span class="math inline">\(F\)</span></span> is <span><span class="math inline">\(\mathbf{NP}\)</span></span>-hard and <span><span class="math inline">\(F\in \mathbf{BPP}\)</span></span>. We will now show that this implies that <span><span class="math inline">\(\mathbf{NP} \subseteq \mathbf{BPP}\)</span></span>. Let <span><span class="math inline">\(G \in \mathbf{NP}\)</span></span>. By the definition of <span><span class="math inline">\(\mathbf{NP}\)</span></span>-hardness, it follows that <span><span class="math inline">\(G \leq_p F\)</span></span>, or that in other words there exists a polynomial-time computable function <span><span class="math inline">\(R:\{0,1\}^* \rightarrow \{0,1\}^*\)</span></span> such that <span><span class="math inline">\(G(x)=F(R(x))\)</span></span> for every <span><span class="math inline">\(x\in \{0,1\}^*\)</span></span>. Now if <span><span class="math inline">\(F\)</span></span> is in <span><span class="math inline">\(\mathbf{BPP}\)</span></span> then there is a polynomial-time RNAND-TM program <span><span class="math inline">\(P\)</span></span> such that <span>
<div class='myequationbox'><span class="math display">\[
\Pr[ P(y)= F(y) ] \geq 2/3 \;\;(19.6)
\]</span><a id='FinBPPeq'></a></div></span> for <em>every</em> <span><span class="math inline">\(y\in \{0,1\}^*\)</span></span> (where the probability is taken over the random coin tosses of <span><span class="math inline">\(P\)</span></span>). Hence we can get a polynomial-time RNAND-TM program <span><span class="math inline">\(P&#39;\)</span></span> to compute <span><span class="math inline">\(G\)</span></span> by setting <span><span class="math inline">\(P&#39;(x)=P(R(x))\)</span></span>. By <a href='#FinBPPeq'>Equation 19.6</a> <span><span class="math inline">\(\Pr[ P&#39;(x) = F(R(x))] \geq 2/3\)</span></span> and since <span><span class="math inline">\(F(R(x))=G(x)\)</span></span> this implies that <span><span class="math inline">\(\Pr[ P&#39;(x) = G(x)] \geq 2/3\)</span></span>, which proves that <span><span class="math inline">\(G \in \mathbf{BPP}\)</span></span>.</p>
</div>
<p>Most of the results we’ve seen about <span><span class="math inline">\(\mathbf{NP}\)</span></span> hardness, including the search to decision reduction of <a href='lec_14_PvsNP.html#search-dec-thm'>Theorem 15.1</a>, the decision to optimization reduction of <a href='lec_14_PvsNP.html#optimizationnp'>Theorem 15.3</a>, and the quantifier elimination result of <a href='lec_14_PvsNP.html#PH-collapse-thm'>Theorem 15.6</a>, all carry over in the same way if we replace <span><span class="math inline">\(\mathbf{P}\)</span></span> with <span><span class="math inline">\(\mathbf{BPP}\)</span></span> as our model of efficient computation. Thus if <span><span class="math inline">\(\mathbf{NP} \subseteq \mathbf{BPP}\)</span></span> then we get essentially all of the strange and wonderful consequences of <span><span class="math inline">\(\mathbf{P}=\mathbf{NP}\)</span></span>. Unsurprisingly, we cannot rule out this possibility. In fact, unlike <span><span class="math inline">\(\mathbf{P}=\mathbf{EXP}\)</span></span>, which is ruled out by the time hierarchy theorem, we don’t even know how to rule out the possibility that <span><span class="math inline">\(\mathbf{BPP}=\mathbf{EXP}\)</span></span>! Thus a priori it’s possible (though seems highly unlikely) that randomness is a magical tool that allows us to speed up arbitrary exponential time computation.<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup> Nevertheless, as we discuss below, it is believed that randomization’s power is much weaker and <span><span class="math inline">\(\mathbf{BPP}\)</span></span> lies in much more “pedestrian” territory.</p>
<h2 id="the-power-of-randomization" data-number="19.3">The power of randomization</h2>
<p>A major question is whether randomization can add power to computation. Mathematically, we can phrase this as the following question: does <span><span class="math inline">\(\mathbf{BPP}=\mathbf{P}\)</span></span>? Given what we’ve seen so far about the relations of other complexity classes such as <span><span class="math inline">\(\mathbf{P}\)</span></span> and <span><span class="math inline">\(\mathbf{NP}\)</span></span>, or <span><span class="math inline">\(\mathbf{NP}\)</span></span> and <span><span class="math inline">\(\mathbf{EXP}\)</span></span>, one might guess that:</p>
<ol type="1">
<li><p>We do not know the answer to this question.</p></li>
<li><p>But we suspect that <span><span class="math inline">\(\mathbf{BPP}\)</span></span> is different than <span><span class="math inline">\(\mathbf{P}\)</span></span>.</p></li>
</ol>
<p>One would be correct about the former, but wrong about the latter. As we will see, we do in fact have reasons to believe that <span><span class="math inline">\(\mathbf{BPP}=\mathbf{P}\)</span></span>. This can be thought of as supporting the <em>extended Church Turing hypothesis</em> that deterministic polynomial-time Turing machines capture what can be feasibly computed in the physical world.</p>
<p>We now survey some of the relations that are known between <span><span class="math inline">\(\mathbf{BPP}\)</span></span> and other complexity classes we have encountered. (See also <a href='#BPPscenariosfig'>Figure 19.4</a>.)</p>
<figure>
<img src="../figure/BPPscenarios.png" alt="19.4: Some possibilities for the relations between \mathbf{BPP} and other complexity classes. Most researchers believe that \mathbf{BPP}=\mathbf{P} and that these classes are not powerful enough to solve \mathbf{NP}-complete problems, let alone all problems in \mathbf{EXP}. However, we have not even been able yet to rule out the possibility that randomness is a “silver bullet” that allows exponential speedup on all problems, and hence \mathbf{BPP}=\mathbf{EXP}. As we’ve already seen, we also can’t rule out that \mathbf{P}=\mathbf{NP}. Interestingly, in the latter case, \mathbf{P}=\mathbf{BPP}." id="BPPscenariosfig" class="margin" /><figcaption>19.4: Some possibilities for the relations between <span><span class="math inline">\(\mathbf{BPP}\)</span></span> and other complexity classes. Most researchers believe that <span><span class="math inline">\(\mathbf{BPP}=\mathbf{P}\)</span></span> and that these classes are <em>not</em> powerful enough to solve <span><span class="math inline">\(\mathbf{NP}\)</span></span>-complete problems, let alone all problems in <span><span class="math inline">\(\mathbf{EXP}\)</span></span>. However, we have not even been able yet to rule out the possibility that randomness is a “silver bullet” that allows exponential speedup on all problems, and hence <span><span class="math inline">\(\mathbf{BPP}=\mathbf{EXP}\)</span></span>. As we’ve already seen, we also can’t rule out that <span><span class="math inline">\(\mathbf{P}=\mathbf{NP}\)</span></span>. Interestingly, in the latter case, <span><span class="math inline">\(\mathbf{P}=\mathbf{BPP}\)</span></span>.</figcaption>
</figure>
<h3 id="solving-mathbfbpp-in-exponential-time" data-number="19.3.1">Solving <span><span class="math inline">\(\mathbf{BPP}\)</span></span> in exponential time</h3>
<p>It is not hard to see that if <span><span class="math inline">\(F\)</span></span> is in <span><span class="math inline">\(\mathbf{BPP}\)</span></span> then it can be computed in <em>exponential</em> time.</p>
<div id="BPPEXP" class="theorem" title="Simulating randomized algorithms in exponential time" name="Theorem 19.7 (Simulating randomized algorithms in exponential time) ">
<p><span><span class="math inline">\(\mathbf{BPP} \subseteq \mathbf{EXP}\)</span></span></p>
</div>
<div id="section-5" class="pause" name="Pause">
<p>The proof of <a href='#BPPEXP'>Theorem 19.7</a> readily follows by enumerating over all the (exponentially many) choices for the random coins. We omit the formal proof, as doing it by yourself is an excellent way to get comfortable with <a href='#BPPdef'>Definition 19.1</a>.</p>
</div>
<h3 id="simulating-randomized-algorithms-by-circuits" data-number="19.3.2">Simulating randomized algorithms by circuits</h3>
<p>We have seen in <a href='lec_11_running_time.html#non-uniform-thm'>Theorem 12.12</a> that if <span><span class="math inline">\(F\)</span></span> is in <span><span class="math inline">\(\mathbf{P}\)</span></span>, then there is a polynomial <span><span class="math inline">\(p:\N \rightarrow \N\)</span></span> such that for every <span><span class="math inline">\(n\)</span></span>, the restriction <span><span class="math inline">\(F_{\upharpoonright n}\)</span></span> of <span><span class="math inline">\(F\)</span></span> to inputs <span><span class="math inline">\(\{0,1\}^n\)</span></span> is in <span><span class="math inline">\(\ensuremath{\mathit{SIZE}}(p(n))\)</span></span>. (In other words, that <span><span class="math inline">\(\mathbf{P} \subseteq \mathbf{P_{/poly}}\)</span></span>.) A priori it is not at all clear that the same holds for a function in <span><span class="math inline">\(\mathbf{BPP}\)</span></span>, but this does turn out to be the case.</p>
<figure>
<img src="../figure/randomizedcomp.png" alt="19.5: The possible guarantees for a randomized algorithm A computing some function F. In the tables above, the columns correspond to different inputs and the rows to different choices of the random tape. A cell at position r,x is colored green if A(x;r)=F(x) (i.e., the algorithm outputs the correct answer) and red otherwise. The standard \mathbf{BPP} guarantee corresponds to the middle figure, where for every input x, at least two thirds of the choices r for a random tape will result in A computing the correct value. That is, every column is colored green in at least two thirds of its coordinates. In the left figure we have an “average case” guarantee where the algorithm is only guaranteed to output the correct answer with probability two thirds over a random input (i.e., at most one third of the total entries of the table are colored red, but there could be an all red column). The right figure corresponds to the “offline \mathbf{BPP}” case, with probability at least two thirds over the random choice r, r will be good for every input. That is, at least two thirds of the rows are all green.  (\mathbf{BPP} \subseteq \mathbf{P_{/poly}}) is proven by amplifying the success of a \mathbf{BPP} algorithm until we have the “offline \mathbf{BPP}” guarantee, and then hardwiring the choice of the randomness r to obtain a nonuniform deterministic algorithm." id="randomizedcompfig" class="margin" /><figcaption>19.5: The possible guarantees for a randomized algorithm <span><span class="math inline">\(A\)</span></span> computing some function <span><span class="math inline">\(F\)</span></span>. In the tables above, the columns correspond to different inputs and the rows to different choices of the random tape. A cell at position <span><span class="math inline">\(r,x\)</span></span> is colored green if <span><span class="math inline">\(A(x;r)=F(x)\)</span></span> (i.e., the algorithm outputs the correct answer) and red otherwise. The standard <span><span class="math inline">\(\mathbf{BPP}\)</span></span> guarantee corresponds to the middle figure, where for every input <span><span class="math inline">\(x\)</span></span>, at least two thirds of the choices <span><span class="math inline">\(r\)</span></span> for a random tape will result in <span><span class="math inline">\(A\)</span></span> computing the correct value. That is, every column is colored green in at least two thirds of its coordinates. In the left figure we have an “average case” guarantee where the algorithm is only guaranteed to output the correct answer with probability two thirds over a <em>random</em> input (i.e., at most one third of the total entries of the table are colored red, but there could be an all red column). The right figure corresponds to the “offline <span><span class="math inline">\(\mathbf{BPP}\)</span></span>” case, with probability at least two thirds over the random choice <span><span class="math inline">\(r\)</span></span>, <span><span class="math inline">\(r\)</span></span> will be good for <em>every</em> input. That is, at least two thirds of the rows are all green. <a href='#rnandthm'>Theorem 19.8</a> (<span><span class="math inline">\(\mathbf{BPP} \subseteq \mathbf{P_{/poly}}\)</span></span>) is proven by amplifying the success of a <span><span class="math inline">\(\mathbf{BPP}\)</span></span> algorithm until we have the “offline <span><span class="math inline">\(\mathbf{BPP}\)</span></span>” guarantee, and then hardwiring the choice of the randomness <span><span class="math inline">\(r\)</span></span> to obtain a nonuniform deterministic algorithm.</figcaption>
</figure>
<div id="rnandthm" class="theorem" title="Randomness does not help for non uniform computation" name="Theorem 19.8 (Randomness does not help for non uniform computation) ">
<p><span><span class="math inline">\(\mathbf{BPP} \subseteq \mathbf{P_{/poly}}\)</span></span>.</p>
</div>
<p>That is, for every <span><span class="math inline">\(F\in \mathbf{BPP}\)</span></span>, there exist some <span><span class="math inline">\(a,b\in \N\)</span></span> such that for every <span><span class="math inline">\(n&gt;0\)</span></span>, <span><span class="math inline">\(F_{\upharpoonright n} \in \ensuremath{\mathit{SIZE}}(an^b)\)</span></span> where <span><span class="math inline">\(F_{\upharpoonright n}\)</span></span> is the restriction of <span><span class="math inline">\(F\)</span></span> to inputs in <span><span class="math inline">\(\{0,1\}^n\)</span></span>.</p>
<div id="section-6" class="proofidea" data-ref="rnandthm" name="Proofidea">
<p>The idea behind the proof is that we can first amplify by repetition the probability of success from <span><span class="math inline">\(2/3\)</span></span> to <span><span class="math inline">\(1-0.1 \cdot 2^{-n}\)</span></span>. This will allow us to show that for every <span><span class="math inline">\(n\in\N\)</span></span> there exists a <em>single fixed choice</em> of “favorable coins” which is a string <span><span class="math inline">\(r\)</span></span> of length polynomial in <span><span class="math inline">\(n\)</span></span> such that if <span><span class="math inline">\(r\)</span></span> is used for the randomness then we output the right answer on <em>all</em> of the possible <span><span class="math inline">\(2^n\)</span></span> inputs. We can then use the standard “unravelling the loop” technique to transform an RNAND-TM program to an RNAND-CIRC program, and “hardwire” the favorable choice of random coins to transform the RNAND-CIRC program into a plain old deterministic NAND-CIRC program.</p>
</div>
<div class="proof" data-ref="rnandthm" name="Proof 19.3.2">
<p>Suppose that <span><span class="math inline">\(F\in \mathbf{BPP}\)</span></span>. Let <span><span class="math inline">\(P\)</span></span> be a polynomial-time RNAND-TM program that computes <span><span class="math inline">\(F\)</span></span> as per <a href='#BPPdef'>Definition 19.1</a>. Using <a href='#amplificationthm'>Theorem 19.5</a>, we can <em>amplify</em> the success probability of <span><span class="math inline">\(P\)</span></span> to obtain an RNAND-TM program <span><span class="math inline">\(P&#39;\)</span></span> that is at most a factor of <span><span class="math inline">\(O(n)\)</span></span> slower (and hence still polynomial time) such that for every <span><span class="math inline">\(x\in \{0,1\}^n\)</span></span></p>
<p><span>
<div class='myequationbox'><span class="math display">\[
\Pr_{r \sim \{0,1\}^m}[ P&#39;(x;r)=F(x)] \geq 1 - 0.1\cdot 2^{-n} \;, \;\;(19.7)
\]</span><a id='ampeq'></a></div></span></p>
<p>where <span><span class="math inline">\(m\)</span></span> is the number of coin tosses that <span><span class="math inline">\(P&#39;\)</span></span> uses on inputs of length <span><span class="math inline">\(n\)</span></span>. We use the notation <span><span class="math inline">\(P&#39;(x;r)\)</span></span> to denote the execution of <span><span class="math inline">\(P&#39;\)</span></span> on input <span><span class="math inline">\(x\)</span></span> and when the result of the coin tosses corresponds to the string <span><span class="math inline">\(r\)</span></span>.</p>
<p>For every <span><span class="math inline">\(x\in \{0,1\}^n\)</span></span>, define the “bad” event <span><span class="math inline">\(B_x\)</span></span> to hold if <span><span class="math inline">\(P&#39;(x) \neq F(x)\)</span></span>, where the sample space for this event consists of the coins of <span><span class="math inline">\(P&#39;\)</span></span>. Then by <a href='#ampeq'>Equation 19.7</a>, <span><span class="math inline">\(\Pr[B_x] \leq 0.1\cdot 2^{-n}\)</span></span> for every <span><span class="math inline">\(x \in \{0,1\}^n\)</span></span>. Since there are <span><span class="math inline">\(2^n\)</span></span> many such <span><span class="math inline">\(x\)</span></span>’s, by the union bound we see that the probability that the <em>union</em> of the events <span><span class="math inline">\(\{ B_x \}_{x\in \{0,1\}^n}\)</span></span> is at most <span><span class="math inline">\(0.1\)</span></span>. This means that if we choose <span><span class="math inline">\(r \sim \{0,1\}^m\)</span></span>, then with probability at least <span><span class="math inline">\(0.9\)</span></span> it will be the case that for <em>every</em> <span><span class="math inline">\(x\in \{0,1\}^n\)</span></span>, <span><span class="math inline">\(F(x)=P&#39;(x;r)\)</span></span>. (Indeed, otherwise the event <span><span class="math inline">\(B_x\)</span></span> would hold for some <span><span class="math inline">\(x\)</span></span>.) In particular, because of the mere fact that the the probability of <span><span class="math inline">\(\cup_{x \in \{0,1\}^n} B_x\)</span></span> is smaller than <span><span class="math inline">\(1\)</span></span>, this means that <em>there exists</em> a particular <span><span class="math inline">\(r^* \in \{0,1\}^m\)</span></span> such that</p>
<p><span>
<div class='myequationbox'><span class="math display">\[P&#39;(x;r^*)=F(x) \;\;(19.8)
\]</span><a id='hardwirecorrecteq'></a></div></span></p>
<p>for every <span><span class="math inline">\(x\in \{0,1\}^n\)</span></span>.</p>
<p>Now let us use the standard “unravelling the loop” the technique and transform <span><span class="math inline">\(P&#39;\)</span></span> into a NAND-CIRC program <span><span class="math inline">\(Q\)</span></span> of polynomial in <span><span class="math inline">\(n\)</span></span> size, such that <span><span class="math inline">\(Q(xr)=P&#39;(x;r)\)</span></span> for every <span><span class="math inline">\(x\in \{0,1\}^n\)</span></span> and <span><span class="math inline">\(r \in \{0,1\}^m\)</span></span>. Then by “hardwiring” the values <span><span class="math inline">\(r^*_0,\ldots,r^*_{m-1}\)</span></span> in place of the last <span><span class="math inline">\(m\)</span></span> inputs of <span><span class="math inline">\(Q\)</span></span>, we obtain a new NAND-CIRC program <span><span class="math inline">\(Q_{r^*}\)</span></span> that satisfies by <a href='#hardwirecorrecteq'>Equation 19.8</a> that <span><span class="math inline">\(Q_{r^*}(x)=F(x)\)</span></span> for every <span><span class="math inline">\(x\in \{0,1\}^n\)</span></span>. This demonstrates that <span><span class="math inline">\(F_{\upharpoonright n}\)</span></span> has a polynomial sized NAND-CIRC program, hence completing the proof of <a href='#rnandthm'>Theorem 19.8</a>.</p>
</div>
<h2 id="derandomization" data-number="19.4">Derandomization</h2>
<p>The proof of <a href='#rnandthm'>Theorem 19.8</a> can be summarized as follows: we can replace a <span><span class="math inline">\(poly(n)\)</span></span>-time algorithm that tosses coins as it runs with an algorithm that uses a single set of coin tosses <span><span class="math inline">\(r^* \in \{0,1\}^{poly(n)}\)</span></span> which will be good enough for all inputs of size <span><span class="math inline">\(n\)</span></span>. Another way to say it is that for the purposes of computing functions, we do not need “online” access to random coins and can generate a set of coins “offline” ahead of time, before we see the actual input.</p>
<p>But this does not really help us with answering the question of whether <span><span class="math inline">\(\mathbf{BPP}\)</span></span> equals <span><span class="math inline">\(\mathbf{P}\)</span></span>, since we still need to find a way to generate these “offline” coins in the first place. To derandomize an RNAND-TM program we will need to come up with a <em>single</em> deterministic algorithm that will work for <em>all input lengths</em>. That is, unlike in the case of RNAND-CIRC programs, we cannot choose for every input length <span><span class="math inline">\(n\)</span></span> some string <span><span class="math inline">\(r^* \in \{0,1\}^{poly(n)}\)</span></span> to use as our random coins.</p>
<p>Can we derandomize randomized algorithms, or does randomness add an inherent extra power for computation? This is a fundamentally interesting question but is also of practical significance. Ever since people started to use randomized algorithms during the Manhattan project, they have been trying to remove the need for randomness and replace it with numbers that are selected through some deterministic process. Throughout the years this approach has often been used successfully, though there have been a number of failures as well.<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup></p>
<p>A common approach people used over the years was to replace the random coins of the algorithm by a “randomish looking” string that they generated through some arithmetic progress. For example, one can use the digits of <span><span class="math inline">\(\pi\)</span></span> for the random tape. Using these type of methods corresponds to what von Neumann referred to as a “state of sin”. (Though this is a sin that he himself frequently committed, as generating true randomness in sufficient quantity was and still is often too expensive.) The reason that this is considered a “sin” is that such a procedure will not work in general. For example, it is easy to modify any probabilistic algorithm <span><span class="math inline">\(A\)</span></span> such as the ones we have seen in <a href='lec_16_randomized_alg.html#randomizedalgchap'>Chapter 18</a>, to an algorithm <span><span class="math inline">\(A&#39;\)</span></span> that is <em>guaranteed to fail</em> if the random tape happens to equal the digits of <span><span class="math inline">\(\pi\)</span></span>. This means that the procedure “replace the random tape by the digits of <span><span class="math inline">\(\pi\)</span></span>” does not yield a <em>general</em> way to transform a probabilistic algorithm to a deterministic one that will solve the same problem. Of course, this procedure does not <em>always</em> fail, but we have no good way to determine when it fails and when it succeeds. This reasoning is not specific to <span><span class="math inline">\(\pi\)</span></span> and holds for every deterministically produced string, whether it obtained by <span><span class="math inline">\(\pi\)</span></span>, <span><span class="math inline">\(e\)</span></span>, the Fibonacci series, or anything else.</p>
<!---


> ### {.lemma title="Can't replace tape deterministically" #nodet}
Let $G:\{0,1\}^* \rightarrow \{0,1\}^m$
There is a linear time probabilistic algorithm $A$ such that for every $x\in \{0,1\}^*$, $\Pr[A(x)=1]< 1/10$ but
for every $n>10$ and fixed string $r\in \{0,1\}^n$, there is some $x\in \{0,1\}^n$ such that $A(x;r)=1$ where $A(x;r)$ denotes the execution of $A$ on input $x$ and where the randomness is supplied from $r$.

> ### {.proof data-ref="nodet"}
The algorithm $A$ is very simple. On input $x$ of length $n$, it tosses $n$ random coins $r_1,\ldots,r_n$ and outputs $1$ if and only if $x_0=r_0$, $x_1=r_1$, $\ldots$, $x_{9}=r_{9}$ (if $n<10$ then $A$ always outputs $0$).
Clearly $A$ runs in $O(n)$ steps and for every $x\in \{0,1\}^*$, $\Pr[ A(x)=1] \leq 2^{-10} < 0.1$.
However, by definition, for every fixed string $r$ of length at least $10$, $A(r;r)=1$.

The proof of [nodet](){.ref} might seem quite silly, but refers to a very serious issue.

--->
<p>An algorithm that checks if its random tape is equal to <span><span class="math inline">\(\pi\)</span></span> and then fails seems to be quite silly, but this is but the “tip of the iceberg” for a very serious issue. Time and again people have learned the hard way that one needs to be very careful about producing random bits using deterministic means. As we will see when we discuss cryptography, many spectacular security failures and break-ins were the result of using “insufficiently random” coins.</p>
<h3 id="pseudorandom-generators" data-number="19.4.1">Pseudorandom generators</h3>
<p>So, we can’t use any <em>single</em> string to “derandomize” a probabilistic algorithm. It turns out however, that we can use a <em>collection</em> of strings to do so. Another way to think about it is that rather than trying to <em>eliminate</em> the need for randomness, we start by focusing on <em>reducing</em> the amount of randomness needed. (Though we will see that if we reduce the randomness sufficiently, we can eventually get rid of it altogether.)</p>
<p>We make the following definition:</p>
<div id="prgdef" class="definition" title="Pseudorandom generator" name="Definition 19.9 (Pseudorandom generator) ">
<p>A function <span><span class="math inline">\(G:\{0,1\}^\ell \rightarrow \{0,1\}^m\)</span></span> is a <em><span><span class="math inline">\((T,\epsilon)\)</span></span>-pseudorandom generator</em> if for every circuit <span><span class="math inline">\(C\)</span></span> with <span><span class="math inline">\(m\)</span></span> inputs, one output, and at most <span><span class="math inline">\(T\)</span></span> gates, <span>
<div class='myequationbox'><span class="math display">\[
\left| \Pr_{s\sim \{0,1\}^\ell}[C(G(s))=1] - \Pr_{r \sim \{0,1\}^m}[C(r)=1] \right| &lt; \epsilon \;\;(19.9)
\]</span><a id='eq:prg'></a></div></span></p>
</div>
<figure>
<img src="../figure/prg_experiment.png" alt="19.6: A pseudorandom generator G maps a short string s\in \{0,1\}^\ell into a long string r\in \{0,1\}^m such that an small program/circuit P cannot distinguish between the case that it is provided a random input r \sim \{0,1\}^m and the case that it is provided a “pseudorandom” input of the form r=G(s) where s \sim \{0,1\}^\ell. The short string s is sometimes called the seed of the pseudorandom generator, as it is a small object that can be thought as yielding a large “tree of randomness”." id="pseudorandomgeneratorfig" class="margin" /><figcaption>19.6: A pseudorandom generator <span><span class="math inline">\(G\)</span></span> maps a short string <span><span class="math inline">\(s\in \{0,1\}^\ell\)</span></span> into a long string <span><span class="math inline">\(r\in \{0,1\}^m\)</span></span> such that an small program/circuit <span><span class="math inline">\(P\)</span></span> cannot distinguish between the case that it is provided a random input <span><span class="math inline">\(r \sim \{0,1\}^m\)</span></span> and the case that it is provided a “pseudorandom” input of the form <span><span class="math inline">\(r=G(s)\)</span></span> where <span><span class="math inline">\(s \sim \{0,1\}^\ell\)</span></span>. The short string <span><span class="math inline">\(s\)</span></span> is sometimes called the <em>seed</em> of the pseudorandom generator, as it is a small object that can be thought as yielding a large “tree of randomness”.</figcaption>
</figure>
<div class="pause" name="Pause 19.4.1">
<p>This is a definition that’s worth reading more than once, and spending some time to digest it. Note that it takes several parameters:</p>
<ul>
<li><p><span><span class="math inline">\(T\)</span></span> is the limit on the number of gates of the circuit <span><span class="math inline">\(C\)</span></span> that the generator needs to “fool”. The larger <span><span class="math inline">\(T\)</span></span> is, the stronger the generator.</p></li>
<li><p><span><span class="math inline">\(\epsilon\)</span></span> is how close is the output of the pseudorandom generator to the true uniform distribution over <span><span class="math inline">\(\{0,1\}^m\)</span></span>. The smaller <span><span class="math inline">\(\epsilon\)</span></span> is, the stronger the generator.</p></li>
<li><p><span><span class="math inline">\(\ell\)</span></span> is the input length and <span><span class="math inline">\(m\)</span></span> is the output length. If <span><span class="math inline">\(\ell \geq m\)</span></span> then it is trivial to come up with such a generator: on input <span><span class="math inline">\(s\in \{0,1\}^\ell\)</span></span>, we can output <span><span class="math inline">\(s_0,\ldots,s_{m-1}\)</span></span>. In this case <span><span class="math inline">\(\Pr_{s\sim \{0,1\}^\ell}[ P(G(s))=1]\)</span></span> will simply equal <span><span class="math inline">\(\Pr_{r\in \{0,1\}^m}[ P(r)=1]\)</span></span>, no matter how many lines <span><span class="math inline">\(P\)</span></span> has. So, the smaller <span><span class="math inline">\(\ell\)</span></span> is and the larger <span><span class="math inline">\(m\)</span></span> is, the stronger the generator, and to get anything non-trivial, we need <span><span class="math inline">\(m&gt;\ell\)</span></span>.</p></li>
</ul>
<p>Furthermore note that although our eventual goal is to fool probabilistic randomized algorithms that take an unbounded number of inputs, <a href='#prgdef'>Definition 19.9</a> refers to <em>finite</em> and <em>deterministic</em> NAND-CIRC programs.</p>
</div>
<p>We can think of a pseudorandom generator as a “randomness amplifier.” It takes an input <span><span class="math inline">\(s\)</span></span> of <span><span class="math inline">\(\ell\)</span></span> bits chosen at random and expands these <span><span class="math inline">\(\ell\)</span></span> bits into an output <span><span class="math inline">\(r\)</span></span> of <span><span class="math inline">\(m&gt;\ell\)</span></span> <em>pseudorandom</em> bits. If <span><span class="math inline">\(\epsilon\)</span></span> is small enough then the pseudorandom bits will “look random” to any NAND-CIRC program that is not too big. Still, there are two questions we haven’t answered:</p>
<ul>
<li><p><em>What reason do we have to believe that pseudorandom generators with non-trivial parameters exist?</em></p></li>
<li><p><em>Even if they do exist, why would such generators be useful to derandomize randomized algorithms?</em> After all, <a href='#prgdef'>Definition 19.9</a> does not involve RNAND-TM or RNAND-RAM programs, but rather deterministic NAND-CIRC programs with no randomness and no loops.</p></li>
</ul>
<p>We will now (partially) answer both questions. For the first question, let us come clean and confess we do not know how to <em>prove</em> that interesting pseudorandom generators exist. By <em>interesting</em> we mean pseudorandom generators that satisfy that <span><span class="math inline">\(\epsilon\)</span></span> is some small constant (say <span><span class="math inline">\(\epsilon&lt;1/3\)</span></span>), <span><span class="math inline">\(m&gt;\ell\)</span></span>, and the function <span><span class="math inline">\(G\)</span></span> itself can be computed in <span><span class="math inline">\(poly(m)\)</span></span> time. Nevertheless, <a href='#prgexist'>Lemma 19.12</a> (whose statement and proof is deferred to the end of this chapter) shows that if we only drop the last condition (polynomial-time computability), then there do in fact exist pseudorandom generators where <span><span class="math inline">\(m\)</span></span> is <em>exponentially larger</em> than <span><span class="math inline">\(\ell\)</span></span>.</p>
<div id="section-7" class="pause" name="Pause">
<p>At this point you might want to skip ahead and look at the <em>statement</em> of <a href='#prgexist'>Lemma 19.12</a>. However, since its <em>proof</em> is somewhat subtle, I recommend you defer reading it until you’ve finished reading the rest of this chapter.</p>
</div>
<h3 id="optimalprgconj" data-number="19.4.2">From existence to constructivity</h3>
<p>The fact that there <em>exists</em> a pseudorandom generator does not mean that there is one that can be efficiently computed. However, it turns out that we can turn complexity “on its head” and use the assumed <em>non existence</em> of fast algorithms for problems such as 3SAT to obtain pseudorandom generators that can then be used to transform randomized algorithms into deterministic ones. This is known as the <em>Hardness vs Randomness</em> paradigm. A number of results along those lines, most of which are outside the scope of this course, have led researchers to believe the following conjecture:</p>
<blockquote>
<div class="quote" name="Quote 19.4.2">
<p><strong>Optimal PRG conjecture:</strong> There is a polynomial-time computable function <span><span class="math inline">\(\ensuremath{\mathit{PRG}}:\{0,1\}^* \rightarrow \{0,1\}\)</span></span> that yields an <em>exponentially secure pseudorandom generator</em>.</p>
<p>Specifically, there exists a constant <span><span class="math inline">\(\delta &gt;0\)</span></span> such that for every <span><span class="math inline">\(\ell\)</span></span> and <span><span class="math inline">\(m &lt; 2^{\delta \ell}\)</span></span>, if we define <span><span class="math inline">\(G:\{0,1\}^\ell \rightarrow \{0,1\}^m\)</span></span> as <span><span class="math inline">\(G(s)_i = \ensuremath{\mathit{PRG}}(s,i)\)</span></span> for every <span><span class="math inline">\(s\in \{0,1\}^\ell\)</span></span> and <span><span class="math inline">\(i \in [m]\)</span></span>, then <span><span class="math inline">\(G\)</span></span> is a <span><span class="math inline">\((2^{\delta \ell},2^{-\delta \ell})\)</span></span> pseudorandom generator.</p>
</div>
</blockquote>
<div class="pause" name="Proofidea 19.4.3">
<p>The “optimal PRG conjecture” is worth while reading more than once. What it posits is that we can obtain <span><span class="math inline">\((T,\epsilon)\)</span></span> pseudorandom generator <span><span class="math inline">\(G\)</span></span> such that every output bit of <span><span class="math inline">\(G\)</span></span> can be computed in time polynomial in the length <span><span class="math inline">\(\ell\)</span></span> of the input, where <span><span class="math inline">\(T\)</span></span> is exponentially large in <span><span class="math inline">\(\ell\)</span></span> and <span><span class="math inline">\(\epsilon\)</span></span> is exponentially small in <span><span class="math inline">\(\ell\)</span></span>. (Note that we could not hope for the entire output to be computable in <span><span class="math inline">\(\ell\)</span></span>, as just writing the output down will take too long.)</p>
<p>To understand why we call such a pseudorandom generator “optimal,” it is a great exercise to convince yourself that, for example, there does not exist a <span><span class="math inline">\((2^{1.1\ell},2^{-1.1\ell})\)</span></span> pseudorandom generator (in fact, the number <span><span class="math inline">\(\delta\)</span></span> in the conjecture must be smaller than <span><span class="math inline">\(1\)</span></span>). To see that we can’t have <span><span class="math inline">\(T \gg 2^{\ell}\)</span></span>, note that if we allow a NAND-CIRC program with much more than <span><span class="math inline">\(2^\ell\)</span></span> lines then this NAND-CIRC program could “hardwire” inside it all the outputs of <span><span class="math inline">\(G\)</span></span> on all its <span><span class="math inline">\(2^\ell\)</span></span> inputs, and use that to distinguish between a string of the form <span><span class="math inline">\(G(s)\)</span></span> and a uniformly chosen string in <span><span class="math inline">\(\{0,1\}^m\)</span></span>. To see that we can’t have <span><span class="math inline">\(\epsilon \ll 2^{-\ell}\)</span></span>, note that by guessing the input <span><span class="math inline">\(s\)</span></span> (which will be successful with probability <span><span class="math inline">\(2^{-2\ell}\)</span></span>), we can obtain a small (i.e., <span><span class="math inline">\(O(\ell)\)</span></span> line) NAND-CIRC program that achieves a <span><span class="math inline">\(2^{-\ell}\)</span></span> advantage in distinguishing a pseudorandom and uniform input. Working out these details is a highly recommended exercise.</p>
</div>
<p>We emphasize again that the optimal PRG conjecture is, as its name implies, a <em>conjecture</em>, and we still do not know how to <em>prove</em> it. In particular, it is stronger than the conjecture that <span><span class="math inline">\(\mathbf{P} \neq \mathbf{NP}\)</span></span>. But we do have some evidence for its truth. There is a spectrum of different types of pseudorandom generators, and there are weaker assumptions than the optimal PRG conjecture that suffice to prove that <span><span class="math inline">\(\mathbf{BPP}=\mathbf{P}\)</span></span>. In particular this is known to hold under the assumption that there exists a function <span><span class="math inline">\(F\in \mathbf{TIME}(2^{O(n)})\)</span></span> and <span><span class="math inline">\(\epsilon &gt;0\)</span></span> such that for every sufficiently large <span><span class="math inline">\(n\)</span></span>, <span><span class="math inline">\(F_{\upharpoonright n}\)</span></span> is not in <span><span class="math inline">\(\ensuremath{\mathit{SIZE}}(2^{\epsilon n})\)</span></span>. The name “Optimal PRG conjecture” is non standard. This conjecture is sometimes known in the literature as the existence of <em>exponentially strong pseudorandom functions</em>.<sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup></p>
<h3 id="usefulness-of-pseudorandom-generators" data-number="19.4.3">Usefulness of pseudorandom generators</h3>
<p>We now show that optimal pseudorandom generators are indeed very useful, by proving the following theorem:</p>
<div id="derandBPPthm" class="theorem" title="Derandomization of BPP" name="Theorem 19.10 (Derandomization of BPP) ">
<p>Suppose that the optimal PRG conjecture is true. Then <span><span class="math inline">\(\mathbf{BPP}=\mathbf{P}\)</span></span>.</p>
</div>
<div id="section-8" class="proofidea" data-ref="derandBPPthm" name="Proofidea">
<p>The optimal PRG conjecture tells us that we can achieve <em>exponential expansion</em> of <span><span class="math inline">\(\ell\)</span></span> truly random coins into as many as <span><span class="math inline">\(2^{\delta \ell}\)</span></span> “pseudorandom coins.” Looked at from the other direction, it allows us to reduce the need for randomness by taking an algorithm that uses <span><span class="math inline">\(m\)</span></span> coins and converting it into an algorithm that only uses <span><span class="math inline">\(O(\log m)\)</span></span> coins. Now an algorithm of the latter type by can be made fully deterministic by enumerating over all the <span><span class="math inline">\(2^{O(\log m)}\)</span></span> (which is polynomial in <span><span class="math inline">\(m\)</span></span>) possibilities for its random choices.</p>
</div>
<p>We now proceed with the proof details.</p>
<div class="proof" data-ref="derandBPPthm" name="Proof 19.4.3">
<p>Let <span><span class="math inline">\(F\in \mathbf{BPP}\)</span></span> and let <span><span class="math inline">\(P\)</span></span> be a NAND-TM program and <span><span class="math inline">\(a,b,c,d\)</span></span> constants such that for every <span><span class="math inline">\(x\in \{0,1\}^n\)</span></span>, <span><span class="math inline">\(P(x)\)</span></span> runs in at most <span><span class="math inline">\(c \cdot n^d\)</span></span> steps and <span><span class="math inline">\(\Pr_{r\sim \{0,1\}^m}[ P(x;r) = F(x) ] \geq 2/3\)</span></span>. By “unrolling the loop” and hardwiring the input <span><span class="math inline">\(x\)</span></span>, we can obtain for every input <span><span class="math inline">\(x\in \{0,1\}^n\)</span></span> a NAND-CIRC program <span><span class="math inline">\(Q_x\)</span></span> of at most, say, <span><span class="math inline">\(T=10c \cdot n^d\)</span></span> lines, that takes <span><span class="math inline">\(m\)</span></span> bits of input and such that <span><span class="math inline">\(Q(r)=P(x;r)\)</span></span>.</p>
<p>Now suppose that <span><span class="math inline">\(G:\{0,1\}^\ell \rightarrow \{0,1\}\)</span></span> is a <span><span class="math inline">\((T,0.1)\)</span></span> pseudorandom generator. Then we could deterministically estimate the probability <span><span class="math inline">\(p(x)= \Pr_{r\sim \{0,1\}^m}[ Q_x(r) = 1 ]\)</span></span> up to <span><span class="math inline">\(0.1\)</span></span> accuracy in time <span><span class="math inline">\(O(T \cdot 2^\ell \cdot m \cdot cost(G))\)</span></span> where <span><span class="math inline">\(cost(G)\)</span></span> is the time that it takes to compute a single output bit of <span><span class="math inline">\(G\)</span></span>.</p>
<p>The reason is that we know that <span><span class="math inline">\(\tilde{p}(x)= \Pr_{s \sim \{0,1\}^\ell}[ Q_x(G(s)) = 1]\)</span></span> will give us such an estimate for <span><span class="math inline">\(p(x)\)</span></span>, and we can compute the probability <span><span class="math inline">\(\tilde{p}(x)\)</span></span> by simply trying all <span><span class="math inline">\(2^\ell\)</span></span> possibillites for <span><span class="math inline">\(s\)</span></span>. Now, under the optimal PRG conjecture we can set <span><span class="math inline">\(T = 2^{\delta \ell}\)</span></span> or equivalently <span><span class="math inline">\(\ell = \tfrac{1}{\delta}\log T\)</span></span>, and our total computation time is polynomial in <span><span class="math inline">\(2^\ell = T^{1/\delta}\)</span></span>. Since <span><span class="math inline">\(T \leq 10c \cdot n^d\)</span></span>, this running time will be polynomial in <span><span class="math inline">\(n\)</span></span>.</p>
<p>This completes the proof, since we are guaranteed that <span><span class="math inline">\(\Pr_{r\sim \{0,1\}^m}[ Q_x(r) = F(x) ] \geq 2/3\)</span></span>, and hence estimating the probability <span><span class="math inline">\(p(x)\)</span></span> to within <span><span class="math inline">\(0.1\)</span></span> accuracy is sufficient to compute <span><span class="math inline">\(F(x)\)</span></span>.</p>
</div>
<h2 id="mathbfpmathbfnp-and-mathbfbpp-vs-mathbfp" data-number="19.5"><span><span class="math inline">\(\mathbf{P}=\mathbf{NP}\)</span></span> and <span><span class="math inline">\(\mathbf{BPP}\)</span></span> vs <span><span class="math inline">\(\mathbf{P}\)</span></span></h2>
<p>Two computational complexity questions that we cannot settle are:</p>
<ul>
<li><p>Is <span><span class="math inline">\(\mathbf{P}=\mathbf{NP}\)</span></span>? Where we believe the answer is <em>negative</em>.</p></li>
<li><p>Is <span><span class="math inline">\(\mathbf{BPP}=\mathbf{P}\)</span></span>? Where we believe the answer is <em>positive</em>.</p></li>
</ul>
<p>However we can say that the “conventional wisdom” is correct on at least one of these questions. Namely, if we’re wrong on the first count, then we’ll be right on the second one:</p>
<div id="BPPvsNP" class="theorem" title="Sipser–Gács Theorem" name="Theorem 19.11 (Sipser–Gács Theorem) ">
<p>If <span><span class="math inline">\(\mathbf{P}=\mathbf{NP}\)</span></span> then <span><span class="math inline">\(\mathbf{BPP}=\mathbf{P}\)</span></span>.</p>
</div>
<div class="pause" name="Pause 19.5">
<p>Before reading the proof, it is instructive to think why this result is not “obvious.” If <span><span class="math inline">\(\mathbf{P}=\mathbf{NP}\)</span></span> then given any randomized algorithm <span><span class="math inline">\(A\)</span></span> and input <span><span class="math inline">\(x\)</span></span>, we will be able to figure out in polynomial time if there is a string <span><span class="math inline">\(r\in \{0,1\}^m\)</span></span> of random coins for <span><span class="math inline">\(A\)</span></span> such that <span><span class="math inline">\(A(xr)=1\)</span></span>. The problem is that even if <span><span class="math inline">\(\Pr_{r\in \{0,1\}^m}[A(xr)=F(x)] \geq 0.9999\)</span></span>, it can still be the case that even when <span><span class="math inline">\(F(x)=0\)</span></span> there exists a string <span><span class="math inline">\(r\)</span></span> such that <span><span class="math inline">\(A(xr)=1\)</span></span>.</p>
<p>The proof is rather subtle. It is much more important that you understand the <em>statement</em> of the theorem than that you follow all the details of the proof.</p>
</div>
<div class="proofidea" data-ref="BPPvsNP" name="Proofidea 19.5">
<p>The construction follows the “quantifier elimination” idea which we have seen in <a href='lec_14_PvsNP.html#PH-collapse-thm'>Theorem 15.6</a>. We will show that for every <span><span class="math inline">\(F \in \mathbf{BPP}\)</span></span>, we can reduce the question of some input <span><span class="math inline">\(x\)</span></span> satisfies <span><span class="math inline">\(F(x)=1\)</span></span> to the question of whether a formula of the form <span><span class="math inline">\(\exists_{u\in \{0,1\}^m} \forall_{v \in \{0,1\}^k} P(u,v)\)</span></span> is true, where <span><span class="math inline">\(m,k\)</span></span> are polynomial in the length of <span><span class="math inline">\(x\)</span></span> and <span><span class="math inline">\(P\)</span></span> is polynomial-time computable. By <a href='lec_14_PvsNP.html#PH-collapse-thm'>Theorem 15.6</a>, if <span><span class="math inline">\(\mathbf{P}=\mathbf{NP}\)</span></span> then we can decide in polynomial time whether such a formula is true or false.</p>
<p>The idea behind this construction is that using amplification we can obtain a randomized algorithm <span><span class="math inline">\(A\)</span></span> for computing <span><span class="math inline">\(F\)</span></span> using <span><span class="math inline">\(m\)</span></span> coins such that for every <span><span class="math inline">\(x\in \{0,1\}^n\)</span></span>, if <span><span class="math inline">\(F(x)=0\)</span></span> then the set <span><span class="math inline">\(S \subseteq \{0,1\}^m\)</span></span> of coins that make <span><span class="math inline">\(A\)</span></span> output <span><span class="math inline">\(1\)</span></span> is extremely tiny, and if <span><span class="math inline">\(F(x)=1\)</span></span> then it is very large. Now in the case <span><span class="math inline">\(F(x)=1\)</span></span>, one can show that this means that there exists a small number <span><span class="math inline">\(k\)</span></span> of “shifts” <span><span class="math inline">\(s_0,\ldots,s_{k-1}\)</span></span> such that the union of the sets <span><span class="math inline">\(S \oplus s_i\)</span></span> (i.e., sets of the form <span><span class="math inline">\(\{ s\oplus s_i \;|\; s\in S \}\)</span></span>) covers <span><span class="math inline">\(\{0,1\}^m\)</span></span>, while in the case <span><span class="math inline">\(F(x)=0\)</span></span> this union will always be of size at most <span><span class="math inline">\(k|S|\)</span></span> which is much smaller than <span><span class="math inline">\(2^m\)</span></span>. We can express the condition that there exists <span><span class="math inline">\(s_0,\ldots,s_{k-1}\)</span></span> such that <span><span class="math inline">\(\cup_{i\in [k]} (S \oplus s_i) = \{0,1\}^m\)</span></span> as a statement with a constant number of quantifiers.</p>
</div>
<figure>
<img src="../figure/strongamplification.png" alt="19.7: If F\in \mathbf{BPP} then through amplification we can ensure that there is an algorithm A to compute F on n-length inputs and using m coins such that \Pr_{r\in \{0,1\}^m}[ A(xr)\neq F(x)] \ll 1/poly(m). Hence if F(x)=1 then almost all of the 2^m choices for r will cause A(xr) to output 1, while if F(x)=0 then A(xr)=0 for almost all r’s. To prove the Sipser–Gács Theorem we consider several “shifts” of the set S \subseteq \{0,1\}^m of the coins r such that A(xr)=1. If F(x)=1 then we can find a set of k shifts s_0,\ldots,s_{k-1} for which \cup_{i\in [k]} (S \oplus s_i) = \{0,1\}^m. If F(x)=0 then for every such set |cup_{i\in [k]} S_i| \leq k |S| \ll 2^m. We can phrase the question of whether there is such a set of shift using a constant number of quantifiers, and so can solve it in polynomial time if \mathbf{P}=\mathbf{NP}." id="strongampbppfig" class="margin" /><figcaption>19.7: If <span><span class="math inline">\(F\in \mathbf{BPP}\)</span></span> then through amplification we can ensure that there is an algorithm <span><span class="math inline">\(A\)</span></span> to compute <span><span class="math inline">\(F\)</span></span> on <span><span class="math inline">\(n\)</span></span>-length inputs and using <span><span class="math inline">\(m\)</span></span> coins such that <span><span class="math inline">\(\Pr_{r\in \{0,1\}^m}[ A(xr)\neq F(x)] \ll 1/poly(m)\)</span></span>. Hence if <span><span class="math inline">\(F(x)=1\)</span></span> then almost all of the <span><span class="math inline">\(2^m\)</span></span> choices for <span><span class="math inline">\(r\)</span></span> will cause <span><span class="math inline">\(A(xr)\)</span></span> to output <span><span class="math inline">\(1\)</span></span>, while if <span><span class="math inline">\(F(x)=0\)</span></span> then <span><span class="math inline">\(A(xr)=0\)</span></span> for almost all <span><span class="math inline">\(r\)</span></span>’s. To prove the Sipser–Gács Theorem we consider several “shifts” of the set <span><span class="math inline">\(S \subseteq \{0,1\}^m\)</span></span> of the coins <span><span class="math inline">\(r\)</span></span> such that <span><span class="math inline">\(A(xr)=1\)</span></span>. If <span><span class="math inline">\(F(x)=1\)</span></span> then we can find a set of <span><span class="math inline">\(k\)</span></span> shifts <span><span class="math inline">\(s_0,\ldots,s_{k-1}\)</span></span> for which <span><span class="math inline">\(\cup_{i\in [k]} (S \oplus s_i) = \{0,1\}^m\)</span></span>. If <span><span class="math inline">\(F(x)=0\)</span></span> then for every such set <span><span class="math inline">\(|cup_{i\in [k]} S_i| \leq k |S| \ll 2^m\)</span></span>. We can phrase the question of whether there is such a set of shift using a constant number of quantifiers, and so can solve it in polynomial time if <span><span class="math inline">\(\mathbf{P}=\mathbf{NP}\)</span></span>.</figcaption>
</figure>
<div class="proof" data-ref="BPPvsNP" name="Proof 19.5">
<p>Let <span><span class="math inline">\(F \in \mathbf{BPP}\)</span></span>. Using <a href='#amplificationthm'>Theorem 19.5</a>, there exists a polynomial-time algorithm <span><span class="math inline">\(A\)</span></span> such that for every <span><span class="math inline">\(x\in \{0,1\}^n\)</span></span>, <span><span class="math inline">\(\Pr_{x \in \{0,1\}^m}[ A(xr)=F(x)] \geq 1 - 2^{-n}\)</span></span> where <span><span class="math inline">\(m\)</span></span> is polynomial in <span><span class="math inline">\(n\)</span></span>. In particular (since an exponential dominates a polynomial, and we can always assume <span><span class="math inline">\(n\)</span></span> is sufficiently large), it holds that <span>
<div class='myequationbox'><span class="math display">\[
\Pr_{x \in \{0,1\}^m}[ A(xr)=F(x)] \geq 1 - \tfrac{1}{10m^2}  \;. \;\;(19.10)
\]</span><a id='sipsergacseq'></a></div></span></p>
<p>Let <span><span class="math inline">\(x\in \{0,1\}^n\)</span></span>, and let <span><span class="math inline">\(S_x \subseteq \{0,1\}^m\)</span></span> be the set <span><span class="math inline">\(\{ r\in \{0,1\}^m \;:\; A(xr)=1 \}\)</span></span>. By our assumption, if <span><span class="math inline">\(F(x)=0\)</span></span> then <span><span class="math inline">\(|S_x| \leq \tfrac{1}{10m^2}2^{m}\)</span></span> and if <span><span class="math inline">\(F(x)=1\)</span></span> then <span><span class="math inline">\(|S_x| \geq (1-\tfrac{1}{10m^2})2^m\)</span></span>.</p>
<p>For a set <span><span class="math inline">\(S \subseteq \{0,1\}^m\)</span></span> and a string <span><span class="math inline">\(s\in \{0,1\}^m\)</span></span>, we define the set <span><span class="math inline">\(S \oplus s\)</span></span> to be <span><span class="math inline">\(\{ r \oplus s \;:\; r\in S \}\)</span></span> where <span><span class="math inline">\(\oplus\)</span></span> denotes the XOR operation. That is, <span><span class="math inline">\(S \oplus s\)</span></span> is the set <span><span class="math inline">\(S\)</span></span> “shifted” by <span><span class="math inline">\(s\)</span></span>. Note that <span><span class="math inline">\(|S \oplus s| = |S|\)</span></span>. (Please make sure that you see why this is true.)</p>
<p>The heart of the proof is the following two claims:</p>
<p><strong>CLAIM I:</strong> For every subset <span><span class="math inline">\(S \subseteq \{0,1\}^m\)</span></span>, if <span><span class="math inline">\(|S| \leq \tfrac{1}{1000m}2^m\)</span></span>, then for every <span><span class="math inline">\(s_0,\ldots,s_{100m-1} \in \{0,1\}^m\)</span></span>, <span><span class="math inline">\(\cup_{i\in [100m]} (S \oplus s_i) \subsetneq \{0,1\}^m\)</span></span>.</p>
<p><strong>CLAIM II:</strong> For every subset <span><span class="math inline">\(S \subseteq \{0,1\}^m\)</span></span>, if <span><span class="math inline">\(|S| \geq \tfrac{1}{2}2^m\)</span></span> then there exist a set of string <span><span class="math inline">\(s_0,\ldots,s_{100m-1}\)</span></span> such that <span><span class="math inline">\(\cup_{i\in [100m]} (S \oplus s_i) = \{0,1\}^m\)</span></span>.</p>
<p>CLAIM I and CLAIM II together imply the theorem. Indeed, they mean that under our assumptions, for every <span><span class="math inline">\(x\in \{0,1\}^n\)</span></span>, <span><span class="math inline">\(F(x)=1\)</span></span> if and only if</p>
<p><span>
<div class='myequationbox'><span class="math display">\[
\exists_{s_0,\ldots, s_{100m-1} \in \{0,1\}^m} \cup_{i\in [100m]} (S_x \oplus s_i) = \{0,1\}^m
\]</span></div></span></p>
<p>which we can re-write as</p>
<p><span>
<div class='myequationbox'><span class="math display">\[
\exists_{s_0,\ldots, s_{100m-1} \in \{0,1\}^m} \forall_{w\in \{0,1\}^m} \Bigl( w \in (S_x \oplus s_0) \vee w \in (S_x \oplus s_1) \vee \cdots w \in (S_x \oplus s_{100m-1}) \Bigr)
\]</span></div></span></p>
<p>or equivalently</p>
<p><span>
<div class='myequationbox'><span class="math display">\[
\exists_{s_0,\ldots, s_{100m-1} \in \{0,1\}^m} \forall_{w\in \{0,1\}^m} \Bigl( A(x(w\oplus s_0))=1 \vee A(x(w\oplus s_1))=1 \vee \cdots \vee A(x(w\oplus s_{100m-1}))=1    \Bigr)
\]</span></div></span></p>
<p>which (since <span><span class="math inline">\(A\)</span></span> is computable in polynomial time) is exactly the type of statement shown in <a href='lec_14_PvsNP.html#PH-collapse-thm'>Theorem 15.6</a> to be decidable in polynomial time if <span><span class="math inline">\(\mathbf{P}=\mathbf{NP}\)</span></span>.</p>
<p>We see that all that is left is to prove <strong>CLAIM I</strong> and <strong>CLAIM II</strong>. <strong>CLAIM I</strong> follows immediately from the fact that</p>
<p><span>
<div class='myequationbox'><span class="math display">\[
\cup_{i \in [100m-1]} |S_x \oplus s_i| \leq \sum_{i=0}^{100m-1} |S_x \oplus s_i| = \sum_{i=0}^{100m -1} |S_x| = 100m|S_x| \;.
\]</span></div></span></p>
<p>To prove <strong>CLAIM II</strong>, we will use a technique known as the <em>probabilistic method</em> (see the proof of <a href='#prgexist'>Lemma 19.12</a> for a more extensive discussion). Note that this is a completely different use of probability than in the theorem statement, we just use the methods of probability to prove an <em>existential</em> statement.</p>
<p><strong>Proof of CLAIM II:</strong> Let <span><span class="math inline">\(S \subseteq \{0,1\}^m\)</span></span> with <span><span class="math inline">\(|S| \geq 0.5 \cdot 2^m\)</span></span> be as in the claim’s statement. Consider the following probabilistic experiment: we choose <span><span class="math inline">\(100m\)</span></span> random shifts <span><span class="math inline">\(s_0,\ldots,s_{100m-1}\)</span></span> independently at random in <span><span class="math inline">\(\{0,1\}^m\)</span></span>, and consider the event <span><span class="math inline">\(\ensuremath{\mathit{GOOD}}\)</span></span> that <span><span class="math inline">\(\cup_{i\in [100m]}(S \oplus s_i) = \{0,1\}^m\)</span></span>. To prove CLAIM II it is enough to show that <span><span class="math inline">\(\Pr[ \ensuremath{\mathit{GOOD}} ] &gt; 0\)</span></span>, since that means that in particular there must <em>exist</em> shifts <span><span class="math inline">\(s_0,\ldots,s_{100m-1}\)</span></span> that satisfy this condition.</p>
<p>For every <span><span class="math inline">\(z \in \{0,1\}^m\)</span></span>, define the event <span><span class="math inline">\(\ensuremath{\mathit{BAD}}_z\)</span></span> to hold if <span><span class="math inline">\(z \not\in \cup_{i\in [100m-1]}(S \oplus s_i)\)</span></span>. The event <span><span class="math inline">\(\ensuremath{\mathit{GOOD}}\)</span></span> holds if <span><span class="math inline">\(\ensuremath{\mathit{BAD}}_z\)</span></span> fails for every <span><span class="math inline">\(z\in \{0,1\}^m\)</span></span>, and so our goal is to prove that <span><span class="math inline">\(\Pr[ \cup_{z\in \{0,1\}^m} \ensuremath{\mathit{BAD}}_z ] &lt;1\)</span></span>. By the union bound, to show this, it is enough to show that <span><span class="math inline">\(\Pr[ \ensuremath{\mathit{BAD}}_z ] &lt; 2^{-m}\)</span></span> for every <span><span class="math inline">\(z\in \{0,1\}^m\)</span></span>. Define the event <span><span class="math inline">\(\ensuremath{\mathit{BAD}}_z^i\)</span></span> to hold if <span><span class="math inline">\(z\not\in S \oplus s_i\)</span></span>. Since every shift <span><span class="math inline">\(s_i\)</span></span> is chosen independently, for every fixed <span><span class="math inline">\(z\)</span></span> the events <span><span class="math inline">\(\ensuremath{\mathit{BAD}}_z^0,\ldots,\ensuremath{\mathit{BAD}}_z^{100m-1}\)</span></span> are mutually independent,<sup id="fnref:4"><a href="#fn:4" rel="footnote">4</a></sup> and hence</p>
<p><span>
<div class='myequationbox'><span class="math display">\[\Pr[ \ensuremath{\mathit{BAD}}_z ] = \Pr[ \cap_{i\in [100m-1]} \ensuremath{\mathit{BAD}}_z^i ] = \prod_{i=0}^{100m-1} \Pr[\ensuremath{\mathit{BAD}}_z^i]  \;\;(19.15)\;.\]</span><a id='sipsergacsprodboundeq'></a></div></span></p>
<p>So this means that the result will follow by showing that <span><span class="math inline">\(\Pr[ \ensuremath{\mathit{BAD}}_z^i ] \leq \tfrac{1}{2}\)</span></span> for every <span><span class="math inline">\(z\in \{0,1\}^m\)</span></span> and <span><span class="math inline">\(i\in [100m]\)</span></span> (as that would allow to bound the righthand side of <a href='#sipsergacsprodboundeq'>Equation 19.15</a> by <span><span class="math inline">\(2^{-100m}\)</span></span>). In other words, we need to show that for every <span><span class="math inline">\(z\in \{0,1\}^m\)</span></span> and set <span><span class="math inline">\(S \subseteq \{0,1\}^m\)</span></span> with <span><span class="math inline">\(|S| \geq \tfrac{1}{2} 2^m\)</span></span>,</p>
<p><span>
<div class='myequationbox'><span class="math display">\[\Pr_{s \in \{0,1\}^m}[ z \in S \oplus s ] \geq \tfrac{1}{2}\; \;\;(19.16).\]</span><a id='sipsergacsprodboundtwoeq'></a></div></span></p>
<p>To show this, we observe that <span><span class="math inline">\(z \in S \oplus s\)</span></span> if and only if <span><span class="math inline">\(s \in S \oplus z\)</span></span> (can you see why). Hence we can rewrite the probability on the lefthand side of <a href='#sipsergacsprodboundtwoeq'>Equation 19.16</a> as <span><span class="math inline">\(\Pr_{s\in \{0,1\}^m}[ s\in S \oplus z]\)</span></span> which simply equals <span><span class="math inline">\(|S \oplus z|/2^m = |S|/2^m \geq 1/2\)</span></span>! This concludes the proof of <strong>CLAIM I</strong> and hence of <a href='#BPPvsNP'>Theorem 19.11</a>.</p>
</div>
<h2 id="non-constructive-existence-of-pseudorandom-generators-advanced-optional" data-number="19.6">Non-constructive existence of pseudorandom generators (advanced, optional)</h2>
<p>We now show that, if we don’t insist on <em>constructivity</em> of pseudorandom generators, then we can show that there exists pseudorandom generators with output that <em>exponentially larger</em> in the input length.</p>
<div id="prgexist" class="lemma" title="Existence of inefficient pseudorandom generators" name="Lemma 19.12 (Existence of inefficient pseudorandom generators) ">
<p>There is some absolute constant <span><span class="math inline">\(C\)</span></span> such that for every <span><span class="math inline">\(\epsilon,T\)</span></span>, if <span><span class="math inline">\(\ell &gt; C (\log T + \log (1/\epsilon))\)</span></span> and <span><span class="math inline">\(m \leq T\)</span></span>, then there is an <span><span class="math inline">\((T,\epsilon)\)</span></span> pseudorandom generator <span><span class="math inline">\(G: \{0,1\}^\ell \rightarrow \{0,1\}^m\)</span></span>.</p>
</div>
<div id="section-9" class="proofidea" data-ref="prgexist" name="Proofidea">
<p>The proof uses an extremely useful technique known as the “probabilistic method” which is not too hard mathematically but can be confusing at first.<sup id="fnref:5"><a href="#fn:5" rel="footnote">5</a></sup> The idea is to give a “non constructive” proof of existence of the pseudorandom generator <span><span class="math inline">\(G\)</span></span> by showing that if <span><span class="math inline">\(G\)</span></span> was chosen at random, then the probability that it would be a valid <span><span class="math inline">\((T,\epsilon)\)</span></span> pseudorandom generator is positive. In particular this means that there <em>exists</em> a single <span><span class="math inline">\(G\)</span></span> that is a valid <span><span class="math inline">\((T,\epsilon)\)</span></span> pseudorandom generator. The probabilistic method is just a <em>proof technique</em> to demonstrate the existence of such a function. Ultimately, our goal is to show the existence of a <em>deterministic</em> function <span><span class="math inline">\(G\)</span></span> that satisfies the condition.</p>
</div>
<p>The above discussion might be rather abstract at this point, but would become clearer after seeing the proof.</p>
<div class="proof" data-ref="prgexist" name="Proof 19.6">
<p>Let <span><span class="math inline">\(\epsilon,T,\ell,m\)</span></span> be as in the lemma’s statement. We need to show that there exists a function <span><span class="math inline">\(G:\{0,1\}^\ell \rightarrow \{0,1\}^m\)</span></span> that “fools” every <span><span class="math inline">\(T\)</span></span> line program <span><span class="math inline">\(P\)</span></span> in the sense of <a href='#eq:prg'>Equation 19.9</a>. We will show that this follows from the following claim:</p>
<p><strong>Claim I:</strong> For every fixed NAND-CIRC program <span><span class="math inline">\(P\)</span></span>, if we pick <span><span class="math inline">\(G:\{0,1\}^\ell \rightarrow \{0,1\}^m\)</span></span> <em>at random</em> then the probability that <a href='#eq:prg'>Equation 19.9</a> is violated is at most <span><span class="math inline">\(2^{-T^2}\)</span></span>.</p>
<p>Before proving Claim I, let us see why it implies <a href='#prgexist'>Lemma 19.12</a>. We can identify a function <span><span class="math inline">\(G:\{0,1\}^\ell \rightarrow \{0,1\}^m\)</span></span> with its “truth table” or simply the list of evaluations on all its possible <span><span class="math inline">\(2^\ell\)</span></span> inputs. Since each output is an <span><span class="math inline">\(m\)</span></span> bit string, we can also think of <span><span class="math inline">\(G\)</span></span> as a string in <span><span class="math inline">\(\{0,1\}^{m\cdot 2^\ell}\)</span></span>. We define <span><span class="math inline">\(\mathcal{F}^m_\ell\)</span></span> to be the set of all functions from <span><span class="math inline">\(\{0,1\}^\ell\)</span></span> to <span><span class="math inline">\(\{0,1\}^m\)</span></span>. As discussed above we can identify <span><span class="math inline">\(\mathcal{F}_\ell^m\)</span></span> with <span><span class="math inline">\(\{0,1\}^{m\cdot 2^\ell}\)</span></span> and choosing a random function <span><span class="math inline">\(G \sim \mathcal{F}_\ell^m\)</span></span> corresponds to choosing a random <span><span class="math inline">\(m\cdot 2^\ell\)</span></span>-long bit string.</p>
<p>For every NAND-CIRC program <span><span class="math inline">\(P\)</span></span> let <span><span class="math inline">\(B_P\)</span></span> be the event that, if we choose <span><span class="math inline">\(G\)</span></span> at random from <span><span class="math inline">\(\mathcal{F}_\ell^m\)</span></span> then <a href='#eq:prg'>Equation 19.9</a> is violated with respect to the program <span><span class="math inline">\(P\)</span></span>. It is important to understand what is the sample space that the event <span><span class="math inline">\(B_P\)</span></span> is defined over, namely this event depends on the choice of <span><span class="math inline">\(G\)</span></span> and so <span><span class="math inline">\(B_P\)</span></span> is a subset of <span><span class="math inline">\(\mathcal{F}_\ell^m\)</span></span>. An equivalent way to define the event <span><span class="math inline">\(B_P\)</span></span> is that it is the subset of all functions mapping <span><span class="math inline">\(\{0,1\}^\ell\)</span></span> to <span><span class="math inline">\(\{0,1\}^m\)</span></span> that violate <a href='#eq:prg'>Equation 19.9</a>, or in other words:</p>
<p><span>
<div class='myequationbox'><span class="math display">\[
B_P = \left\{ G \in \mathcal{F}_\ell^m  \; \big| \; \left| \tfrac{1}{2^\ell}\sum_{s\in \{0,1\}^\ell} P(G(s)) - \tfrac{1}{2^m}\sum_{r \in \{0,1\}^m}P(r)  \right| &gt; \epsilon  \right\} \;\; \;\;(19.17)
\]</span><a id='eq:eventdefine'></a></div></span> (We’ve replaced here the probability statements in <a href='#eq:prg'>Equation 19.9</a> with the equivalent sums so as to reduce confusion as to what is the sample space that <span><span class="math inline">\(B_P\)</span></span> is defined over.)</p>
<p>To understand this proof it is crucial that you pause here and see how the definition of <span><span class="math inline">\(B_P\)</span></span> above corresponds to <a href='#eq:eventdefine'>Equation 19.17</a>. This may well take re-reading the above text once or twice, but it is a good exercise at parsing probabilistic statements and learning how to identify the <em>sample space</em> that these statements correspond to.</p>
<p>Now, we’ve shown in <a href='lec_04_code_and_data.html#program-count'>Theorem 5.2</a> that up to renaming variables (which makes no difference to program’s functionality) there are <span><span class="math inline">\(2^{O(T\log T)}\)</span></span> NAND-CIRC programs of at most <span><span class="math inline">\(T\)</span></span> lines. Since <span><span class="math inline">\(T\log T &lt; T^2\)</span></span> for sufficiently large <span><span class="math inline">\(T\)</span></span>, this means that if Claim I is true, then by the union bound it holds that the probability of the union of <span><span class="math inline">\(B_P\)</span></span> over <em>all</em> NAND-CIRC programs of at most <span><span class="math inline">\(T\)</span></span> lines is at most <span><span class="math inline">\(2^{O(T\log T)}2^{-T^2} &lt; 0.1\)</span></span> for sufficiently large <span><span class="math inline">\(T\)</span></span>. What is important for us about the number <span><span class="math inline">\(0.1\)</span></span> is that it is smaller than <span><span class="math inline">\(1\)</span></span>. In particular this means that there <em>exists</em> a single <span><span class="math inline">\(G^* \in \mathcal{F}_\ell^m\)</span></span> such that <span><span class="math inline">\(G^*\)</span></span> <em>does not</em> violate <a href='#eq:prg'>Equation 19.9</a> with respect to any NAND-CIRC program of at most <span><span class="math inline">\(T\)</span></span> lines, but that precisely means that <span><span class="math inline">\(G^*\)</span></span> is a <span><span class="math inline">\((T,\epsilon)\)</span></span> pseudorandom generator.</p>
<p>Hence to conclude the proof of <a href='#prgexist'>Lemma 19.12</a>, it suffices to prove Claim I. Choosing a random <span><span class="math inline">\(G: \{0,1\}^\ell \rightarrow \{0,1\}^m\)</span></span> amounts to choosing <span><span class="math inline">\(L=2^\ell\)</span></span> random strings <span><span class="math inline">\(y_0,\ldots,y_{L-1} \in \{0,1\}^m\)</span></span> and letting <span><span class="math inline">\(G(x)=y_x\)</span></span> (identifying <span><span class="math inline">\(\{0,1\}^\ell\)</span></span> and <span><span class="math inline">\([L]\)</span></span> via the binary representation). This means that proving the claim amounts to showing that for every fixed function <span><span class="math inline">\(P:\{0,1\}^m \rightarrow \{0,1\}\)</span></span>, if <span><span class="math inline">\(L &gt; 2^{C (\log T + \log \epsilon)}\)</span></span> (which by setting <span><span class="math inline">\(C&gt;4\)</span></span>, we can ensure is larger than <span><span class="math inline">\(10 T^2/\epsilon^2\)</span></span>) then the probability that <span>
<div class='myequationbox'><span class="math display">\[
\left| \tfrac{1}{L}\sum_{i=0}^{L-1} P(y_i)  -  \Pr_{s \sim \{0,1\}^m}[P(s)=1] \right| &gt; \epsilon \;\;(19.18)
\]</span><a id='eq:prgchernoff'></a></div></span> is at most <span><span class="math inline">\(2^{-T^2}\)</span></span>.</p>
<p><a href='#{eq:prgchernoff}'>?? ??</a> follows directly from the Chernoff bound. Indeed, if we let for every <span><span class="math inline">\(i\in [L]\)</span></span> the random variable <span><span class="math inline">\(X_i\)</span></span> denote <span><span class="math inline">\(P(y_i)\)</span></span>, then since <span><span class="math inline">\(y_0,\ldots,y_{L-1}\)</span></span> is chosen independently at random, these are independently and identically distributed random variables with mean <span><span class="math inline">\(\E_{y \sim \{0,1\}^m}[P(y)]= \Pr_{y\sim \{0,1\}^m}[ P(y)=1]\)</span></span> and hence the probability that they deviate from their expectation by <span><span class="math inline">\(\epsilon\)</span></span> is at most <span><span class="math inline">\(2\cdot 2^{-\epsilon^2 L/2}\)</span></span>.</p>
</div>
<figure>
<img src="../figure/bppcomplexitypicture.png" alt="19.8: The relation between \mathbf{BPP} and the other complexity classes that we have seen. We know that \mathbf{P} \subseteq \mathbf{BPP} \subseteq \mathbf{EXP} and \mathbf{BPP} \subseteq \mathbf{P_{/poly}} but we don’t know how \mathbf{BPP} compares with \mathbf{NP} and can’t rule out even \mathbf{BPP} =\mathbf{EXP}. Most evidence points out to the possibliity that \mathbf{BPP}=\mathbf{P}." id="bppcomplexitypicturefig" /><figcaption>19.8: The relation between <span><span class="math inline">\(\mathbf{BPP}\)</span></span> and the other complexity classes that we have seen. We know that <span><span class="math inline">\(\mathbf{P} \subseteq \mathbf{BPP} \subseteq \mathbf{EXP}\)</span></span> and <span><span class="math inline">\(\mathbf{BPP} \subseteq \mathbf{P_{/poly}}\)</span></span> but we don’t know how <span><span class="math inline">\(\mathbf{BPP}\)</span></span> compares with <span><span class="math inline">\(\mathbf{NP}\)</span></span> and can’t rule out even <span><span class="math inline">\(\mathbf{BPP} =\mathbf{EXP}\)</span></span>. Most evidence points out to the possibliity that <span><span class="math inline">\(\mathbf{BPP}=\mathbf{P}\)</span></span>.</figcaption>
</figure>
<div class="recap" name="Recap 19.6">
<ul>
<li><p>We can model randomized algorithms by either adding a special “coin toss” operation or assuming an extra randomly chosen input.</p></li>
<li><p>The class <span><span class="math inline">\(\mathbf{BPP}\)</span></span> contains the set of Boolean functions that can be computed by polynomial time randomized algorithms.</p></li>
<li><p><span><span class="math inline">\(\mathbf{BPP}\)</span></span> is a <em>worst case</em> class of computation: a randomized algorithm to compute a function must compute it correctly with high probability <em>on every input</em>.</p></li>
<li><p>We can <em>amplify</em> the success probability of randomized algorithm from any value strictly larger than <span><span class="math inline">\(1/2\)</span></span> into a success probability that is <em>exponentiall close to <span><span class="math inline">\(1\)</span></span></em>.</p></li>
<li><p>We know that <span><span class="math inline">\(\mathbf{P} \subseteq \mathbf{BPP} \subseteq \mathbf{EXP}\)</span></span>.</p></li>
<li><p>We also know that <span><span class="math inline">\(\mathbf{BPP} \subseteq \mathbf{P_{/poly}}\)</span></span>.</p></li>
<li><p>The relation between <span><span class="math inline">\(\mathbf{BPP}\)</span></span> and <span><span class="math inline">\(\mathbf{NP}\)</span></span> is not known, but we do know that if <span><span class="math inline">\(\mathbf{P}=\mathbf{NP}\)</span></span> then <span><span class="math inline">\(\mathbf{BPP}=\mathbf{P}\)</span></span>.</p></li>
<li><p>Pseudorandom generators are objects that take a short random “seed” and expand it to a much longer output that “appears random” for efficient algorithms. We conjecture that exponentially strong pseudorandom generators exist. Under this conjecture, <span><span class="math inline">\(\mathbf{BPP}=\mathbf{P}\)</span></span>.</p></li>
</ul>
</div>
<h2 id="exercises" data-number="19.7">Exercises</h2>
<h2 id="modelrandbibnotes" data-number="19.8">Bibliographical notes</h2>
<p>In this chapter we ignore the issue of how we actually get random bits in practice. The output of many physical processes, whether it is thermal heat, network and hard drive latency, user typing pattern and mouse movements, and more can be thought of as a binary string sampled from some distribution <span><span class="math inline">\(\mu\)</span></span> that might have significant unpredictability (or <em>entropy</em>) but is not necessarily the <em>uniform</em> distribution over <span><span class="math inline">\(\{0,1\}^n\)</span></span>. Indeed, as <a href="http://statweb.stanford.edu/~susan/papers/headswithJ.pdf">this paper</a> shows, even (real-world) coin tosses do not have exactly the distribution of a uniformly random string. Therefore, to use the resulting measurements for randomized algorithms, one typically needs to apply a “distillation” or <a href="https://en.wikipedia.org/wiki/Randomness_extractor">randomness extraction</a> process to the raw measurements to transform them to the uniform distribution. Vadhan’s book  (<a href="https://scholar.google.com/scholar?hl=en&q=Vadhan,+others+Pseudorandomness" target="_blank">Vadhan, others, 2012</a>)  is an excellent source for more discussion on both randomness extractors and pseudorandom generators.</p>
<p>The name <span><span class="math inline">\(\mathbf{BPP}\)</span></span> stands for “bounded probability polynomial time”. This is an historical accident: this class probably should have been called <span><span class="math inline">\(\mathbf{RP}\)</span></span> or <span><span class="math inline">\(\mathbf{PP}\)</span></span> but both names were taken by other classes.</p>
<p>The proof of <a href='#rnandthm'>Theorem 19.8</a> actually yields more than its statement. We can use the same “unrolling the loop” arguments we’ve used before to show that the restriction to <span><span class="math inline">\(\{0,1\}^n\)</span></span> of every function in <span><span class="math inline">\(\mathbf{BPP}\)</span></span> is also computable by a polynomial-size RNAND-CIRC program (i.e., NAND-CIRC program with the <code>RAND</code> operation). Like in the <span><span class="math inline">\(\mathbf{P}\)</span></span> vs <span><span class="math inline">\(\ensuremath{\mathit{SIZE}}(poly(n))\)</span></span> case, there are also functions outside <span><span class="math inline">\(\mathbf{BPP}\)</span></span> whose restrictions can be computed by polynomial-size RNAND-CIRC programs. Nevertheless the proof of <a href='#rnandthm'>Theorem 19.8</a> shows that even such functions can be computed by polynomial sized NAND-CIRC programs without using the <code>rand</code> operations. This can be phrased as saying that <span><span class="math inline">\(\ensuremath{\mathit{BPSIZE}}(T(n)) \subseteq \ensuremath{\mathit{SIZE}}(O(n T(n)))\)</span></span> (where <span><span class="math inline">\(\ensuremath{\mathit{BPSIZE}}\)</span></span> is defined in the natural way using RNAND progams). The stronger version of <a href='#rnandthm'>Theorem 19.8</a> we mentioned can be phrased as saying that <span><span class="math inline">\(\mathbf{BPP_{/poly}} = \mathbf{P_{/poly}}\)</span></span>.</p>
<div id="footnotediv" class="footnotes">
<ol>
<li class="footnote" id="fn:1"><p>
<div>
<p>At the time of this writing, the largest “natural” complexity class which we can’t rule out being contained in <span><span class="math inline">\(\mathbf{BPP}\)</span></span> is the class <span><span class="math inline">\(\mathbf{NEXP}\)</span></span>, which we did not define in this course, but corresponds to non deterministic exponential time. See <a href="https://people.csail.mit.edu/rrw/nexp-v-bpp.pdf">this paper</a> for a discussion of this question.</p>
</div>
<a href="#fnref:1" title="return to article"> ↩</a><p></li>
<li class="footnote" id="fn:2"><p>
<div>
<p>One amusing anecdote is a <a href="https://www.wired.com/2017/02/russians-engineer-brilliant-slot-machine-cheat-casinos-no-fix/">recent case</a> where scammers managed to predict the imperfect “pseudorandom generator” used by slot machines to cheat casinos. Unfortunately we don’t know the details of how they did it, since the case was <a href="https://www.plainsite.org/dockets/2j3mlaig6/missouri-eastern-district-court/usa-v-bliev-et-al/">sealed</a>.</p>
</div>
<a href="#fnref:2" title="return to article"> ↩</a><p></li>
<li class="footnote" id="fn:3"><p>
<div>
<p>A pseudorandom generator of the form we posit, where each output bit can be computed individually in time polynomial in the seed length, is commonly known as a <em>pseudorandom function generator</em>. For more on the many interesting results and connections in the study of <em>pseudorandomness</em>, see <a href="https://people.seas.harvard.edu/~salil/pseudorandomness/">this monograph of Salil Vadhan</a>.</p>
</div>
<a href="#fnref:3" title="return to article"> ↩</a><p></li>
<li class="footnote" id="fn:4"><p>
<div>
<p>The condition of independence here is subtle. It is <em>not</em> the case that all of the <span><span class="math inline">\(2^m \times 100m\)</span></span> events <span><span class="math inline">\(\{ \ensuremath{\mathit{BAD}}_z^i \}_{z\in \{0,1\}^m,i\in [100m]}\)</span></span> are mutually independent. Only for a fixed <span><span class="math inline">\(z \in \{0,1\}^m\)</span></span>, the <span><span class="math inline">\(100m\)</span></span> events of the form <span><span class="math inline">\(\ensuremath{\mathit{BAD}}_z^i\)</span></span> are mutually independent.</p>
</div>
<a href="#fnref:4" title="return to article"> ↩</a><p></li>
<li class="footnote" id="fn:5"><p>
<div>
<p>There is a whole (highly recommended) <a href="https://www.amazon.com/Probabilistic-Method-Discrete-Mathematics-Optimization/dp/1119061954/ref=dp_ob_title_bk">book by Alon and Spencer</a> devoted to this method.</p>
</div>
<a href="#fnref:5" title="return to article"> ↩</a><p></li>
</ol>
</div>
<!--bookdown:body:end-->


<!-- end of  actual content -->

<!-- start of comments -->


<a name="commentform"></a>
<h2 id="comments" class="nocount">Comments</h2>

<p>Comments are posted on the <a href="https://github.com/boazbk/tcs/issues">GitHub repository</a> using the <a href="https://utteranc.es">utteranc.es</a> app.
A GitHub login is required to comment.
If you don't want to authorize the app to post on your behalf, you can also comment directly on the <a href="https://github.com/boazbk/tcs/issues?q=Defining Computation+in%3Atitle">GitHub issue for this page</a>.


<p>


<script src="https://utteranc.es/client.js" 
repo="boazbk/tcs" 
issue-term="title" 
label="comments"
theme="github-light" 
crossorigin="anonymous" async>
  </script>


<!-- end of comments -->

<p>Compiled on 12/02/2019 21:39:11</p>

<p>Copyright 2019, Boaz Barak.


<a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/"><img alt="Creative Commons License"
    style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png" /></a><br />This work is
licensed under a <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/">Creative Commons
  Attribution-NonCommercial-NoDerivatives 4.0 International License</a>.

<p>Produced using <a href="https://pandoc.org/">pandoc</a> and <a href="http://scorreia.com/software/panflute/">panflute</a> with templates derived from <a href="https://www.gitbook.com/">gitbook</a> and <a href="https://bookdown.org/">bookdown</a>.</p>



</div>


            </section>

          </div>
        </div>
      </div>
<!--bookdown:link_prev-->
<!--bookdown:link_next-->



    </div>
  </div>
<!--bookdown:config-->
<script src="js/app.min.js"></script>
<script src="js/lunr.js"></script>
<script src="js/plugin-search.js"></script>
<script src="js/plugin-sharing.js"></script>
<script src="js/plugin-fontsettings.js"></script>
<script src="js/fullscreen.js"></script>
<script src="js/plugin-bookdown.js"></script>
<script src="js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"history": {
"link": null,
"text": null
},
"download": ["https://files.boazbarak.org/introtcs/lec_17_model_rand.pdf"],
"toc": {
"collapse": "section"
}
});
});
</script>


</body>

</html>
