<!DOCTYPE html>
<html  lang="en">

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Introduction to Theoretical Computer Science: Equivalent models of computation</title>
  <meta name="description" content="Textbook on Theoretical Computer Science by Boaz Barak">

  <meta property="og:title" content="Introduction to Theoretical Computer Science: Equivalent models of computation" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://introtcs.org/" />
  <meta property="og:image" content="icons/cover.png" />
  <meta property="og:description" content="Textbook on Theoretical Computer Science by Boaz Barak" />
  <meta name="github-repo" content="boazbk/tcs" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Introduction to Theoretical Computer Science" />
  <meta name="twitter:description" content="Textbook on Theoretical Computer Science by Boaz Barak" />
  <meta name="twitter:image" content="https://introtcs.org/icons/cover.png" />

<meta name="author" content="Boaz Barak">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <link rel="apple-touch-icon-precomposed" sizes="120x120" href="icons/apple-touch-icon-120x120.png">
  <link rel="shortcut icon" href="icons/favicon.ico" type="image/x-icon">

<!-- Boaz: resources -->

<!-- <script src="https://kit.fontawesome.com/ab08ce82a8.js"></script> -->

<link rel="stylesheet" src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.9.0/css/all.min.css">


<!-- KaTeX -->


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css"
  integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">

<!-- The loading of KaTeX is deferred to speed up page rendering -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js"
  integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>

<!-- To automatically render math in text elements, include the auto-render extension: -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js"
  integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
  onload='renderMathInElement(document.body, {  throwOnError: false, macros: { "\\N": "\\mathbb{N}", "\\R": "\\mathbb{R}", "\\Z": "\\mathbb{Z}","\\E": "\\mathbb{E}","\\val": "\\mathrm{val}", "\\label": "\\;\\;\\;\\;\\;\\;\\;\\;","\\floor": "\\lfloor #1 \\rfloor","\\ceil": "\\lceil #1 \\rceil", "\\ensuremath": "#1"}});'>
</script>




<!-- KaTeX -->
<!-- pseudocode -->
<link rel="stylesheet" href="css/pseudocode.css">
<!-- <script src="js/pseudocode.min.js"></script> -->


<!-- Gitbook resources -->

  <script src="js/jquery.min.js"></script>
  <link href="css/style.css" rel="stylesheet" />
  
  <link href="css/plugin-table.css" rel="stylesheet" />
  <link href="css/plugin-bookdown.css" rel="stylesheet" />
  <link href="css/plugin-highlight.css" rel="stylesheet" />
  <link href="css/plugin-search.css" rel="stylesheet" />
  <link href="css/plugin-fontsettings.css" rel="stylesheet" />
  <link href="css/moregitbook.css" rel="stylesheet" />

  <link href="css/resmisc.css" rel="stylesheet" />





<!-- Boaz: end resources -->



<!--bookdown:link_prev-->
<!--bookdown:link_next-->



<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<!-- bigfoot-->

<link href="css/bigfoot-default.css" rel="stylesheet" />
<script type="text/javascript" src="js/bigfoot.js"></script>

<script type="text/javascript">
    var bigfoot = jQuery.bigfoot(
        {
            deleteOnUnhover: false,
            preventPageScroll: false,
            hoverDelay: 250
        }
    );
</script>

<!-- end bigfoot -->


</head>

<body>



<!--bookdown:title:start-->
<!--bookdown:title:end-->


<!--bookdown:toc:start-->
  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">
<!--bookdown:toc2:start-->
<ul class="summary">
<li><a href="./">Introduction to Theoretical Computer Science</a></li>
<li class="divider"></li><li class="chapter" data-level="p" data-path="lec_00_0_preface.html"><a href="lec_00_0_preface.html"><i class="fa fa-check"></i><b>p</b> Preface</a><ul><li class="chapter" data-level="p.1" data-path="lec_00_0_preface.html"><a href="lec_00_0_preface.html#to-the-student"><i class="fa fa-check"></i><b>p.1</b> To the student</a><ul><li class="chapter" data-level="p.1.1" data-path="lec_00_0_preface.html"><a href="lec_00_0_preface.html#is-the-effort-worth-it"><i class="fa fa-check"></i><b>p.1.1</b> Is the effort worth it?</a></li></ul></li><li class="chapter" data-level="p.2" data-path="lec_00_0_preface.html"><a href="lec_00_0_preface.html#to-potential-instructors"><i class="fa fa-check"></i><b>p.2</b> To potential instructors</a></li><li class="chapter" data-level="p.3" data-path="lec_00_0_preface.html"><a href="lec_00_0_preface.html#acknowledgements"><i class="fa fa-check"></i><b>p.3</b> Acknowledgements</a></li></ul></li><li class="chapter" data-level="0" data-path="lec_01_introduction.html"><a href="lec_01_introduction.html"><i class="fa fa-check"></i><b>0</b> Introduction</a><ul><li class="chapter" data-level="0.1" data-path="lec_01_introduction.html"><a href="lec_01_introduction.html#integer-multiplication-an-example-of-an-algorithm"><i class="fa fa-check"></i><b>0.1</b> Integer multiplication: an example of an algorithm</a></li><li class="chapter" data-level="0.2" data-path="lec_01_introduction.html"><a href="lec_01_introduction.html#karatsubasec"><i class="fa fa-check"></i><b>0.2</b> Extended Example: A faster way to multiply (optional)</a></li><li class="chapter" data-level="0.3" data-path="lec_01_introduction.html"><a href="lec_01_introduction.html#algsbeyondarithmetic"><i class="fa fa-check"></i><b>0.3</b> Algorithms beyond arithmetic</a></li><li class="chapter" data-level="0.4" data-path="lec_01_introduction.html"><a href="lec_01_introduction.html#on-the-importance-of-negative-results."><i class="fa fa-check"></i><b>0.4</b> On the importance of negative results.</a></li><li class="chapter" data-level="0.5" data-path="lec_01_introduction.html"><a href="lec_01_introduction.html#roadmapsec"><i class="fa fa-check"></i><b>0.5</b> Roadmap to the rest of this book</a><ul><li class="chapter" data-level="0.5.1" data-path="lec_01_introduction.html"><a href="lec_01_introduction.html#dependencies-between-chapters"><i class="fa fa-check"></i><b>0.5.1</b> Dependencies between chapters</a></li></ul></li><li class="chapter" data-level="0.6" data-path="lec_01_introduction.html"><a href="lec_01_introduction.html#exercises"><i class="fa fa-check"></i><b>0.6</b> Exercises</a></li><li class="chapter" data-level="0.7" data-path="lec_01_introduction.html"><a href="lec_01_introduction.html#bnotesintrosec"><i class="fa fa-check"></i><b>0.7</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="1" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html"><i class="fa fa-check"></i><b>1</b> Mathematical Background</a><ul><li class="chapter" data-level="1.1" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#manualbackground"><i class="fa fa-check"></i><b>1.1</b> This chapter: a reader’s manual</a></li><li class="chapter" data-level="1.2" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#secmathoverview"><i class="fa fa-check"></i><b>1.2</b> A quick overview of mathematical prerequisites</a></li><li class="chapter" data-level="1.3" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#reading-mathematical-texts"><i class="fa fa-check"></i><b>1.3</b> Reading mathematical texts</a><ul><li class="chapter" data-level="1.3.1" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#definitions"><i class="fa fa-check"></i><b>1.3.1</b> Definitions</a></li><li class="chapter" data-level="1.3.2" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#assertions-theorems-lemmas-claims"><i class="fa fa-check"></i><b>1.3.2</b> Assertions: Theorems, lemmas, claims</a></li><li class="chapter" data-level="1.3.3" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#proofs"><i class="fa fa-check"></i><b>1.3.3</b> Proofs</a></li></ul></li><li class="chapter" data-level="1.4" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#basic-discrete-math-objects"><i class="fa fa-check"></i><b>1.4</b> Basic discrete math objects</a><ul><li class="chapter" data-level="1.4.1" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#sets"><i class="fa fa-check"></i><b>1.4.1</b> Sets</a></li><li class="chapter" data-level="1.4.2" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#specialsets"><i class="fa fa-check"></i><b>1.4.2</b> Special sets</a></li><li class="chapter" data-level="1.4.3" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#functionsec"><i class="fa fa-check"></i><b>1.4.3</b> Functions</a></li><li class="chapter" data-level="1.4.4" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#graphsec"><i class="fa fa-check"></i><b>1.4.4</b> Graphs</a></li><li class="chapter" data-level="1.4.5" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#secquantifiers"><i class="fa fa-check"></i><b>1.4.5</b> Logic operators and quantifiers</a></li><li class="chapter" data-level="1.4.6" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#secquantifierssums"><i class="fa fa-check"></i><b>1.4.6</b> Quantifiers for summations and products</a></li><li class="chapter" data-level="1.4.7" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#boundvarsec"><i class="fa fa-check"></i><b>1.4.7</b> Parsing formulas: bound and free variables</a></li><li class="chapter" data-level="1.4.8" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#secbigohnotation"><i class="fa fa-check"></i><b>1.4.8</b> Asymptotics and Big-O notation</a></li><li class="chapter" data-level="1.4.9" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#some-rules-of-thumb-for-big-o-notation"><i class="fa fa-check"></i><b>1.4.9</b> Some rules of thumb for Big-O notation</a></li></ul></li><li class="chapter" data-level="1.5" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#proofsbackgroundsec"><i class="fa fa-check"></i><b>1.5</b> Proofs</a><ul><li class="chapter" data-level="1.5.1" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#proofs-and-programs"><i class="fa fa-check"></i><b>1.5.1</b> Proofs and programs</a></li><li class="chapter" data-level="1.5.2" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#proof-writing-style"><i class="fa fa-check"></i><b>1.5.2</b> Proof writing style</a></li><li class="chapter" data-level="1.5.3" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#patterns-in-proofs"><i class="fa fa-check"></i><b>1.5.3</b> Patterns in proofs</a></li></ul></li><li class="chapter" data-level="1.6" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#topsortsec"><i class="fa fa-check"></i><b>1.6</b> Extended example: Topological Sorting</a><ul><li class="chapter" data-level="1.6.1" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#inductionsec"><i class="fa fa-check"></i><b>1.6.1</b> Mathematical induction</a></li><li class="chapter" data-level="1.6.2" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#proving-the-result-by-induction"><i class="fa fa-check"></i><b>1.6.2</b> Proving the result by induction</a></li><li class="chapter" data-level="1.6.3" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#minimality-and-uniqueness"><i class="fa fa-check"></i><b>1.6.3</b> Minimality and uniqueness</a></li></ul></li><li class="chapter" data-level="1.7" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#notationsec"><i class="fa fa-check"></i><b>1.7</b> This book: notation and conventions</a><ul><li class="chapter" data-level="1.7.1" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#conventionsec"><i class="fa fa-check"></i><b>1.7.1</b> Variable name conventions</a></li><li class="chapter" data-level="1.7.2" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#some-idioms"><i class="fa fa-check"></i><b>1.7.2</b> Some idioms</a></li></ul></li><li class="chapter" data-level="1.8" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#exercises"><i class="fa fa-check"></i><b>1.8</b> Exercises</a></li><li class="chapter" data-level="1.9" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#notesmathchap"><i class="fa fa-check"></i><b>1.9</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="2" data-path="lec_02_representation.html"><a href="lec_02_representation.html"><i class="fa fa-check"></i><b>2</b> Computation and Representation</a><ul><li class="chapter" data-level="2.1" data-path="lec_02_representation.html"><a href="lec_02_representation.html#defining-representations"><i class="fa fa-check"></i><b>2.1</b> Defining representations</a><ul><li class="chapter" data-level="2.1.1" data-path="lec_02_representation.html"><a href="lec_02_representation.html#representing-natural-numbers"><i class="fa fa-check"></i><b>2.1.1</b> Representing natural numbers</a></li><li class="chapter" data-level="2.1.2" data-path="lec_02_representation.html"><a href="lec_02_representation.html#meaning-of-representations-discussion"><i class="fa fa-check"></i><b>2.1.2</b> Meaning of representations (discussion)</a></li></ul></li><li class="chapter" data-level="2.2" data-path="lec_02_representation.html"><a href="lec_02_representation.html#representations-beyond-natural-numbers"><i class="fa fa-check"></i><b>2.2</b> Representations beyond natural numbers</a><ul><li class="chapter" data-level="2.2.1" data-path="lec_02_representation.html"><a href="lec_02_representation.html#repnegativeintegerssec"><i class="fa fa-check"></i><b>2.2.1</b> Representing (potentially negative) integers</a></li><li class="chapter" data-level="2.2.2" data-path="lec_02_representation.html"><a href="lec_02_representation.html#twoscomplement"><i class="fa fa-check"></i><b>2.2.2</b> Two’s complement representation (optional)</a></li><li class="chapter" data-level="2.2.3" data-path="lec_02_representation.html"><a href="lec_02_representation.html#rational-numbers-and-representing-pairs-of-strings"><i class="fa fa-check"></i><b>2.2.3</b> Rational numbers, and representing pairs of strings</a></li></ul></li><li class="chapter" data-level="2.3" data-path="lec_02_representation.html"><a href="lec_02_representation.html#representing-real-numbers"><i class="fa fa-check"></i><b>2.3</b> Representing real numbers</a><ul><li class="chapter" data-level="2.3.1" data-path="lec_02_representation.html"><a href="lec_02_representation.html#cantorsec"><i class="fa fa-check"></i><b>2.3.1</b> Can we represent reals exactly?</a></li></ul></li><li class="chapter" data-level="2.4" data-path="lec_02_representation.html"><a href="lec_02_representation.html#representing-objects-beyond-numbers"><i class="fa fa-check"></i><b>2.4</b> Representing objects beyond numbers</a><ul><li class="chapter" data-level="2.4.1" data-path="lec_02_representation.html"><a href="lec_02_representation.html#finite-representations"><i class="fa fa-check"></i><b>2.4.1</b> Finite representations</a></li><li class="chapter" data-level="2.4.2" data-path="lec_02_representation.html"><a href="lec_02_representation.html#prefixfreesec"><i class="fa fa-check"></i><b>2.4.2</b> Prefix-free encoding</a></li><li class="chapter" data-level="2.4.3" data-path="lec_02_representation.html"><a href="lec_02_representation.html#making-representations-prefix-free"><i class="fa fa-check"></i><b>2.4.3</b> Making representations prefix-free</a></li><li class="chapter" data-level="2.4.4" data-path="lec_02_representation.html"><a href="lec_02_representation.html#proof-by-python-optional"><i class="fa fa-check"></i><b>2.4.4</b> Proof by Python (optional)</a></li><li class="chapter" data-level="2.4.5" data-path="lec_02_representation.html"><a href="lec_02_representation.html#representing-letters-and-text"><i class="fa fa-check"></i><b>2.4.5</b> Representing letters and text</a></li><li class="chapter" data-level="2.4.6" data-path="lec_02_representation.html"><a href="lec_02_representation.html#representing-vectors-matrices-images"><i class="fa fa-check"></i><b>2.4.6</b> Representing vectors, matrices, images</a></li><li class="chapter" data-level="2.4.7" data-path="lec_02_representation.html"><a href="lec_02_representation.html#representing-graphs"><i class="fa fa-check"></i><b>2.4.7</b> Representing graphs</a></li><li class="chapter" data-level="2.4.8" data-path="lec_02_representation.html"><a href="lec_02_representation.html#representing-lists-and-nested-lists"><i class="fa fa-check"></i><b>2.4.8</b> Representing lists and nested lists</a></li><li class="chapter" data-level="2.4.9" data-path="lec_02_representation.html"><a href="lec_02_representation.html#notation"><i class="fa fa-check"></i><b>2.4.9</b> Notation</a></li></ul></li><li class="chapter" data-level="2.5" data-path="lec_02_representation.html"><a href="lec_02_representation.html#defining-computational-tasks-as-mathematical-functions"><i class="fa fa-check"></i><b>2.5</b> Defining computational tasks as mathematical functions</a><ul><li class="chapter" data-level="2.5.1" data-path="lec_02_representation.html"><a href="lec_02_representation.html#secimplvsspec"><i class="fa fa-check"></i><b>2.5.1</b> Distinguish functions from programs!</a></li></ul></li><li class="chapter" data-level="2.6" data-path="lec_02_representation.html"><a href="lec_02_representation.html#exercises"><i class="fa fa-check"></i><b>2.6</b> Exercises</a></li><li class="chapter" data-level="2.7" data-path="lec_02_representation.html"><a href="lec_02_representation.html#bibnotesrepres"><i class="fa fa-check"></i><b>2.7</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="3" data-path="lec_03_computation.html"><a href="lec_03_computation.html"><i class="fa fa-check"></i><b>3</b> Defining computation</a><ul><li class="chapter" data-level="3.1" data-path="lec_03_computation.html"><a href="lec_03_computation.html#defining-computation"><i class="fa fa-check"></i><b>3.1</b> Defining computation</a></li><li class="chapter" data-level="3.2" data-path="lec_03_computation.html"><a href="lec_03_computation.html#computing-using-and-or-and-not."><i class="fa fa-check"></i><b>3.2</b> Computing using AND, OR, and NOT.</a><ul><li class="chapter" data-level="3.2.1" data-path="lec_03_computation.html"><a href="lec_03_computation.html#some-properties-of-and-and-or"><i class="fa fa-check"></i><b>3.2.1</b> Some properties of AND and OR</a></li><li class="chapter" data-level="3.2.2" data-path="lec_03_computation.html"><a href="lec_03_computation.html#xoraonexample"><i class="fa fa-check"></i><b>3.2.2</b> Extended example: Computing \ensuremath{\mathit{XOR}} from \ensuremath{\mathit{AND}}, \ensuremath{\mathit{OR}}, and \ensuremath{\mathit{NOT}}</a></li><li class="chapter" data-level="3.2.3" data-path="lec_03_computation.html"><a href="lec_03_computation.html#informally-defining-basic-operations-and-algorithms"><i class="fa fa-check"></i><b>3.2.3</b> Informally defining basic operations and algorithms</a></li></ul></li><li class="chapter" data-level="3.3" data-path="lec_03_computation.html"><a href="lec_03_computation.html#booleancircuitfig"><i class="fa fa-check"></i><b>3.3</b> Boolean Circuits</a><ul><li class="chapter" data-level="3.3.1" data-path="lec_03_computation.html"><a href="lec_03_computation.html#boolean-circuits-a-formal-definition"><i class="fa fa-check"></i><b>3.3.1</b> Boolean circuits: a formal definition</a></li><li class="chapter" data-level="3.3.2" data-path="lec_03_computation.html"><a href="lec_03_computation.html#equivalence-of-circuits-and-straight-line-programs"><i class="fa fa-check"></i><b>3.3.2</b> Equivalence of circuits and straight-line programs</a></li></ul></li><li class="chapter" data-level="3.4" data-path="lec_03_computation.html"><a href="lec_03_computation.html#physicalimplementationsec"><i class="fa fa-check"></i><b>3.4</b> Physical implementations of computing devices (digression)</a><ul><li class="chapter" data-level="3.4.1" data-path="lec_03_computation.html"><a href="lec_03_computation.html#transistors"><i class="fa fa-check"></i><b>3.4.1</b> Transistors</a></li><li class="chapter" data-level="3.4.2" data-path="lec_03_computation.html"><a href="lec_03_computation.html#logical-gates-from-transistors"><i class="fa fa-check"></i><b>3.4.2</b> Logical gates from transistors</a></li><li class="chapter" data-level="3.4.3" data-path="lec_03_computation.html"><a href="lec_03_computation.html#biological-computing"><i class="fa fa-check"></i><b>3.4.3</b> Biological computing</a></li><li class="chapter" data-level="3.4.4" data-path="lec_03_computation.html"><a href="lec_03_computation.html#cellular-automata-and-the-game-of-life"><i class="fa fa-check"></i><b>3.4.4</b> Cellular automata and the game of life</a></li><li class="chapter" data-level="3.4.5" data-path="lec_03_computation.html"><a href="lec_03_computation.html#neural-networks"><i class="fa fa-check"></i><b>3.4.5</b> Neural networks</a></li><li class="chapter" data-level="3.4.6" data-path="lec_03_computation.html"><a href="lec_03_computation.html#a-computer-made-from-marbles-and-pipes"><i class="fa fa-check"></i><b>3.4.6</b> A computer made from marbles and pipes</a></li></ul></li><li class="chapter" data-level="3.5" data-path="lec_03_computation.html"><a href="lec_03_computation.html#nandsec"><i class="fa fa-check"></i><b>3.5</b> The NAND function</a><ul><li class="chapter" data-level="3.5.1" data-path="lec_03_computation.html"><a href="lec_03_computation.html#nand-circuits"><i class="fa fa-check"></i><b>3.5.1</b> NAND Circuits</a></li><li class="chapter" data-level="3.5.2" data-path="lec_03_computation.html"><a href="lec_03_computation.html#more-examples-of-nand-circuits-optional"><i class="fa fa-check"></i><b>3.5.2</b> More examples of NAND circuits (optional)</a></li><li class="chapter" data-level="3.5.3" data-path="lec_03_computation.html"><a href="lec_03_computation.html#nandcircsec"><i class="fa fa-check"></i><b>3.5.3</b> The NAND-CIRC Programming language</a></li></ul></li><li class="chapter" data-level="3.6" data-path="lec_03_computation.html"><a href="lec_03_computation.html#equivalence-of-all-these-models"><i class="fa fa-check"></i><b>3.6</b> Equivalence of all these models</a><ul><li class="chapter" data-level="3.6.1" data-path="lec_03_computation.html"><a href="lec_03_computation.html#othergatessec"><i class="fa fa-check"></i><b>3.6.1</b> Circuits with other gate sets</a></li><li class="chapter" data-level="3.6.2" data-path="lec_03_computation.html"><a href="lec_03_computation.html#specvsimplrem"><i class="fa fa-check"></i><b>3.6.2</b> Specification vs. implementation (again)</a></li></ul></li><li class="chapter" data-level="3.7" data-path="lec_03_computation.html"><a href="lec_03_computation.html#exercises"><i class="fa fa-check"></i><b>3.7</b> Exercises</a></li><li class="chapter" data-level="3.8" data-path="lec_03_computation.html"><a href="lec_03_computation.html#biographical-notes"><i class="fa fa-check"></i><b>3.8</b> Biographical notes</a></li></ul></li><li class="chapter" data-level="4" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html"><i class="fa fa-check"></i><b>4</b> Syntactic sugar, and computing every function</a><ul><li class="chapter" data-level="4.1" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#secsyntacticsugar"><i class="fa fa-check"></i><b>4.1</b> Some examples of syntactic sugar</a><ul><li class="chapter" data-level="4.1.1" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#user-defined-procedures"><i class="fa fa-check"></i><b>4.1.1</b> User-defined procedures</a></li><li class="chapter" data-level="4.1.2" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#functionsynsugarthmpython"><i class="fa fa-check"></i><b>4.1.2</b> Proof by Python (optional)</a></li><li class="chapter" data-level="4.1.3" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#ifstatementsec"><i class="fa fa-check"></i><b>4.1.3</b> Conditional statements</a></li></ul></li><li class="chapter" data-level="4.2" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#addexample"><i class="fa fa-check"></i><b>4.2</b> Extended example: Addition and Multiplication (optional)</a></li><li class="chapter" data-level="4.3" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#seclookupfunc"><i class="fa fa-check"></i><b>4.3</b> The LOOKUP function</a><ul><li class="chapter" data-level="4.3.1" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#constructing-a-nand-circ-program-for-lookup"><i class="fa fa-check"></i><b>4.3.1</b> Constructing a NAND-CIRC program for \ensuremath{\mathit{LOOKUP}}</a></li></ul></li><li class="chapter" data-level="4.4" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#seccomputeallfunctions"><i class="fa fa-check"></i><b>4.4</b> Computing every function</a><ul><li class="chapter" data-level="4.4.1" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#proof-of-nands-universality"><i class="fa fa-check"></i><b>4.4.1</b> Proof of NAND’s Universality</a></li><li class="chapter" data-level="4.4.2" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#tight-upper-bound"><i class="fa fa-check"></i><b>4.4.2</b> Improving by a factor of n (optional)</a></li></ul></li><li class="chapter" data-level="4.5" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#seccomputalternative"><i class="fa fa-check"></i><b>4.5</b> Computing every function: An alternative proof</a></li><li class="chapter" data-level="4.6" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#secdefinesizeclasses"><i class="fa fa-check"></i><b>4.6</b> The class \ensuremath{\mathit{SIZE}}(T)</a></li><li class="chapter" data-level="4.7" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#exercises"><i class="fa fa-check"></i><b>4.7</b> Exercises</a></li><li class="chapter" data-level="4.8" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#computeeveryfunctionbibnotes"><i class="fa fa-check"></i><b>4.8</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="5" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html"><i class="fa fa-check"></i><b>5</b> Code as data, data as code</a><ul><li class="chapter" data-level="5.1" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#representprogramsec"><i class="fa fa-check"></i><b>5.1</b> Representing programs as strings</a></li><li class="chapter" data-level="5.2" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#countingcircuitsec"><i class="fa fa-check"></i><b>5.2</b> Counting programs, and lower bounds on the size of NAND-CIRC programs</a><ul><li class="chapter" data-level="5.2.1" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#size-hierarchy-theorem-optional"><i class="fa fa-check"></i><b>5.2.1</b> Size hierarchy theorem (optional)</a></li></ul></li><li class="chapter" data-level="5.3" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#listoftuplesrepsec"><i class="fa fa-check"></i><b>5.3</b> The tuples representation</a><ul><li class="chapter" data-level="5.3.1" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#stringrepresentationrpgoramsec"><i class="fa fa-check"></i><b>5.3.1</b> From tuples to strings</a></li></ul></li><li class="chapter" data-level="5.4" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#a-nand-circ-interpreter-in-nand-circ"><i class="fa fa-check"></i><b>5.4</b> A NAND-CIRC interpreter in NAND-CIRC</a><ul><li class="chapter" data-level="5.4.1" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#efficient-universal-programs"><i class="fa fa-check"></i><b>5.4.1</b> Efficient universal programs</a></li><li class="chapter" data-level="5.4.2" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#a-nand-circ-interpeter-in-pseudocode"><i class="fa fa-check"></i><b>5.4.2</b> A NAND-CIRC interpeter in pseudocode</a></li><li class="chapter" data-level="5.4.3" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#nandevalpythonsec"><i class="fa fa-check"></i><b>5.4.3</b> A NAND interpreter in Python</a></li><li class="chapter" data-level="5.4.4" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#constructing-the-nand-circ-interpreter-in-nand-circ"><i class="fa fa-check"></i><b>5.4.4</b> Constructing the NAND-CIRC interpreter in NAND-CIRC</a></li></ul></li><li class="chapter" data-level="5.5" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#a-python-interpreter-in-nand-circ-discussion"><i class="fa fa-check"></i><b>5.5</b> A Python interpreter in NAND-CIRC (discussion)</a></li><li class="chapter" data-level="5.6" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#PECTTsec"><i class="fa fa-check"></i><b>5.6</b> The physical extended Church-Turing thesis (discussion)</a><ul><li class="chapter" data-level="5.6.1" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#attempts-at-refuting-the-pectt"><i class="fa fa-check"></i><b>5.6.1</b> Attempts at refuting the PECTT</a></li></ul></li><li class="chapter" data-level="5.7" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#recap-of-part-i-finite-computation"><i class="fa fa-check"></i><b>5.7</b> Recap of Part I: Finite Computation</a></li><li class="chapter" data-level="5.8" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#exercises"><i class="fa fa-check"></i><b>5.8</b> Exercises</a></li><li class="chapter" data-level="5.9" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#bibnotescodeasdata"><i class="fa fa-check"></i><b>5.9</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="6" data-path="lec_06_loops.html"><a href="lec_06_loops.html"><i class="fa fa-check"></i><b>6</b> Loops and infinity</a><ul><li class="chapter" data-level="6.1" data-path="lec_06_loops.html"><a href="lec_06_loops.html#turing-machines"><i class="fa fa-check"></i><b>6.1</b> Turing Machines</a><ul><li class="chapter" data-level="6.1.1" data-path="lec_06_loops.html"><a href="lec_06_loops.html#turingmachinepalindrome"><i class="fa fa-check"></i><b>6.1.1</b> Extended example: A Turing machine for palindromes</a></li><li class="chapter" data-level="6.1.2" data-path="lec_06_loops.html"><a href="lec_06_loops.html#turing-machines-a-formal-definition"><i class="fa fa-check"></i><b>6.1.2</b> Turing machines: a formal definition</a></li><li class="chapter" data-level="6.1.3" data-path="lec_06_loops.html"><a href="lec_06_loops.html#computable-functions"><i class="fa fa-check"></i><b>6.1.3</b> Computable functions</a></li><li class="chapter" data-level="6.1.4" data-path="lec_06_loops.html"><a href="lec_06_loops.html#infinite-loops-and-partial-functions"><i class="fa fa-check"></i><b>6.1.4</b> Infinite loops and partial functions</a></li></ul></li><li class="chapter" data-level="6.2" data-path="lec_06_loops.html"><a href="lec_06_loops.html#turing-machines-as-programming-languages"><i class="fa fa-check"></i><b>6.2</b> Turing machines as programming languages</a><ul><li class="chapter" data-level="6.2.1" data-path="lec_06_loops.html"><a href="lec_06_loops.html#the-nand-tm-programming-language"><i class="fa fa-check"></i><b>6.2.1</b> The NAND-TM Programming language</a></li><li class="chapter" data-level="6.2.2" data-path="lec_06_loops.html"><a href="lec_06_loops.html#sneak-peak-nand-tm-vs-turing-machines"><i class="fa fa-check"></i><b>6.2.2</b> Sneak peak: NAND-TM vs Turing machines</a></li><li class="chapter" data-level="6.2.3" data-path="lec_06_loops.html"><a href="lec_06_loops.html#examples"><i class="fa fa-check"></i><b>6.2.3</b> Examples</a></li></ul></li><li class="chapter" data-level="6.3" data-path="lec_06_loops.html"><a href="lec_06_loops.html#equivalence-of-turing-machines-and-nand-tm-programs"><i class="fa fa-check"></i><b>6.3</b> Equivalence of Turing machines and NAND-TM programs</a><ul><li class="chapter" data-level="6.3.1" data-path="lec_06_loops.html"><a href="lec_06_loops.html#specification-vs-implementation-again"><i class="fa fa-check"></i><b>6.3.1</b> Specification vs implementation (again)</a></li></ul></li><li class="chapter" data-level="6.4" data-path="lec_06_loops.html"><a href="lec_06_loops.html#nand-tm-syntactic-sugar"><i class="fa fa-check"></i><b>6.4</b> NAND-TM syntactic sugar</a><ul><li class="chapter" data-level="6.4.1" data-path="lec_06_loops.html"><a href="lec_06_loops.html#nandtminnerloopssec"><i class="fa fa-check"></i><b>6.4.1</b> GOTO and inner loops</a></li></ul></li><li class="chapter" data-level="6.5" data-path="lec_06_loops.html"><a href="lec_06_loops.html#uniformity-and-nand-vs-nand-tm-discussion"><i class="fa fa-check"></i><b>6.5</b> Uniformity, and NAND vs NAND-TM (discussion)</a></li><li class="chapter" data-level="6.6" data-path="lec_06_loops.html"><a href="lec_06_loops.html#exercises"><i class="fa fa-check"></i><b>6.6</b> Exercises</a></li><li class="chapter" data-level="6.7" data-path="lec_06_loops.html"><a href="lec_06_loops.html#chaploopnotes"><i class="fa fa-check"></i><b>6.7</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="7" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html"><i class="fa fa-check"></i><b>7</b> Equivalent models of computation</a><ul><li class="chapter" data-level="7.1" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#ram-machines-and-nand-ram"><i class="fa fa-check"></i><b>7.1</b> RAM machines and NAND-RAM</a></li><li class="chapter" data-level="7.2" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#nandtmgorydetailssec"><i class="fa fa-check"></i><b>7.2</b> The gory details (optional)</a><ul><li class="chapter" data-level="7.2.1" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#indexed-access-in-nand-tm"><i class="fa fa-check"></i><b>7.2.1</b> Indexed access in NAND-TM</a></li><li class="chapter" data-level="7.2.2" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#two-dimensional-arrays-in-nand-tm"><i class="fa fa-check"></i><b>7.2.2</b> Two dimensional arrays in NAND-TM</a></li><li class="chapter" data-level="7.2.3" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#all-the-rest"><i class="fa fa-check"></i><b>7.2.3</b> All the rest</a></li></ul></li><li class="chapter" data-level="7.3" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#turing-equivalence-discussion"><i class="fa fa-check"></i><b>7.3</b> Turing equivalence (discussion)</a><ul><li class="chapter" data-level="7.3.1" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#the-best-of-both-worlds-paradigm"><i class="fa fa-check"></i><b>7.3.1</b> The Best of both worlds paradigm</a></li><li class="chapter" data-level="7.3.2" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#lets-talk-about-abstractions."><i class="fa fa-check"></i><b>7.3.2</b> Let’s talk about abstractions.</a></li><li class="chapter" data-level="7.3.3" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#turingcompletesec"><i class="fa fa-check"></i><b>7.3.3</b> Turing completeness and equivalence, a formal definition (optional)</a></li></ul></li><li class="chapter" data-level="7.4" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#cellularautomatasec"><i class="fa fa-check"></i><b>7.4</b> Cellular automata</a><ul><li class="chapter" data-level="7.4.1" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#one-dimensional-cellular-automata-are-turing-complete"><i class="fa fa-check"></i><b>7.4.1</b> One dimensional cellular automata are Turing complete</a></li><li class="chapter" data-level="7.4.2" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#turingmachinesconfigsec"><i class="fa fa-check"></i><b>7.4.2</b> Configurations of Turing machines and the next-step function</a></li></ul></li><li class="chapter" data-level="7.5" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#lambdacalculussec"><i class="fa fa-check"></i><b>7.5</b> Lambda calculus and functional programming languages</a><ul><li class="chapter" data-level="7.5.1" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#applying-functions-to-functions"><i class="fa fa-check"></i><b>7.5.1</b> Applying functions to functions</a></li><li class="chapter" data-level="7.5.2" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#curryingsec"><i class="fa fa-check"></i><b>7.5.2</b> Obtaining multi-argument functions via Currying</a></li><li class="chapter" data-level="7.5.3" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#formal-description-of-the-λ-calculus."><i class="fa fa-check"></i><b>7.5.3</b> Formal description of the λ calculus.</a></li><li class="chapter" data-level="7.5.4" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#infiniteloopslambda"><i class="fa fa-check"></i><b>7.5.4</b> Infinite loops in the λ calculus</a></li></ul></li><li class="chapter" data-level="7.6" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#the-enhanced-λ-calculus"><i class="fa fa-check"></i><b>7.6</b> The Enhanced λ calculus</a><ul><li class="chapter" data-level="7.6.1" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#computing-a-function-in-the-enhanced-λ-calculus"><i class="fa fa-check"></i><b>7.6.1</b> Computing a function in the enhanced λ calculus</a></li><li class="chapter" data-level="7.6.2" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#enhanced-λ-calculus-is-turing-complete"><i class="fa fa-check"></i><b>7.6.2</b> Enhanced λ calculus is Turing-complete</a></li></ul></li><li class="chapter" data-level="7.7" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#lambdacacluluspuresec"><i class="fa fa-check"></i><b>7.7</b> From enhanced to pure λ calculus</a><ul><li class="chapter" data-level="7.7.1" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#list-processing"><i class="fa fa-check"></i><b>7.7.1</b> List processing</a></li><li class="chapter" data-level="7.7.2" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#ycombinatorsec"><i class="fa fa-check"></i><b>7.7.2</b> The Y combinator, or recursion without recursion</a></li></ul></li><li class="chapter" data-level="7.8" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#churchturingdiscussionsec"><i class="fa fa-check"></i><b>7.8</b> The Church-Turing Thesis (discussion)</a><ul><li class="chapter" data-level="7.8.1" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#different-models-of-computation"><i class="fa fa-check"></i><b>7.8.1</b> Different models of computation</a></li></ul></li><li class="chapter" data-level="7.9" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#exercises"><i class="fa fa-check"></i><b>7.9</b> Exercises</a></li><li class="chapter" data-level="7.10" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#othermodelsbibnotes"><i class="fa fa-check"></i><b>7.10</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="8" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html"><i class="fa fa-check"></i><b>8</b> Universality and uncomputability</a><ul><li class="chapter" data-level="8.1" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#universality-or-a-meta-circular-evaluator"><i class="fa fa-check"></i><b>8.1</b> Universality or a meta-circular evaluator</a><ul><li class="chapter" data-level="8.1.1" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#representtmsec"><i class="fa fa-check"></i><b>8.1.1</b> Proving the existence of a universal Turing Machine</a></li><li class="chapter" data-level="8.1.2" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#implications-of-universality-discussion"><i class="fa fa-check"></i><b>8.1.2</b> Implications of universality (discussion)</a></li></ul></li><li class="chapter" data-level="8.2" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#is-every-function-computable"><i class="fa fa-check"></i><b>8.2</b> Is every function computable?</a></li><li class="chapter" data-level="8.3" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#haltingsec"><i class="fa fa-check"></i><b>8.3</b> The Halting problem</a><ul><li class="chapter" data-level="8.3.1" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#is-the-halting-problem-really-hard-discussion"><i class="fa fa-check"></i><b>8.3.1</b> Is the Halting problem really hard? (discussion)</a></li><li class="chapter" data-level="8.3.2" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#haltalternativesec"><i class="fa fa-check"></i><b>8.3.2</b> A direct proof of the uncomputability of \ensuremath{\mathit{HALT}} (optional)</a></li></ul></li><li class="chapter" data-level="8.4" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#reductionsuncompsec"><i class="fa fa-check"></i><b>8.4</b> Reductions</a><ul><li class="chapter" data-level="8.4.1" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#example-halting-on-the-zero-problem"><i class="fa fa-check"></i><b>8.4.1</b> Example: Halting on the zero problem</a></li></ul></li><li class="chapter" data-level="8.5" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#rices-theorem-and-the-impossibility-of-general-software-verification"><i class="fa fa-check"></i><b>8.5</b> Rice’s Theorem and the impossibility of general software verification</a><ul><li class="chapter" data-level="8.5.1" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#ricethmsec"><i class="fa fa-check"></i><b>8.5.1</b> Rice’s Theorem</a></li><li class="chapter" data-level="8.5.2" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#halting-and-rices-theorem-for-other-turing-complete-models"><i class="fa fa-check"></i><b>8.5.2</b> Halting and Rice’s Theorem for other Turing-complete models</a></li><li class="chapter" data-level="8.5.3" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#is-software-verification-doomed-discussion"><i class="fa fa-check"></i><b>8.5.3</b> Is software verification doomed? (discussion)</a></li></ul></li><li class="chapter" data-level="8.6" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#exercises"><i class="fa fa-check"></i><b>8.6</b> Exercises</a></li><li class="chapter" data-level="8.7" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#uncomputablebibnotes"><i class="fa fa-check"></i><b>8.7</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="9" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html"><i class="fa fa-check"></i><b>9</b> Restricted computational models</a><ul><li class="chapter" data-level="9.1" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#turing-completeness-as-a-bug"><i class="fa fa-check"></i><b>9.1</b> Turing completeness as a bug</a></li><li class="chapter" data-level="9.2" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#regular-expressions"><i class="fa fa-check"></i><b>9.2</b> Regular expressions</a></li><li class="chapter" data-level="9.3" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#deterministic-finite-automata-and-efficient-matching-of-regular-expressions-optional"><i class="fa fa-check"></i><b>9.3</b> Deterministic finite automata, and efficient matching of regular expressions (optional)</a><ul><li class="chapter" data-level="9.3.1" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#matching-regular-expressions-using-constant-memory"><i class="fa fa-check"></i><b>9.3.1</b> Matching regular expressions using constant memory</a></li><li class="chapter" data-level="9.3.2" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#secdfa"><i class="fa fa-check"></i><b>9.3.2</b> Deterministic Finite Automata</a></li><li class="chapter" data-level="9.3.3" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#regular-functions-are-closed-under-complement"><i class="fa fa-check"></i><b>9.3.3</b> Regular functions are closed under complement</a></li></ul></li><li class="chapter" data-level="9.4" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#limitations-of-regular-expressions"><i class="fa fa-check"></i><b>9.4</b> Limitations of regular expressions</a></li><li class="chapter" data-level="9.5" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#other-semantic-properties-of-regular-expressions"><i class="fa fa-check"></i><b>9.5</b> Other semantic properties of regular expressions</a></li><li class="chapter" data-level="9.6" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#seccfg"><i class="fa fa-check"></i><b>9.6</b> Context free grammars</a><ul><li class="chapter" data-level="9.6.1" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#context-free-grammars-as-a-computational-model"><i class="fa fa-check"></i><b>9.6.1</b> Context-free grammars as a computational model</a></li><li class="chapter" data-level="9.6.2" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#the-power-of-context-free-grammars"><i class="fa fa-check"></i><b>9.6.2</b> The power of context free grammars</a></li><li class="chapter" data-level="9.6.3" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#limitations-of-context-free-grammars-optional"><i class="fa fa-check"></i><b>9.6.3</b> Limitations of context-free grammars (optional)</a></li></ul></li><li class="chapter" data-level="9.7" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#semantic-properties-of-context-free-languages"><i class="fa fa-check"></i><b>9.7</b> Semantic properties of context free languages</a><ul><li class="chapter" data-level="9.7.1" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#uncomputability-of-context-free-grammar-equivalence-optional"><i class="fa fa-check"></i><b>9.7.1</b> Uncomputability of context-free grammar equivalence (optional)</a></li></ul></li><li class="chapter" data-level="9.8" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#summary-of-semantic-properties-for-regular-expressions-and-context-free-grammars"><i class="fa fa-check"></i><b>9.8</b> Summary of semantic properties for regular expressions and context-free grammars</a></li><li class="chapter" data-level="9.9" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#exercises"><i class="fa fa-check"></i><b>9.9</b> Exercises</a></li><li class="chapter" data-level="9.10" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#bibliographical-notes"><i class="fa fa-check"></i><b>9.10</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="10" data-path="lec_09_godel.html"><a href="lec_09_godel.html"><i class="fa fa-check"></i><b>10</b> Is every theorem provable?</a><ul><li class="chapter" data-level="10.1" data-path="lec_09_godel.html"><a href="lec_09_godel.html#godelproofdef"><i class="fa fa-check"></i><b>10.1</b> Hilbert’s Program and Gödel’s Incompleteness Theorem</a><ul><li class="chapter" data-level="10.1.1" data-path="lec_09_godel.html"><a href="lec_09_godel.html#godelproofsystemssec"><i class="fa fa-check"></i><b>10.1.1</b> Defining Proof Systems</a></li></ul></li><li class="chapter" data-level="10.2" data-path="lec_09_godel.html"><a href="lec_09_godel.html#gödels-incompleteness-theorem-computational-variant"><i class="fa fa-check"></i><b>10.2</b> Gödel’s Incompleteness Theorem: Computational variant</a></li><li class="chapter" data-level="10.3" data-path="lec_09_godel.html"><a href="lec_09_godel.html#quantified-integer-statements"><i class="fa fa-check"></i><b>10.3</b> Quantified integer statements</a></li><li class="chapter" data-level="10.4" data-path="lec_09_godel.html"><a href="lec_09_godel.html#diophantine-equations-and-the-mrdp-theorem"><i class="fa fa-check"></i><b>10.4</b> Diophantine equations and the MRDP Theorem</a></li><li class="chapter" data-level="10.5" data-path="lec_09_godel.html"><a href="lec_09_godel.html#hardness-of-quantified-integer-statements"><i class="fa fa-check"></i><b>10.5</b> Hardness of quantified integer statements</a><ul><li class="chapter" data-level="10.5.1" data-path="lec_09_godel.html"><a href="lec_09_godel.html#step-1-quantified-mixed-statements-and-computation-histories"><i class="fa fa-check"></i><b>10.5.1</b> Step 1: Quantified mixed statements and computation histories</a></li><li class="chapter" data-level="10.5.2" data-path="lec_09_godel.html"><a href="lec_09_godel.html#step-2-reducing-mixed-statements-to-integer-statements"><i class="fa fa-check"></i><b>10.5.2</b> Step 2: Reducing mixed statements to integer statements</a></li></ul></li><li class="chapter" data-level="10.6" data-path="lec_09_godel.html"><a href="lec_09_godel.html#exercises"><i class="fa fa-check"></i><b>10.6</b> Exercises</a></li><li class="chapter" data-level="10.7" data-path="lec_09_godel.html"><a href="lec_09_godel.html#bibliographical-notes"><i class="fa fa-check"></i><b>10.7</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="11" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html"><i class="fa fa-check"></i><b>11</b> Efficient computation</a><ul><li class="chapter" data-level="11.1" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#problems-on-graphs"><i class="fa fa-check"></i><b>11.1</b> Problems on graphs</a><ul><li class="chapter" data-level="11.1.1" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#finding-the-shortest-path-in-a-graph"><i class="fa fa-check"></i><b>11.1.1</b> Finding the shortest path in a graph</a></li><li class="chapter" data-level="11.1.2" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#finding-the-longest-path-in-a-graph"><i class="fa fa-check"></i><b>11.1.2</b> Finding the longest path in a graph</a></li><li class="chapter" data-level="11.1.3" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#mincutsec"><i class="fa fa-check"></i><b>11.1.3</b> Finding the minimum cut in a graph</a></li><li class="chapter" data-level="11.1.4" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#linerprogsec"><i class="fa fa-check"></i><b>11.1.4</b> Min-Cut Max-Flow and Linear programming</a></li><li class="chapter" data-level="11.1.5" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#finding-the-maximum-cut-in-a-graph"><i class="fa fa-check"></i><b>11.1.5</b> Finding the maximum cut in a graph</a></li><li class="chapter" data-level="11.1.6" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#a-note-on-convexity"><i class="fa fa-check"></i><b>11.1.6</b> A note on convexity</a></li></ul></li><li class="chapter" data-level="11.2" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#beyond-graphs"><i class="fa fa-check"></i><b>11.2</b> Beyond graphs</a><ul><li class="chapter" data-level="11.2.1" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#sat"><i class="fa fa-check"></i><b>11.2.1</b> SAT</a></li><li class="chapter" data-level="11.2.2" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#solving-linear-equations"><i class="fa fa-check"></i><b>11.2.2</b> Solving linear equations</a></li><li class="chapter" data-level="11.2.3" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#solving-quadratic-equations"><i class="fa fa-check"></i><b>11.2.3</b> Solving quadratic equations</a></li></ul></li><li class="chapter" data-level="11.3" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#more-advanced-examples"><i class="fa fa-check"></i><b>11.3</b> More advanced examples</a><ul><li class="chapter" data-level="11.3.1" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#determinant-of-a-matrix"><i class="fa fa-check"></i><b>11.3.1</b> Determinant of a matrix</a></li><li class="chapter" data-level="11.3.2" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#permanent-of-a-matrix"><i class="fa fa-check"></i><b>11.3.2</b> Permanent of a matrix</a></li><li class="chapter" data-level="11.3.3" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#finding-a-zero-sum-equilibrium"><i class="fa fa-check"></i><b>11.3.3</b> Finding a zero-sum equilibrium</a></li><li class="chapter" data-level="11.3.4" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#finding-a-nash-equilibrium"><i class="fa fa-check"></i><b>11.3.4</b> Finding a Nash equilibrium</a></li><li class="chapter" data-level="11.3.5" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#primality-testing"><i class="fa fa-check"></i><b>11.3.5</b> Primality testing</a></li><li class="chapter" data-level="11.3.6" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#integer-factoring"><i class="fa fa-check"></i><b>11.3.6</b> Integer factoring</a></li></ul></li><li class="chapter" data-level="11.4" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#our-current-knowledge"><i class="fa fa-check"></i><b>11.4</b> Our current knowledge</a></li><li class="chapter" data-level="11.5" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#exercises"><i class="fa fa-check"></i><b>11.5</b> Exercises</a></li><li class="chapter" data-level="11.6" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#effalgnotes"><i class="fa fa-check"></i><b>11.6</b> Bibliographical notes</a></li><li class="chapter" data-level="11.7" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#further-explorations"><i class="fa fa-check"></i><b>11.7</b> Further explorations</a></li></ul></li><li class="chapter" data-level="12" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html"><i class="fa fa-check"></i><b>12</b> Modeling running time</a><ul><li class="chapter" data-level="12.1" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#formally-defining-running-time"><i class="fa fa-check"></i><b>12.1</b> Formally defining running time</a><ul><li class="chapter" data-level="12.1.1" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#polynomial-and-exponential-time"><i class="fa fa-check"></i><b>12.1.1</b> Polynomial and Exponential Time</a></li></ul></li><li class="chapter" data-level="12.2" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#modeling-running-time-using-ram-machines-nand-ram"><i class="fa fa-check"></i><b>12.2</b> Modeling running time using RAM Machines / NAND-RAM</a></li><li class="chapter" data-level="12.3" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#ECTTsec"><i class="fa fa-check"></i><b>12.3</b> Extended Church-Turing Thesis (discussion)</a></li><li class="chapter" data-level="12.4" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#efficient-universal-machine-a-nand-ram-interpreter-in-nand-ram"><i class="fa fa-check"></i><b>12.4</b> Efficient universal machine: a NAND-RAM interpreter in NAND-RAM</a><ul><li class="chapter" data-level="12.4.1" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#timed-universal-turing-machine"><i class="fa fa-check"></i><b>12.4.1</b> Timed Universal Turing Machine</a></li></ul></li><li class="chapter" data-level="12.5" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#the-time-hierarchy-theorem"><i class="fa fa-check"></i><b>12.5</b> The time hierarchy theorem</a></li><li class="chapter" data-level="12.6" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#nonuniformcompsec"><i class="fa fa-check"></i><b>12.6</b> Non uniform computation</a><ul><li class="chapter" data-level="12.6.1" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#obliviousnandtm"><i class="fa fa-check"></i><b>12.6.1</b> Oblivious NAND-TM programs</a></li><li class="chapter" data-level="12.6.2" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#unrollloopsec"><i class="fa fa-check"></i><b>12.6.2</b> Unrolling the loop: algorithmic transformation of Turing Machines to circuits</a></li><li class="chapter" data-level="12.6.3" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#can-uniform-algorithms-simulate-non-uniform-ones"><i class="fa fa-check"></i><b>12.6.3</b> Can uniform algorithms simulate non uniform ones?</a></li><li class="chapter" data-level="12.6.4" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#uniform-vs.-nonuniform-computation-a-recap"><i class="fa fa-check"></i><b>12.6.4</b> Uniform vs. Nonuniform computation: A recap</a></li></ul></li><li class="chapter" data-level="12.7" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#exercises"><i class="fa fa-check"></i><b>12.7</b> Exercises</a></li><li class="chapter" data-level="12.8" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#bibnotesrunningtime"><i class="fa fa-check"></i><b>12.8</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="13" data-path="lec_12_NP.html"><a href="lec_12_NP.html"><i class="fa fa-check"></i><b>13</b> Polynomial-time reductions</a><ul><li class="chapter" data-level="13.1" data-path="lec_12_NP.html"><a href="lec_12_NP.html#formaldefdecisionexamplessec"><i class="fa fa-check"></i><b>13.1</b> Formal definitions of problems</a></li><li class="chapter" data-level="13.2" data-path="lec_12_NP.html"><a href="lec_12_NP.html#polytimeredsec"><i class="fa fa-check"></i><b>13.2</b> Polynomial-time reductions</a></li><li class="chapter" data-level="13.3" data-path="lec_12_NP.html"><a href="lec_12_NP.html#reducing-3sat-to-zero-one-equations"><i class="fa fa-check"></i><b>13.3</b> Reducing 3SAT to zero one equations</a><ul><li class="chapter" data-level="13.3.1" data-path="lec_12_NP.html"><a href="lec_12_NP.html#quadratic-equations"><i class="fa fa-check"></i><b>13.3.1</b> Quadratic equations</a></li></ul></li><li class="chapter" data-level="13.4" data-path="lec_12_NP.html"><a href="lec_12_NP.html#the-independent-set-problem"><i class="fa fa-check"></i><b>13.4</b> The independent set problem</a></li><li class="chapter" data-level="13.5" data-path="lec_12_NP.html"><a href="lec_12_NP.html#reducing-independent-set-to-maximum-cut"><i class="fa fa-check"></i><b>13.5</b> Reducing Independent Set to Maximum Cut</a></li><li class="chapter" data-level="13.6" data-path="lec_12_NP.html"><a href="lec_12_NP.html#reducing-3sat-to-longest-path"><i class="fa fa-check"></i><b>13.6</b> Reducing 3SAT to Longest Path</a><ul><li class="chapter" data-level="13.6.1" data-path="lec_12_NP.html"><a href="lec_12_NP.html#summary-of-relations"><i class="fa fa-check"></i><b>13.6.1</b> Summary of relations</a></li></ul></li><li class="chapter" data-level="13.7" data-path="lec_12_NP.html"><a href="lec_12_NP.html#exercises"><i class="fa fa-check"></i><b>13.7</b> Exercises</a></li><li class="chapter" data-level="13.8" data-path="lec_12_NP.html"><a href="lec_12_NP.html#reductionsbibnotes"><i class="fa fa-check"></i><b>13.8</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="14" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html"><i class="fa fa-check"></i><b>14</b> NP, NP completeness, and the Cook-Levin Theorem</a><ul><li class="chapter" data-level="14.1" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#the-class-mathbfnp"><i class="fa fa-check"></i><b>14.1</b> The class \mathbf{NP}</a><ul><li class="chapter" data-level="14.1.1" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#examples-of-functions-in-mathbfnp"><i class="fa fa-check"></i><b>14.1.1</b> Examples of functions in \mathbf{NP}</a></li><li class="chapter" data-level="14.1.2" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#basic-facts-about-mathbfnp"><i class="fa fa-check"></i><b>14.1.2</b> Basic facts about \mathbf{NP}</a></li></ul></li><li class="chapter" data-level="14.2" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#from-mathbfnp-to-3sat-the-cook-levin-theorem"><i class="fa fa-check"></i><b>14.2</b> From \mathbf{NP} to 3SAT: The Cook-Levin Theorem</a><ul><li class="chapter" data-level="14.2.1" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#what-does-this-mean"><i class="fa fa-check"></i><b>14.2.1</b> What does this mean?</a></li><li class="chapter" data-level="14.2.2" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#the-cook-levin-theorem-proof-outline"><i class="fa fa-check"></i><b>14.2.2</b> The Cook-Levin Theorem: Proof outline</a></li></ul></li><li class="chapter" data-level="14.3" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#the-nandsat-problem-and-why-it-is-mathbfnp-hard."><i class="fa fa-check"></i><b>14.3</b> The \ensuremath{\mathit{NANDSAT}} Problem, and why it is \mathbf{NP} hard.</a></li><li class="chapter" data-level="14.4" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#the-3nand-problem"><i class="fa fa-check"></i><b>14.4</b> The 3\ensuremath{\mathit{NAND}} problem</a></li><li class="chapter" data-level="14.5" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#from-3nand-to-3sat"><i class="fa fa-check"></i><b>14.5</b> From 3\ensuremath{\mathit{NAND}} to 3\ensuremath{\mathit{SAT}}</a></li><li class="chapter" data-level="14.6" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#wrapping-up"><i class="fa fa-check"></i><b>14.6</b> Wrapping up</a></li><li class="chapter" data-level="14.7" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#exercises"><i class="fa fa-check"></i><b>14.7</b> Exercises</a></li><li class="chapter" data-level="14.8" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#bibliographical-notes"><i class="fa fa-check"></i><b>14.8</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="15" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html"><i class="fa fa-check"></i><b>15</b> What if P equals NP?</a><ul><li class="chapter" data-level="15.1" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#search-to-decision-reduction"><i class="fa fa-check"></i><b>15.1</b> Search-to-decision reduction</a></li><li class="chapter" data-level="15.2" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#optimizationsection"><i class="fa fa-check"></i><b>15.2</b> Optimization</a><ul><li class="chapter" data-level="15.2.1" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#example-supervised-learning"><i class="fa fa-check"></i><b>15.2.1</b> Example: Supervised learning</a></li><li class="chapter" data-level="15.2.2" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#example-breaking-cryptosystems"><i class="fa fa-check"></i><b>15.2.2</b> Example: Breaking cryptosystems</a></li></ul></li><li class="chapter" data-level="15.3" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#finding-mathematical-proofs"><i class="fa fa-check"></i><b>15.3</b> Finding mathematical proofs</a></li><li class="chapter" data-level="15.4" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#quantifier-elimination-advanced"><i class="fa fa-check"></i><b>15.4</b> Quantifier elimination (advanced)</a><ul><li class="chapter" data-level="15.4.1" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#selfimprovingsat"><i class="fa fa-check"></i><b>15.4.1</b> Application: self improving algorithm for 3\ensuremath{\mathit{SAT}}</a></li></ul></li><li class="chapter" data-level="15.5" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#approximating-counting-problems-and-posterior-sampling-advanced-optional"><i class="fa fa-check"></i><b>15.5</b> Approximating counting problems and posterior sampling (advanced, optional)</a></li><li class="chapter" data-level="15.6" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#what-does-all-of-this-imply"><i class="fa fa-check"></i><b>15.6</b> What does all of this imply?</a></li><li class="chapter" data-level="15.7" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#can-mathbfp-neq-mathbfnp-be-neither-true-nor-false"><i class="fa fa-check"></i><b>15.7</b> Can \mathbf{P} \neq \mathbf{NP} be neither true nor false?</a></li><li class="chapter" data-level="15.8" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#is-mathbfpmathbfnp-in-practice"><i class="fa fa-check"></i><b>15.8</b> Is \mathbf{P}=\mathbf{NP} in practice?</a></li><li class="chapter" data-level="15.9" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#what-if-mathbfp-neq-mathbfnp"><i class="fa fa-check"></i><b>15.9</b> What if \mathbf{P} \neq \mathbf{NP}?</a></li><li class="chapter" data-level="15.10" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#exercises"><i class="fa fa-check"></i><b>15.10</b> Exercises</a></li><li class="chapter" data-level="15.11" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#bibliographical-notes"><i class="fa fa-check"></i><b>15.11</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="16" data-path="lec_14a_space_complexity.html"><a href="lec_14a_space_complexity.html"><i class="fa fa-check"></i><b>16</b> Space bounded computation</a><ul><li class="chapter" data-level="16.1" data-path="lec_14a_space_complexity.html"><a href="lec_14a_space_complexity.html#lecture-summary"><i class="fa fa-check"></i><b>16.1</b> Lecture summary</a></li><li class="chapter" data-level="16.2" data-path="lec_14a_space_complexity.html"><a href="lec_14a_space_complexity.html#exercises"><i class="fa fa-check"></i><b>16.2</b> Exercises</a></li><li class="chapter" data-level="16.3" data-path="lec_14a_space_complexity.html"><a href="lec_14a_space_complexity.html#bibliographical-notes"><i class="fa fa-check"></i><b>16.3</b> Bibliographical notes</a></li><li class="chapter" data-level="16.4" data-path="lec_14a_space_complexity.html"><a href="lec_14a_space_complexity.html#further-explorations"><i class="fa fa-check"></i><b>16.4</b> Further explorations</a></li><li class="chapter" data-level="16.5" data-path="lec_14a_space_complexity.html"><a href="lec_14a_space_complexity.html#acknowledgements"><i class="fa fa-check"></i><b>16.5</b> Acknowledgements</a></li></ul></li><li class="chapter" data-level="17" data-path="lec_15_probability.html"><a href="lec_15_probability.html"><i class="fa fa-check"></i><b>17</b> Probability Theory 101</a><ul><li class="chapter" data-level="17.1" data-path="lec_15_probability.html"><a href="lec_15_probability.html#random-coins"><i class="fa fa-check"></i><b>17.1</b> Random coins</a><ul><li class="chapter" data-level="17.1.1" data-path="lec_15_probability.html"><a href="lec_15_probability.html#random-variables"><i class="fa fa-check"></i><b>17.1.1</b> Random variables</a></li><li class="chapter" data-level="17.1.2" data-path="lec_15_probability.html"><a href="lec_15_probability.html#distributions-over-strings"><i class="fa fa-check"></i><b>17.1.2</b> Distributions over strings</a></li><li class="chapter" data-level="17.1.3" data-path="lec_15_probability.html"><a href="lec_15_probability.html#more-general-sample-spaces."><i class="fa fa-check"></i><b>17.1.3</b> More general sample spaces.</a></li></ul></li><li class="chapter" data-level="17.2" data-path="lec_15_probability.html"><a href="lec_15_probability.html#correlations-and-independence"><i class="fa fa-check"></i><b>17.2</b> Correlations and independence</a><ul><li class="chapter" data-level="17.2.1" data-path="lec_15_probability.html"><a href="lec_15_probability.html#independent-random-variables"><i class="fa fa-check"></i><b>17.2.1</b> Independent random variables</a></li><li class="chapter" data-level="17.2.2" data-path="lec_15_probability.html"><a href="lec_15_probability.html#collections-of-independent-random-variables."><i class="fa fa-check"></i><b>17.2.2</b> Collections of independent random variables.</a></li></ul></li><li class="chapter" data-level="17.3" data-path="lec_15_probability.html"><a href="lec_15_probability.html#concentration-and-tail-bounds"><i class="fa fa-check"></i><b>17.3</b> Concentration and tail bounds</a><ul><li class="chapter" data-level="17.3.1" data-path="lec_15_probability.html"><a href="lec_15_probability.html#chebyshevs-inequality"><i class="fa fa-check"></i><b>17.3.1</b> Chebyshev’s Inequality</a></li><li class="chapter" data-level="17.3.2" data-path="lec_15_probability.html"><a href="lec_15_probability.html#the-chernoff-bound"><i class="fa fa-check"></i><b>17.3.2</b> The Chernoff bound</a></li></ul></li><li class="chapter" data-level="17.4" data-path="lec_15_probability.html"><a href="lec_15_probability.html#exercises"><i class="fa fa-check"></i><b>17.4</b> Exercises</a></li><li class="chapter" data-level="17.5" data-path="lec_15_probability.html"><a href="lec_15_probability.html#bibliographical-notes"><i class="fa fa-check"></i><b>17.5</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="18" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html"><i class="fa fa-check"></i><b>18</b> Probabilistic computation</a><ul><li class="chapter" data-level="18.1" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html#finding-approximately-good-maximum-cuts."><i class="fa fa-check"></i><b>18.1</b> Finding approximately good maximum cuts.</a><ul><li class="chapter" data-level="18.1.1" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html#amplifying-the-success-of-randomized-algorithms"><i class="fa fa-check"></i><b>18.1.1</b> Amplifying the success of randomized algorithms</a></li><li class="chapter" data-level="18.1.2" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html#success-amplification"><i class="fa fa-check"></i><b>18.1.2</b> Success amplification</a></li><li class="chapter" data-level="18.1.3" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html#two-sided-amplification"><i class="fa fa-check"></i><b>18.1.3</b> Two-sided amplification</a></li><li class="chapter" data-level="18.1.4" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html#what-does-this-mean"><i class="fa fa-check"></i><b>18.1.4</b> What does this mean?</a></li><li class="chapter" data-level="18.1.5" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html#solving-sat-through-randomization"><i class="fa fa-check"></i><b>18.1.5</b> Solving SAT through randomization</a></li><li class="chapter" data-level="18.1.6" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html#bipartite-matching."><i class="fa fa-check"></i><b>18.1.6</b> Bipartite matching.</a></li></ul></li><li class="chapter" data-level="18.2" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html#exercises"><i class="fa fa-check"></i><b>18.2</b> Exercises</a></li><li class="chapter" data-level="18.3" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html#bibliographical-notes"><i class="fa fa-check"></i><b>18.3</b> Bibliographical notes</a></li><li class="chapter" data-level="18.4" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html#acknowledgements"><i class="fa fa-check"></i><b>18.4</b> Acknowledgements</a></li></ul></li><li class="chapter" data-level="19" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html"><i class="fa fa-check"></i><b>19</b> Modeling randomized computation</a><ul><li class="chapter" data-level="19.1" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#modeling-randomized-computation"><i class="fa fa-check"></i><b>19.1</b> Modeling randomized computation</a><ul><li class="chapter" data-level="19.1.1" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#an-alternative-view-random-coins-as-an-extra-input"><i class="fa fa-check"></i><b>19.1.1</b> An alternative view: random coins as an extra input</a></li><li class="chapter" data-level="19.1.2" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#successamptwosided"><i class="fa fa-check"></i><b>19.1.2</b> Success amplification of two-sided error algorithms</a></li></ul></li><li class="chapter" data-level="19.2" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#mathbfbpp-and-mathbfnp-completeness"><i class="fa fa-check"></i><b>19.2</b> \mathbf{BPP} and \mathbf{NP} completeness</a></li><li class="chapter" data-level="19.3" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#the-power-of-randomization"><i class="fa fa-check"></i><b>19.3</b> The power of randomization</a><ul><li class="chapter" data-level="19.3.1" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#solving-mathbfbpp-in-exponential-time"><i class="fa fa-check"></i><b>19.3.1</b> Solving \mathbf{BPP} in exponential time</a></li><li class="chapter" data-level="19.3.2" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#simulating-randomized-algorithms-by-circuits"><i class="fa fa-check"></i><b>19.3.2</b> Simulating randomized algorithms by circuits</a></li></ul></li><li class="chapter" data-level="19.4" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#derandomization"><i class="fa fa-check"></i><b>19.4</b> Derandomization</a><ul><li class="chapter" data-level="19.4.1" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#pseudorandom-generators"><i class="fa fa-check"></i><b>19.4.1</b> Pseudorandom generators</a></li><li class="chapter" data-level="19.4.2" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#optimalprgconj"><i class="fa fa-check"></i><b>19.4.2</b> From existence to constructivity</a></li><li class="chapter" data-level="19.4.3" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#usefulness-of-pseudorandom-generators"><i class="fa fa-check"></i><b>19.4.3</b> Usefulness of pseudorandom generators</a></li></ul></li><li class="chapter" data-level="19.5" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#mathbfpmathbfnp-and-mathbfbpp-vs-mathbfp"><i class="fa fa-check"></i><b>19.5</b> \mathbf{P}=\mathbf{NP} and \mathbf{BPP} vs \mathbf{P}</a></li><li class="chapter" data-level="19.6" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#non-constructive-existence-of-pseudorandom-generators-advanced-optional"><i class="fa fa-check"></i><b>19.6</b> Non-constructive existence of pseudorandom generators (advanced, optional)</a></li><li class="chapter" data-level="19.7" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#exercises"><i class="fa fa-check"></i><b>19.7</b> Exercises</a></li><li class="chapter" data-level="19.8" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#modelrandbibnotes"><i class="fa fa-check"></i><b>19.8</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="20" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html"><i class="fa fa-check"></i><b>20</b> Cryptography</a><ul><li class="chapter" data-level="20.1" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#classical-cryptosystems"><i class="fa fa-check"></i><b>20.1</b> Classical cryptosystems</a></li><li class="chapter" data-level="20.2" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#defining-encryption"><i class="fa fa-check"></i><b>20.2</b> Defining encryption</a></li><li class="chapter" data-level="20.3" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#defining-security-of-encryption"><i class="fa fa-check"></i><b>20.3</b> Defining security of encryption</a></li><li class="chapter" data-level="20.4" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#perfect-secrecy"><i class="fa fa-check"></i><b>20.4</b> Perfect secrecy</a><ul><li class="chapter" data-level="20.4.1" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#example-perfect-secrecy-in-the-battlefield"><i class="fa fa-check"></i><b>20.4.1</b> Example: Perfect secrecy in the battlefield</a></li><li class="chapter" data-level="20.4.2" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#constructing-perfectly-secret-encryption"><i class="fa fa-check"></i><b>20.4.2</b> Constructing perfectly secret encryption</a></li></ul></li><li class="chapter" data-level="20.5" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#necessity-of-long-keys"><i class="fa fa-check"></i><b>20.5</b> Necessity of long keys</a></li><li class="chapter" data-level="20.6" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#computational-secrecy"><i class="fa fa-check"></i><b>20.6</b> Computational secrecy</a><ul><li class="chapter" data-level="20.6.1" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#stream-ciphers-or-the-derandomized-one-time-pad"><i class="fa fa-check"></i><b>20.6.1</b> Stream ciphers or the derandomized one-time pad</a></li></ul></li><li class="chapter" data-level="20.7" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#computational-secrecy-and-mathbfnp"><i class="fa fa-check"></i><b>20.7</b> Computational secrecy and \mathbf{NP}</a></li><li class="chapter" data-level="20.8" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#public-key-cryptography"><i class="fa fa-check"></i><b>20.8</b> Public key cryptography</a><ul><li class="chapter" data-level="20.8.1" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#defining-public-key-encryption"><i class="fa fa-check"></i><b>20.8.1</b> Defining public key encryption</a></li><li class="chapter" data-level="20.8.2" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#diffie-hellman-key-exchange"><i class="fa fa-check"></i><b>20.8.2</b> Diffie-Hellman key exchange</a></li></ul></li><li class="chapter" data-level="20.9" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#other-security-notions"><i class="fa fa-check"></i><b>20.9</b> Other security notions</a></li><li class="chapter" data-level="20.10" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#magic"><i class="fa fa-check"></i><b>20.10</b> Magic</a><ul><li class="chapter" data-level="20.10.1" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#zero-knowledge-proofs"><i class="fa fa-check"></i><b>20.10.1</b> Zero knowledge proofs</a></li><li class="chapter" data-level="20.10.2" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#fully-homomorphic-encryption"><i class="fa fa-check"></i><b>20.10.2</b> Fully homomorphic encryption</a></li><li class="chapter" data-level="20.10.3" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#multiparty-secure-computation"><i class="fa fa-check"></i><b>20.10.3</b> Multiparty secure computation</a></li></ul></li><li class="chapter" data-level="20.11" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#exercises"><i class="fa fa-check"></i><b>20.11</b> Exercises</a></li><li class="chapter" data-level="20.12" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#bibliographical-notes"><i class="fa fa-check"></i><b>20.12</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="21" data-path="lec_24_proofs.html"><a href="lec_24_proofs.html"><i class="fa fa-check"></i><b>21</b> Proofs and algorithms</a><ul><li class="chapter" data-level="21.1" data-path="lec_24_proofs.html"><a href="lec_24_proofs.html#exercises"><i class="fa fa-check"></i><b>21.1</b> Exercises</a></li><li class="chapter" data-level="21.2" data-path="lec_24_proofs.html"><a href="lec_24_proofs.html#bibliographical-notes"><i class="fa fa-check"></i><b>21.2</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="22" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html"><i class="fa fa-check"></i><b>22</b> Quantum computing</a><ul><li class="chapter" data-level="22.1" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#the-double-slit-experiment"><i class="fa fa-check"></i><b>22.1</b> The double slit experiment</a></li><li class="chapter" data-level="22.2" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantum-amplitudes"><i class="fa fa-check"></i><b>22.2</b> Quantum amplitudes</a><ul><li class="chapter" data-level="22.2.1" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#linear-algebra-quick-review"><i class="fa fa-check"></i><b>22.2.1</b> Linear algebra quick review</a></li></ul></li><li class="chapter" data-level="22.3" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#bellineqsec"><i class="fa fa-check"></i><b>22.3</b> Bell’s Inequality</a></li><li class="chapter" data-level="22.4" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantum-weirdness"><i class="fa fa-check"></i><b>22.4</b> Quantum weirdness</a></li><li class="chapter" data-level="22.5" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantum-computing-and-computation---an-executive-summary."><i class="fa fa-check"></i><b>22.5</b> Quantum computing and computation - an executive summary.</a></li><li class="chapter" data-level="22.6" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantum-systems"><i class="fa fa-check"></i><b>22.6</b> Quantum systems</a><ul><li class="chapter" data-level="22.6.1" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantum-amplitudes-1"><i class="fa fa-check"></i><b>22.6.1</b> Quantum amplitudes</a></li><li class="chapter" data-level="22.6.2" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantum-systems-an-executive-summary"><i class="fa fa-check"></i><b>22.6.2</b> Quantum systems: an executive summary</a></li></ul></li><li class="chapter" data-level="22.7" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#analysis-of-bells-inequality-optional"><i class="fa fa-check"></i><b>22.7</b> Analysis of Bell’s Inequality (optional)</a></li><li class="chapter" data-level="22.8" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantum-computation"><i class="fa fa-check"></i><b>22.8</b> Quantum computation</a><ul><li class="chapter" data-level="22.8.1" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantum-circuits"><i class="fa fa-check"></i><b>22.8.1</b> Quantum circuits</a></li><li class="chapter" data-level="22.8.2" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#qnand-circ-programs-optional"><i class="fa fa-check"></i><b>22.8.2</b> QNAND-CIRC programs (optional)</a></li><li class="chapter" data-level="22.8.3" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#uniform-computation"><i class="fa fa-check"></i><b>22.8.3</b> Uniform computation</a></li></ul></li><li class="chapter" data-level="22.9" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#physically-realizing-quantum-computation"><i class="fa fa-check"></i><b>22.9</b> Physically realizing quantum computation</a></li><li class="chapter" data-level="22.10" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#shors-algorithm-hearing-the-shape-of-prime-factors"><i class="fa fa-check"></i><b>22.10</b> Shor’s Algorithm: Hearing the shape of prime factors</a><ul><li class="chapter" data-level="22.10.1" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#period-finding"><i class="fa fa-check"></i><b>22.10.1</b> Period finding</a></li><li class="chapter" data-level="22.10.2" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#shors-algorithm-a-birds-eye-view"><i class="fa fa-check"></i><b>22.10.2</b> Shor’s Algorithm: A bird’s eye view</a></li></ul></li><li class="chapter" data-level="22.11" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantum-fourier-transform-advanced-optional"><i class="fa fa-check"></i><b>22.11</b> Quantum Fourier Transform (advanced, optional)</a><ul><li class="chapter" data-level="22.11.1" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantum-fourier-transform-over-the-boolean-cube-simons-algorithm"><i class="fa fa-check"></i><b>22.11.1</b> Quantum Fourier Transform over the Boolean Cube: Simon’s Algorithm</a></li><li class="chapter" data-level="22.11.2" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#from-fourier-to-period-finding-simons-algorithm-advanced-optional"><i class="fa fa-check"></i><b>22.11.2</b> From Fourier to Period finding: Simon’s Algorithm (advanced, optional)</a></li><li class="chapter" data-level="22.11.3" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#from-simon-to-shor-advanced-optional"><i class="fa fa-check"></i><b>22.11.3</b> From Simon to Shor (advanced, optional)</a></li></ul></li><li class="chapter" data-level="22.12" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#exercises"><i class="fa fa-check"></i><b>22.12</b> Exercises</a></li><li class="chapter" data-level="22.13" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantumbibnotessec"><i class="fa fa-check"></i><b>22.13</b> Bibliographical notes</a></li></ul></li><li class="divider"></li>
</ul>
<!--bookdown:toc2:end-->
      </nav>
    </div>

    <div class="book-header" role="navigation">
      <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Equivalent models of computation</a>
      </h1>
    </div>

    <div class="book-body">
      <div class="body-inner">


        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<!--bookdown:toc:end-->
<!--bookdown:body:start-->

<div  class="section level2">

<!-- link to pdf version -->


<!-- start of header referring to comments -->
<div><p></p><p style="color:#871640;"><i class="fas fa-wrench"></i> See any bugs/typos/confusing explanations? <a href="https://github.com/boazbk/tcs/issues/new">Open a GitHub issue</a>. You can also <a href="#commentform">comment below</a> <i class="fas fa-wrench"></i></p></div>



<div><p style="color:#871640;">&#x2605; See also the <a id="pdflink" href='https://files.boazbarak.org/introtcs/lec_07_other_models.pdf'><b>PDF version of this chapter</b></a> (better formatting/references) &#x2605;</p></div>

<!-- end of header referring to comments -->

<!--- start of actual content -->

<h1 id="chapequivalentmodels" data-number="7">Equivalent models of computation</h1>
<div id="section" class="objectives" name="Objectives">
<ul>
<li>Learn about RAM machines and the λ calculus.</li>
<li>Equivalence between these and other models and Turing machines.</li>
<li>Cellular automata and configurations of Turing machines.</li>
<li>Understand the Church-Turing thesis.</li>
</ul>
</div>
<blockquote>
<p><em>“All problems in computer science can be solved by another level of indirection”</em>, attributed to David Wheeler.</p>
</blockquote>
<blockquote>
<p><em>“Because we shall later compute with expressions for functions, we need a distinction between functions and forms and a notation for expressing this distinction. This distinction and a notation for describing it, from which we deviate trivially, is given by Church.”</em>, John McCarthy, 1960 (in paper describing the LISP programming language)</p>
</blockquote>
<p>So far we have defined the notion of computing a function using Turing machines, which are not a close match to the way computation is done in practice. In this chapter we justify this choice by showing that the definition of computable functions will remain the same under a wide variety of computational models. This notion is known as <em>Turing completeness</em> or <em>Turing equivalence</em> and is one of the most fundamental facts of computer science. In fact, a widely believed claim known as the <em>Church-Turing Thesis</em> holds that <em>every</em> “reasonable” definition of computable function is equivalent to being computable by a Turing machine. We discuss the Church-Turing Thesis and the potential definitions of “reasonable” in <a href='#churchturingdiscussionsec'>Section 7.8</a>.</p>
<p>Some of the main computational models we discuss in this chapter include:</p>
<ul>
<li><p><strong>RAM Machines:</strong> Turing Machines do not correspond to standard computing architectures that have <em>Random Access Memory (RAM)</em>. The mathematical model of RAM machines is much closer to actual computers, but we will see that it is equivalent in power to Turing Machines. We also discuss a programming language variant of RAM machines, which we call NAND-RAM. The equivalence of Turing Machines and RAM machines enables demonstrating the <em>Turing Equivalence</em> of many popular programming languages, including all general-purpose languages used in practice such as C, Python, JavaScript, etc.</p></li>
<li><p><strong>Cellular Automata:</strong> Many natural and artificial systems can be modeled as collections of simple components, each evolving according to simple rules based on its state and the state of its immediate neighbors. One well-known such example is <a href="https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life">Conway’s Game of Life</a>. To prove that cellular automata are equivalent to Turing machines we introduce the tool of <em>configurations</em> of Turing Machines. These have other applications, and in particular are used in <a href='lec_09_godel.html#godelchap'>Chapter 10</a> to prove <em>Gödel’s Incompleteness Theorem</em>: a central result in mathematics.</p></li>
<li><p><strong><span><span class="math inline">\(\lambda\)</span></span> calculus:</strong> The <span><span class="math inline">\(\lambda\)</span></span> calculus is a model for expressing computation that originates from the 1930’s, though it is closely connected to functional programming languages widely used today. Showing the equivalence of <span><span class="math inline">\(\lambda\)</span></span> calculus to Turing Machines involves a beautiful technique to eliminate recursion known as the “Y Combinator”.</p></li>
</ul>
<figure>
<img src="../figure/turingcomplete.png" alt="7.1: Some Turing-equivalent models. All of these are equivalent in power to Turing Machines (or equivalently NAND-TM programs) in the sense that they can compute exactly the same class of functions. All of these are models for computing infinite functions that take inputs of unbounded length. In contrast, Boolean circuits / NAND-CIRC programs can only compute finite functions and hence are not Turing complete." id="turingcompletefig" /><figcaption>7.1: Some Turing-equivalent models. All of these are equivalent in power to Turing Machines (or equivalently NAND-TM programs) in the sense that they can compute exactly the same class of functions. All of these are models for computing <em>infinite</em> functions that take inputs of unbounded length. In contrast, Boolean circuits / NAND-CIRC programs can only compute <em>finite</em> functions and hence are not Turing complete.</figcaption>
</figure>
<h2 id="ram-machines-and-nand-ram" data-number="7.1">RAM machines and NAND-RAM</h2>
<p>One of the limitations of Turing Machines (and NAND-TM programs) is that we can only access one location of our arrays/tape at a time. If the head is at position <span><span class="math inline">\(22\)</span></span> in the tape and we want to access the <span><span class="math inline">\(957\)</span></span>-th position then it will take us at least 923 steps to get there. In contrast, almost every programming language has a formalism for directly accessing memory locations. Actual physical computers also provide so called <em>Random Access Memory (RAM)</em> which can be thought of as a large array <code>Memory</code>, such that given an index <span><span class="math inline">\(p\)</span></span> (i.e., memory address, or a <em>pointer</em>), we can read from and write to the <span><span class="math inline">\(p^{th}\)</span></span> location of <code>Memory</code>. (“Random access memory” is quite a misnomer since it has nothing to do with probability, but since it is a standard term in both the theory and practice of computing, we will use it as well.)</p>
<p>The computational model that models access to such a memory is the <em>RAM machine</em> (sometimes also known as the <em>Word RAM model</em>), as depicted in <a href='#rammachinefig'>Figure 7.2</a>. The memory of a RAM machine is an array of unbounded size where each cell can store a single <em>word</em>, which we think of as a string in <span><span class="math inline">\(\{0,1\}^w\)</span></span> and also (equivalently) as a number in <span><span class="math inline">\([2^w]\)</span></span>. For example, many modern computing architectures use <span><span class="math inline">\(64\)</span></span> bit words, in which every memory location holds a string in <span><span class="math inline">\(\{0,1\}^{64}\)</span></span> which can also be thought of as a number between <span><span class="math inline">\(0\)</span></span> and <span><span class="math inline">\(2^{64}-1= 9,223,372,036,854,775,807\)</span></span>. The parameter <span><span class="math inline">\(w\)</span></span> is known as the <em>word size</em>. In practice often <span><span class="math inline">\(w\)</span></span> is a fixed number such as <span><span class="math inline">\(64\)</span></span>, but when doing theory we model <span><span class="math inline">\(w\)</span></span> as a parameter that can depend on the input length or number of steps. (You can think of <span><span class="math inline">\(2^w\)</span></span> as roughly corresponding to the largest memory address that we use in the computation.) In addition to the memory array, a RAM machine also contains a constant number of <em>registers</em> <span><span class="math inline">\(r_0,\ldots,r_{k-1}\)</span></span>, each of which can also contain a single word.</p>
<figure>
<img src="../figure/rammachine.png" alt="7.2: A RAM Machine contains a finite number of local registers, each of which holds an integer, and an unbounded memory array. It can perform arithmetic operations on its register as well as load to a register r the contents of the memory at the address indexed by the number in register r&#39;." id="rammachinefig" class="margin" /><figcaption>7.2: A <em>RAM Machine</em> contains a finite number of local registers, each of which holds an integer, and an unbounded memory array. It can perform arithmetic operations on its register as well as load to a register <span><span class="math inline">\(r\)</span></span> the contents of the memory at the address indexed by the number in register <span><span class="math inline">\(r&#39;\)</span></span>.</figcaption>
</figure>
<p>The operations a RAM machine can carry out include:</p>
<ul>
<li><p><strong>Data movement:</strong> Load data from a certain cell in memory into a register or store the contents of a register into a certain cell of memory. RAM machine can directly access any cell of memory without having to move the “head” (as Turing machines do) to that location. That is, in one step a RAM machine can load into register <span><span class="math inline">\(r_i\)</span></span> the contents of the memory cell indexed by register <span><span class="math inline">\(r_j\)</span></span>, or store into the memory cell indexed by register <span><span class="math inline">\(r_j\)</span></span> the contents of register <span><span class="math inline">\(r_i\)</span></span>.</p></li>
<li><p><strong>Computation:</strong> RAM machines can carry out computation on registers such as arithmetic operations, logical operations, and comparisons.</p></li>
<li><p><strong>Control flow:</strong> As in the case of Turing machines, the choice of what instruction to perform next can depend on the state of the RAM machine, which is captured by the contents of its register.</p></li>
</ul>
<figure>
<img src="../figure/ramvsturing.png" alt="7.3: Different aspects of RAM machines and Turing machines. RAM machines can store integers in their local registers, and can read and write to their memory at a location specified by a register. In contrast, Turing machines can only access their memory in the head location, which moves at most one position to the right or left in each step." id="ramvsturingfig" class="margin" /><figcaption>7.3: Different aspects of RAM machines and Turing machines. RAM machines can store integers in their local registers, and can read and write to their memory at a location specified by a register. In contrast, Turing machines can only access their memory in the head location, which moves at most one position to the right or left in each step.</figcaption>
</figure>
<p>We will not give a formal definition of RAM Machines, though the bibliographical notes section (<a href='#othermodelsbibnotes'>Section 7.10</a>) contains sources for such definitions. Just as the NAND-TM programming language models Turing machines, we can also define a <em>NAND-RAM programming language</em> that models RAM machines. The NAND-RAM programming language extends NAND-TM by adding the following features:</p>
<ul>
<li><p>The variables of NAND-RAM are allowed to be (non negative) <em>integer valued</em> rather than only Boolean as is the case in NAND-TM. That is, a scalar variable <code>foo</code> holds a non negative integer in <span><span class="math inline">\(\N\)</span></span> (rather than only a bit in <span><span class="math inline">\(\{0,1\}\)</span></span>), and an array variable <code>Bar</code> holds an array of integers. As in the case of RAM machines, we will not allow integers of unbounded size. Concretely, each variable holds a number between <span><span class="math inline">\(0\)</span></span> and <span><span class="math inline">\(T-1\)</span></span>, where <span><span class="math inline">\(T\)</span></span> is the number of steps that have been executed by the program so far. (You can ignore this restriction for now: if we want to hold larger numbers, we can simply execute dummy instructions; it will be useful in later chapters.)</p></li>
<li><p>We allow <em>indexed access</em> to arrays. If <code>foo</code> is a scalar and <code>Bar</code> is an array, then <code>Bar[foo]</code> refers to the location of <code>Bar</code> indexed by the value of <code>foo</code>. (Note that this means we don’t need to have a special index variable <code>i</code> any more.)</p></li>
<li><p>As is often the case in programming languages, we will assume that for Boolean operations such as <code>NAND</code>, a zero valued integer is considered as <em>false</em>, and a nonzero valued integer is considered as <em>true</em>.</p></li>
<li><p>In addition to <code>NAND</code>, NAND-RAM also includes all the basic arithmetic operations of addition, subtraction, multiplication, (integer) division, as well as comparisons (equal, greater than, less than, etc..).</p></li>
<li><p>NAND-RAM includes conditional statements <code>if</code>/<code>then</code> as part of the language.</p></li>
<li><p>NAND-RAM contains looping constructs such as <code>while</code> and <code>do</code> as part of the language.</p></li>
</ul>
<p>A full description of the NAND-RAM programming language is in the <a href="http://tiny.cc/introtcsappendix">appendix</a>. However, the most important fact you need to know about NAND-RAM is that you actually don’t need to know much about NAND-RAM at all, since it is equivalent in power to Turing machines:</p>
<div id="RAMTMequivalencethm" class="theorem" title="Turing Machines (aka NAND-TM programs) and RAM machines (aka NAND-RAM programs) are equivalent" name="Theorem 7.1 (Turing Machines (aka NAND-TM programs) and RAM machines (aka NAND-RAM programs) are equivalent) ">
<p>For every function <span><span class="math inline">\(F:\{0,1\}^* \rightarrow \{0,1\}^*\)</span></span>, <span><span class="math inline">\(F\)</span></span> is computable by a NAND-TM program if and only if <span><span class="math inline">\(F\)</span></span> is computable by a NAND-RAM program.</p>
</div>
<p>Since NAND-TM programs are equivalent to Turing machines, and NAND-RAM programs are equivalent to RAM machines, <a href='#RAMTMequivalencethm'>Theorem 7.1</a> shows that all these four models are equivalent to one another.</p>
<figure>
<img src="../figure/nandramproofoverview.png" alt="7.4: Overview of the steps in the proof of  simulating NANDRAM with NANDTM. We first use the inner loop syntactic sugar of  to enable loading an integer from an array to the index variable i of NANDTM. Once we can do that, we can simulate indexed access in NANDTM. We then use an embedding of \N^2 in \N to simulate two dimensional bit arrays in NANDTM. Finally, we use the binary representation to encode one-dimensional arrays of integers as two dimensional arrays of bits hence completing the simulation of NANDRAM with NANDTM." id="nandramoverviewfig" class="margin" /><figcaption>7.4: Overview of the steps in the proof of <a href='#RAMTMequivalencethm'>Theorem 7.1</a> simulating NANDRAM with NANDTM. We first use the inner loop syntactic sugar of <a href='lec_06_loops.html#nandtminnerloopssec'>Subsection 6.4.1</a> to enable loading an integer from an array to the index variable <code>i</code> of NANDTM. Once we can do that, we can simulate <em>indexed access</em> in NANDTM. We then use an embedding of <span><span class="math inline">\(\N^2\)</span></span> in <span><span class="math inline">\(\N\)</span></span> to simulate two dimensional bit arrays in NANDTM. Finally, we use the binary representation to encode one-dimensional arrays of integers as two dimensional arrays of bits hence completing the simulation of NANDRAM with NANDTM.</figcaption>
</figure>
<div class="proofidea" data-ref="RAMTMequivalencethm" name="Proofidea 7.1">
<p>Clearly NAND-RAM is only more powerful than NAND-TM, and so if a function <span><span class="math inline">\(F\)</span></span> is computable by a NAND-TM program then it can be computed by a NAND-RAM program. The challenging direction is to transform a NAND-RAM program <span><span class="math inline">\(P\)</span></span> to an equivalent NAND-TM program <span><span class="math inline">\(Q\)</span></span>. To describe the proof in full we will need to cover the full formal specification of the NAND-RAM language, and show how we can implement every one of its features as syntactic sugar on top of NAND-TM.</p>
<p>This can be done but going over all the operations in detail is rather tedious. Hence we will focus on describing the main ideas behind this transformation. (See also <a href='#nandramoverviewfig'>Figure 7.4</a>.) NAND-RAM generalizes NAND-TM in two main ways: <strong>(a)</strong> adding <em>indexed access</em> to the arrays (ie.., <code>Foo[bar]</code> syntax) and <strong>(b)</strong> moving from <em>Boolean valued</em> variables to <em>integer valued</em> ones. The transformation has two steps:</p>
<ol type="1">
<li><p><em>Indexed access of bit arrays:</em> We start by showing how to handle <strong>(a)</strong>. Namely, we show how we can implement in NAND-TM the operation <code>Setindex(Bar)</code> such that if <code>Bar</code> is an array that encodes some integer <span><span class="math inline">\(j\)</span></span>, then after executing <code>Setindex(Bar)</code> the value of <code>i</code> will equal to <span><span class="math inline">\(j\)</span></span>. This will allow us to simulate syntax of the form <code>Foo[Bar]</code> by <code>Setindex(Bar)</code> followed by <code>Foo[i]</code>.</p></li>
<li><p><em>Two dimensional bit arrays:</em> We then show how we can use “syntactic sugar” to augment NAND-TM with <em>two dimensional arrays</em>. That is, have <em>two indices</em> <code>i</code> and <code>j</code> and <em>two dimensional arrays</em>, such that we can use the syntax <code>Foo[i][j]</code> to access the (<code>i</code>,<code>j</code>)-th location of <code>Foo</code>.</p></li>
<li><p><em>Arrays of integers:</em> Finally we will encode a one dimensional array <code>Arr</code> of <em>integers</em> by a two dimensional <code>Arrbin</code> of <em>bits</em>. The idea is simple: if <span><span class="math inline">\(a_{i,0},\ldots,a_{i,\ell}\)</span></span> is a binary (prefix-free) representation of <code>Arr[</code><span><span class="math inline">\(i\)</span></span><code>]</code>, then <code>Arrbin[</code><span><span class="math inline">\(i\)</span></span><code>][</code><span><span class="math inline">\(j\)</span></span><code>]</code> will be equal to <span><span class="math inline">\(a_{i,j}\)</span></span>.</p></li>
</ol>
<p>Once we have arrays of integers, we can use our usual syntactic sugar for functions, <code>GOTO</code> etc. to implement the arithmetic and control flow operations of NAND-RAM.</p>
</div>
<p>The above approach is not the only way to obtain a proof of <a href='#RAMTMequivalencethm'>Theorem 7.1</a>, see for example <a href='#RAMTMalternativeex'>Exercise 7.1</a></p>
<div id="NANDRAMassembly" class="remark" title="RAM machines / NAND-RAM and assembly language (optional)" name="Remark 7.2 (RAM machines / NAND-RAM and assembly language (optional)) ">
<p>RAM machines correspond quite closely to actual microprocessors such as those in the Intel x86 series that also contains a large <em>primary memory</em> and a constant number of small registers. This is of course no accident: RAM machines aim at modeling more closely than Turing machines the architecture of actual computing systems, which largely follows the so called <a href="https://en.wikipedia.org/wiki/Von_Neumann_architecture">von Neumann architecture</a> as described in the report  (<a href="https://scholar.google.com/scholar?hl=en&q=von+Neumann+First+Draft+of+a+Report+on+the+EDVAC" target="_blank">von Neumann, 1945</a>) . As a result, NAND-RAM is similar in its general outline to assembly languages such as x86 or NIPS. These assembly languages all have instructions to <strong>(1)</strong> move data from registers to memory, <strong>(2)</strong> perform arithmetic or logical computations on registers, and <strong>(3)</strong> conditional execution and loops (“if” and “goto”, commonly known as “branches” and “jumps” in the context of assembly languages).</p>
<p>The main difference between RAM machines and actual microprocessors (and correspondingly between NAND-RAM and assembly languages) is that actual microprocessors have a fixed word size <span><span class="math inline">\(w\)</span></span> so that all registers and memory cells hold numbers in <span><span class="math inline">\([2^w]\)</span></span> (or equivalently strings in <span><span class="math inline">\(\{0,1\}^w\)</span></span>). This number <span><span class="math inline">\(w\)</span></span> can vary among different processors, but common values are either <span><span class="math inline">\(32\)</span></span> or <span><span class="math inline">\(64\)</span></span>. As a theoretical model, RAM machines do not have this limitation, but we rather let <span><span class="math inline">\(w\)</span></span> be the logarithm of our running time (which roughly corresponds to its value in practice as well). Actual microprocessors also have a fixed number of registers (e.g., 14 general purpose registers in x86-64) but this does not make a big difference with RAM machines. It can be shown that RAM machines with as few as two registers are as powerful as full-fledged RAM machines that have an arbitrarily large constant number of registers.</p>
<p>Of course actual microprocessors have many features not shared with RAM machines as well, including parallelism, memory hierarchies, and many others. However, RAM machines do capture actual computers to a first approximation and so (as we will see), the running time of an algorithm on a RAM machine (e.g., <span><span class="math inline">\(O(n)\)</span></span> vs <span><span class="math inline">\(O(n^2)\)</span></span>) is strongly correlated with its practical efficiency.</p>
</div>
<h2 id="nandtmgorydetailssec" data-number="7.2">The gory details (optional)</h2>
<p>We do not show the full formal proof of <a href='#RAMTMequivalencethm'>Theorem 7.1</a> but focus on the most important parts: implementing indexed access, and simulating two dimensional arrays with one dimensional ones. Even these are already quite tedious to describe, as will not be surprising to anyone that has ever written a compiler. Hence you can feel free to merely skim this section. The important point is not for you to know all details by heart but to be convinced that in principle it <em>is</em> possible to transform a NAND-RAM program to an equivalent NAND-TM program, and even be convinced that, with sufficient time and effort, <em>you</em> could do it if you wanted to.</p>
<h3 id="indexed-access-in-nand-tm" data-number="7.2.1">Indexed access in NAND-TM</h3>
<p>In NAND-TM we can only access our arrays in the position of the index variable <code>i</code>, while NAND-RAM has integer-valued variables and can use them for <em>indexed access</em> to arrays, of the form <code>Foo[bar]</code>. To implement indexed access in NAND-TM, we will encode integers in our arrays using some prefix-free representation (see <a href='lec_02_representation.html#prefixfreesec'>Subsection 2.4.2</a>)), and then have a procedure <code>Setindex(Bar)</code> that sets <code>i</code> to the value encoded by <code>Bar</code>. We can simulate the effect of <code>Foo[Bar]</code> using <code>Setindex(Bar)</code> followed by <code>Foo[i]</code>.</p>
<p>Implementing <code>Setindex(Bar)</code> can be achieved as follows:</p>
<ol type="1">
<li><p>We initialize an array <code>Arzero</code> such that <code>Atzero[</code><span><span class="math inline">\(0\)</span></span><code>]</code><span><span class="math inline">\(=1\)</span></span> and <code>Atzero[</code><span><span class="math inline">\(j\)</span></span><code>]</code><span><span class="math inline">\(=0\)</span></span> for all <span><span class="math inline">\(j&gt;0\)</span></span>. (This can be easily done in NAND-TM as all uninitialized variables default to zero.)</p></li>
<li><p>Set <code>i</code> to zero, by decrementing it until we reach the point where <code>Atzero[i]</code><span><span class="math inline">\(=1\)</span></span>.</p></li>
<li><p>Let <code>Temp</code> be an array encoding the number <span><span class="math inline">\(0\)</span></span>.</p></li>
<li><p>We use <code>GOTO</code> to simulate an inner loop of of the form: <strong>while</strong> <code>Temp</code> <span><span class="math inline">\(\neq\)</span></span> <code>Bar</code>, increment <code>Temp</code>.</p></li>
<li><p>At the end of the loop, <code>i</code> is equal to the value encoded by <code>Bar</code>.</p></li>
</ol>
<p>In NAND-TM code (using some syntactic sugar), we can implement the above operations as follows:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" title="1"><span class="co"># assume Atzero is an array such that Atzero[0]=1</span></a>
<a class="sourceLine" id="cb1-2" title="2"><span class="co"># and Atzero[j]=0 for all j&gt;0</span></a>
<a class="sourceLine" id="cb1-3" title="3"></a>
<a class="sourceLine" id="cb1-4" title="4"><span class="co"># set i to 0.</span></a>
<a class="sourceLine" id="cb1-5" title="5">LABEL(<span class="st">&quot;zero_idx&quot;</span>)</a>
<a class="sourceLine" id="cb1-6" title="6">dir0 <span class="op">=</span> zero</a>
<a class="sourceLine" id="cb1-7" title="7">dir1 <span class="op">=</span> one</a>
<a class="sourceLine" id="cb1-8" title="8"><span class="co"># corresponds to i &lt;- i-1</span></a>
<a class="sourceLine" id="cb1-9" title="9">GOTO(<span class="st">&quot;zero_idx&quot;</span>,NOT(Atzero[i]))</a>
<a class="sourceLine" id="cb1-10" title="10">...</a>
<a class="sourceLine" id="cb1-11" title="11"><span class="co"># zero out temp</span></a>
<a class="sourceLine" id="cb1-12" title="12"><span class="co">#(code below assumes a specific prefix-free encoding in which 10 is the &quot;end marker&quot;)</span></a>
<a class="sourceLine" id="cb1-13" title="13">Temp[<span class="dv">0</span>] <span class="op">=</span> <span class="dv">1</span></a>
<a class="sourceLine" id="cb1-14" title="14">Temp[<span class="dv">1</span>] <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb1-15" title="15"><span class="co"># set i to Bar, assume we know how to increment, compare</span></a>
<a class="sourceLine" id="cb1-16" title="16">LABEL(<span class="st">&quot;increment_temp&quot;</span>)</a>
<a class="sourceLine" id="cb1-17" title="17">cond <span class="op">=</span> EQUAL(Temp,Bar)</a>
<a class="sourceLine" id="cb1-18" title="18">dir0 <span class="op">=</span> one</a>
<a class="sourceLine" id="cb1-19" title="19">dir1 <span class="op">=</span> one</a>
<a class="sourceLine" id="cb1-20" title="20"><span class="co"># corresponds to i &lt;- i+1</span></a>
<a class="sourceLine" id="cb1-21" title="21">INC(Temp)</a>
<a class="sourceLine" id="cb1-22" title="22">GOTO(<span class="st">&quot;increment_temp&quot;</span>,cond)</a>
<a class="sourceLine" id="cb1-23" title="23"><span class="co"># if we reach this point, i is number encoded by Bar</span></a>
<a class="sourceLine" id="cb1-24" title="24">...</a>
<a class="sourceLine" id="cb1-25" title="25"><span class="co"># final instruction of program</span></a>
<a class="sourceLine" id="cb1-26" title="26">MODANDJUMP(dir0,dir1)</a></code></pre></div>
<h3 id="two-dimensional-arrays-in-nand-tm" data-number="7.2.2">Two dimensional arrays in NAND-TM</h3>
<p>To implement two dimensional arrays, we want to embed them in a one dimensional array. The idea is that we come up with a <em>one to one</em> function <span><span class="math inline">\(embed:\N \times \N \rightarrow \N\)</span></span>, and so embed the location <span><span class="math inline">\((i,j)\)</span></span> of the two dimensional array <code>Two</code> in the location <span><span class="math inline">\(embed(i,j)\)</span></span> of the array <code>One</code>.</p>
<p>Since the set <span><span class="math inline">\(\N \times \N\)</span></span> seems “much bigger” than the set <span><span class="math inline">\(\N\)</span></span>, a priori it might not be clear that such a one to one mapping exists. However, once you think about it more, it is not that hard to construct. For example, you could ask a child to use scissors and glue to transform a 10" by 10" piece of paper into a 1" by 100" strip. This is essentially a one to one map from <span><span class="math inline">\([10]\times [10]\)</span></span> to <span><span class="math inline">\([100]\)</span></span>. We can generalize this to obtain a one to one map from <span><span class="math inline">\([n]\times [n]\)</span></span> to <span><span class="math inline">\([n^2]\)</span></span> and more generally a one to one map from <span><span class="math inline">\(\N \times \N\)</span></span> to <span><span class="math inline">\(\N\)</span></span>. Specifically, the following map <span><span class="math inline">\(embed\)</span></span> would do (see <a href='#pairingfuncfig'>Figure 7.5</a>):</p>
<p><span>
<div class='myequationbox'><span class="math display">\[embed(x,y) = \tfrac{1}{2}(x+y)(x+y+1)+x\;\;.\]</span></div></span></p>
<figure>
<img src="../figure/pairing_function.png" alt="7.5: Illustration of the map embed(x,y) = \tfrac{1}{2}(x+y)(x+y+1)+x for x,y \in [10], one can see that for every distinct pairs (x,y) and (x&#39;,y&#39;), embed(x,y) \neq embed(x&#39;,y&#39;)." id="pairingfuncfig" class="margin" /><figcaption>7.5: Illustration of the map <span><span class="math inline">\(embed(x,y) = \tfrac{1}{2}(x+y)(x+y+1)+x\)</span></span> for <span><span class="math inline">\(x,y \in [10]\)</span></span>, one can see that for every distinct pairs <span><span class="math inline">\((x,y)\)</span></span> and <span><span class="math inline">\((x&#39;,y&#39;)\)</span></span>, <span><span class="math inline">\(embed(x,y) \neq embed(x&#39;,y&#39;)\)</span></span>.</figcaption>
</figure>
<p><a href='#pair-ex'>Exercise 7.3</a> asks you to prove that <span><span class="math inline">\(embed\)</span></span> is indeed one to one, as well as computable by a NAND-TM program. (The latter can be done by simply following the grade-school algorithms for multiplication, addition, and division.) This means that we can replace code of the form <code>Two[Foo][Bar] = something</code> (i.e., access the two dimensional array <code>Two</code> at the integers encoded by the one dimensional arrays <code>Foo</code> and <code>Bar</code>) by code of the form:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb2-1" title="1">Blah <span class="op">=</span> embed(Foo,Bar)</a>
<a class="sourceLine" id="cb2-2" title="2">Setindex(Blah)</a>
<a class="sourceLine" id="cb2-3" title="3">Two[i] <span class="op">=</span> something</a></code></pre></div>
<h3 id="all-the-rest" data-number="7.2.3">All the rest</h3>
<p>Once we have two dimensional arrays and indexed access, simulating NAND-RAM with NAND-TM is just a matter of implementing the standard algorithms for arithmetic operations and comparisons in NAND-TM. While this is cumbersome, it is not difficult, and the end result is to show that every NAND-RAM program <span><span class="math inline">\(P\)</span></span> can be simulated by an equivalent NAND-TM program <span><span class="math inline">\(Q\)</span></span>, thus completing the proof of <a href='#RAMTMequivalencethm'>Theorem 7.1</a>.</p>
<div id="recursion" class="remark" title="Recursion in NAND-RAM (advanced)" name="Remark 7.3 (Recursion in NAND-RAM (advanced)) ">
<p>One concept that appears in many programming languages but we did not include in NAND-RAM programs is <em>recursion</em>. However, recursion (and function calls in general) can be implemented in NAND-RAM using the <a href="https://goo.gl/JweMj">stack data structure</a>. A <em>stack</em> is a data structure containing a sequence of elements, where we can “push” elements into it and “pop” them from it in “first in last out” order.</p>
<p>We can implement a stack using an array of integers <code>Stack</code> and a scalar variable <code>stackpointer</code> that will be the number of items in the stack. We implement <code>push(foo)</code> by</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb3-1" title="1">Stack[stackpointer]<span class="op">=</span>foo</a>
<a class="sourceLine" id="cb3-2" title="2">stackpointer <span class="op">+=</span> one</a></code></pre></div>
<p>and implement <code>bar = pop()</code> by</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb4-1" title="1">bar <span class="op">=</span> Stack[stackpointer]</a>
<a class="sourceLine" id="cb4-2" title="2">stackpointer <span class="op">-=</span> one</a></code></pre></div>
<p>We implement a function call to <span><span class="math inline">\(F\)</span></span> by pushing the arguments for <span><span class="math inline">\(F\)</span></span> into the stack. The code of <span><span class="math inline">\(F\)</span></span> will “pop” the arguments from the stack, perform the computation (which might involve making recursive or non recursive calls) and then “push” its return value into the stack. Because of the “first in last out” nature of a stack, we do not return control to the calling procedure until all the recursive calls are done.</p>
<p>The fact that we can implement recursion using a non-recursive language is not surprising. Indeed, <em>machine languages</em> typically do not have recursion (or function calls in general), and hence a compiler implements function calls using a stack and <code>GOTO</code>. You can find online tutorials on how recursion is implemented via stack in your favorite programming language, whether it’s <a href="http://interactivepython.org/runestone/static/pythonds/Recursion/StackFramesImplementingRecursion.html">Python</a> , <a href="https://javascript.info/recursion">JavaScript</a>, or <a href="https://mitpress.mit.edu/sicp/full-text/sicp/book/node110.html">Lisp/Scheme</a>.</p>
</div>
<h2 id="turing-equivalence-discussion" data-number="7.3">Turing equivalence (discussion)</h2>
<figure>
<img src="../figure/FortranProg.jpg" alt="7.6: A punched card corresponding to a Fortran statement." id="fortranfig" class="margin" /><figcaption>7.6: A punched card corresponding to a Fortran statement.</figcaption>
</figure>
<p>Any of the standard programming language such as <code>C</code>, <code>Java</code>, <code>Python</code>, <code>Pascal</code>, <code>Fortran</code> have very similar operations to NAND-RAM. (Indeed, ultimately they can all be executed by machines which have a fixed number of registers and a large memory array.) Hence using <a href='#RAMTMequivalencethm'>Theorem 7.1</a>, we can simulate any program in such a programming language by a NAND-TM program. In the other direction, it is a fairly easy programming exercise to write an interpreter for NAND-TM in any of the above programming languages. Hence we can also simulate NAND-TM programs (and so by <a href='lec_06_loops.html#TM-equiv-thm'>Theorem 6.12</a>, Turing machines) using these programming languages. This property of being equivalent in power to Turing Machines / NAND-TM is called <em>Turing Equivalent</em> (or sometimes <em>Turing Complete</em>). Thus all programming languages we are familiar with are Turing equivalent.<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup></p>
<h3 id="the-best-of-both-worlds-paradigm" data-number="7.3.1">The “Best of both worlds” paradigm</h3>
<p>The equivalence between Turing Machines and RAM machines allows us to choose the most convenient language for the task at hand:</p>
<ul>
<li><p>When we want to <em>prove a theorem</em> about all programs/algorithms, we can use Turing machines (or NAND-TM) since they are simpler and easier to analyze. In particular, if we want to show that a certain function <em>can not</em> be computed, then we will use Turing machines.</p></li>
<li><p>When we want to show that a function <em>can be computed</em> we can use RAM machines or NAND-RAM, because they are easier to program in and correspond more closely to high level programming languages we are used to. In fact, we will often describe NAND-RAM programs in an informal manner, trusting that the reader can fill in the details and translate the high level description to the precise program. (This is just like the way people typically use informal or “pseudocode” descriptions of algorithms, trusting that their audience will know to translate these descriptions to code if needed.)</p></li>
</ul>
<p>Our usage of Turing Machines / NAND-TM and RAM Machines / NAND-RAM is very similar to the way people use in practice high and low level programming languages. When one wants to produce a device that executes programs, it is convenient to do so for very simple and “low level” programming language. When one wants to describe an algorithm, it is convenient to use as high level a formalism as possible.</p>
<figure>
<img src="../figure/have_your_cake_and_eat_it_too-img-intro.png" alt="7.7: By having the two equivalent languages NAND-TM and NAND-RAM, we can “have our cake and eat it too”, using NAND-TM when we want to prove that programs can’t do something, and using NAND-RAM or other high level languages when we want to prove that programs can do something." id="cakefig" class="margin" /><figcaption>7.7: By having the two equivalent languages NAND-TM and NAND-RAM, we can “have our cake and eat it too”, using NAND-TM when we want to prove that programs <em>can’t</em> do something, and using NAND-RAM or other high level languages when we want to prove that programs <em>can</em> do something.</figcaption>
</figure>
<div id="eatandhavecake" class="bigidea" name="Bigidea 9">
<p>Using equivalence results such as those between Turing and RAM machines, we can <em>“have our cake and eat it too”</em>.</p>
<p>We can use a simpler model such as Turing machines when we want to prove something <em>can’t</em> be done, and use a feature-rich model such as RAM machines when we want to prove something <em>can</em> be done.</p>
</div>
<h3 id="lets-talk-about-abstractions." data-number="7.3.2">Let’s talk about abstractions.</h3>
<blockquote>
<p>“The programmer is in the unique position that … he has to be able to think in terms of conceptual hierarchies that are much deeper than a single mind ever needed to face before.”, Edsger Dijkstra, “On the cruelty of really teaching computing science”, 1988.</p>
</blockquote>
<p>At some point in any theory of computation course, the instructor and students need to have <em>the talk</em>. That is, we need to discuss the <em>level of abstraction</em> in describing algorithms. In algorithms courses, one typically describes algorithms in English, assuming readers can “fill in the details” and would be able to convert such an algorithm into an implementation if needed. For example, <a href='#bfsalghighlevel'>Algorithm 7.4</a> is a high level description of the <a href="https://goo.gl/ug7Jaj">breadth first search</a> algorithm.</p>
<div  class="pseudocodeoutput">
<div class="ps-root">
<div class="ps-algorithm with-caption" id = bfsalghighlevel>
<p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;">
<span class="ps-keyword">Algorithm 4 </span>Breadth First Search</p>
<div class="ps-algorithmic"><p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"><span class="ps-keyword">Input:</span>  Graph \(G\), vertices \(u,v\)<p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"><span class="ps-keyword">Output:</span>  "connected" when \(u\) is connected to \(v\) in \(G\), "disconnected"<p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"> Initialize empty queue \(Q\).<p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"> Put \(u\) in \(Q\)<p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"><span class="ps-keyword">while</span>{\(Q\) is not empty} 
                <div class="ps-block" style="margin-left:1.2em;">
                <p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;">    Remove top vertex \(w\) from \(Q\)
</div><p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;">   <span class="ps-keyword">if</span>{\(w=v\)} <span class="ps-keyword">return</span> "connected" <span class="ps-keyword">endif</span> 
                <div class="ps-block" style="margin-left:1.2em;">
                <p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;">    Mark \(w\)<p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;">    Add all unmarked neighbors of \(w\) to \(Q\).
</div><p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"><span class="ps-keyword">endwhile</span><p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"><span class="ps-keyword">return</span> "disconnected"<p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"></div>
</div>
</div>
</div>
<p>If we wanted to give more details on how to implement breadth first search in a programming language such as Python or C (or NAND-RAM / NAND-TM for that matter), we would describe how we implement the queue data structure using an array, and similarly how we would use arrays mark vertices. We call such an “intermediate level” description an <em>implementation level</em> or <em>pseudocode</em> description. Finally, if we want to describe the implementation precisely, we would give the full code of the program (or another fully precise representation, such as in the form of a list of tuples). We call this a <em>formal</em> or <em>low level</em> description.</p>
<figure>
<img src="../figure/levelsofdescription.png" alt="7.8: We can describe an algorithm at different levels of granularity/detail and precision. At the highest level we just write the idea in words, omitting all details on representation and implementation. In the intermediate level (also known as implementation or pseudocode) we give enough details of the implementation that would allow someone to derive it, though we still fall short of providing the full code. The lowest level is where the actual code or mathematical description is fully spelled out. These different levels of detail all have their uses, and moving between them is one of the most important skills for a computer scientist." id="levelsdescfig" /><figcaption>7.8: We can describe an algorithm at different levels of granularity/detail and precision. At the highest level we just write the idea in words, omitting all details on representation and implementation. In the intermediate level (also known as <em>implementation</em> or <em>pseudocode</em>) we give enough details of the implementation that would allow someone to derive it, though we still fall short of providing the full code. The lowest level is where the actual code or mathematical description is fully spelled out. These different levels of detail all have their uses, and moving between them is one of the most important skills for a computer scientist.</figcaption>
</figure>
<p>While we started off by describing NAND-CIRC, NAND-TM, and NAND-RAM programs at the full formal level, as we progress in this book we will move to implementation and high level description. After all, our goal is not to use these models for actual computation, but rather to analyze the general phenomenon of computation. That said, if you don’t understand how the high level description translates to an actual implementation, going “down to the metal” is often an excellent exercise. One of the most important skills for a computer scientist is the ability to move up and down hierarchies of abstractions.</p>
<p>A similar distinction applies to the notion of <em>representation</em> of objects as strings. Sometimes, to be precise, we give a <em>low level specification</em> of exactly how an object maps into a binary string. For example, we might describe an encoding of <span><span class="math inline">\(n\)</span></span> vertex graphs as length <span><span class="math inline">\(n^2\)</span></span> binary strings, by saying that we map a graph <span><span class="math inline">\(G\)</span></span> over the vertices <span><span class="math inline">\([n]\)</span></span> to a string <span><span class="math inline">\(x\in \{0,1\}^{n^2}\)</span></span> such that the <span><span class="math inline">\(n\cdot i + j\)</span></span>-th coordinate of <span><span class="math inline">\(x\)</span></span> is <span><span class="math inline">\(1\)</span></span> if and only if the edge <span><span class="math inline">\(\overrightarrow{i \; j}\)</span></span> is present in <span><span class="math inline">\(G\)</span></span>. We can also use an <em>intermediate</em> or <em>implementation level</em> description, by simply saying that we represent a graph using the adjacency matrix representation.</p>
<p>Finally, because we are translating between the various representations of graphs (and objects in general) can be done via a NAND-RAM (and hence a NAND-TM) program, when talking in a high level we also suppress discussion of representation altogether. For example, the fact that graph connectivity is a computable function is true regardless of whether we represent graphs as adjacency lists, adjacency matrices, list of edge-pairs, and so on and so forth. Hence, in cases where the precise representation doesn’t make a difference, we would often talk about our algorithms as taking as input an object <span><span class="math inline">\(X\)</span></span> (that can be a graph, a vector, a program, etc.) without specifying how <span><span class="math inline">\(X\)</span></span> is encoded as a string.</p>
<p><strong>Defining “Algorithms”.</strong> Up until now we have used the term “algorithm” informally. However, Turing Machines and the range of equivalent models yield a way to precisely and formally define algorithms. Hence whenever we refer to an <em>algorithm</em> in this book, we will mean that it is an instance of one of the Turing equivalent models, such as Turing machines, NAND-TM, RAM machines, etc. Because of the equivalence of all these models, in many contexts, it will not matter which of these we use.</p>
<h3 id="turingcompletesec" data-number="7.3.3">Turing completeness and equivalence, a formal definition (optional)</h3>
<p>A <em>computational model</em> is some way to define what it means for a <em>program</em> (which is represented by a string) to compute a (partial) <em>function</em>. A <em>computational model</em> <span><span class="math inline">\(\mathcal{M}\)</span></span> is <em>Turing complete</em>, if we can map every Turing machine (or equivalently NAND-TM program) <span><span class="math inline">\(N\)</span></span> into a program <span><span class="math inline">\(P\)</span></span> for <span><span class="math inline">\(\mathcal{M}\)</span></span> that computes the same function as <span><span class="math inline">\(Q\)</span></span>. It is <em>Turing equivalent</em> if the other direction holds as well (i.e., we can map every program in <span><span class="math inline">\(\mathcal{M}\)</span></span> to a Turing machine that computes the same function). We can define this notion formally as follows. (This formal definition is not crucial for the remainder of this book so feel to skip it as long as you understand the general concept of Turing equivalence; This notion is sometimes referred to in the literature as <a href="https://goo.gl/rzuNPu">Gödel numbering</a> or <a href="https://goo.gl/xXJoUG">admissible numbering</a>.)</p>
<div id="turingcompletedef" class="definition" title="Turing completeness and equivalence (optional)" name="Definition 7.5 (Turing completeness and equivalence (optional)) ">
<p>Let <span><span class="math inline">\(\mathcal{F}\)</span></span> be the set of all partial functions from <span><span class="math inline">\(\{0,1\}^*\)</span></span> to <span><span class="math inline">\(\{0,1\}^*\)</span></span>. A <em>computational model</em> is a map <span><span class="math inline">\(\mathcal{M}:\{0,1\}^* \rightarrow \mathcal{F}\)</span></span>.</p>
<p>We say that a program <span><span class="math inline">\(P \in \{0,1\}^*\)</span></span> <em><span><span class="math inline">\(\mathcal{M}\)</span></span>-computes</em> a function <span><span class="math inline">\(F\in \mathcal{F}\)</span></span> if <span><span class="math inline">\(\mathcal{M}(P) = F\)</span></span>.</p>
<p>A computational model <span><span class="math inline">\(\mathcal{M}\)</span></span> is <em>Turing complete</em> if there is a computable map <span><span class="math inline">\(\ensuremath{\mathit{ENCODE}}_{\mathcal{M}}:\{0,1\}^* \rightarrow \{0,1\}^*\)</span></span> for every Turing machine <span><span class="math inline">\(N\)</span></span> (represented as a string), <span><span class="math inline">\(\mathcal{M}(\ensuremath{\mathit{ENCODE}}_{\mathcal{M}}(N))\)</span></span> is equal to the partial function computed by <span><span class="math inline">\(N\)</span></span>.</p>
<p>A computational model <span><span class="math inline">\(\mathcal{M}\)</span></span> is <em>Turing equivalent</em> if it is Turing complete and there exists a computable map <span><span class="math inline">\(\ensuremath{\mathit{DECODE}}_{\mathcal{M}}:\{0,1\}^* \rightarrow \{0,1\}^*\)</span></span> such that or every string <span><span class="math inline">\(P\in \{0,1\}^*\)</span></span>, <span><span class="math inline">\(N=\ensuremath{\mathit{DECODE}}_{\mathcal{M}}(P)\)</span></span> is a string representation of a Turing machine that computes the function <span><span class="math inline">\(\mathcal{M}(P)\)</span></span>.</p>
</div>
<p>Some examples of Turing equivalent models (some of which we have already seen, and some are discussed below) include:</p>
<ul>
<li>Turing machines</li>
<li>NAND-TM programs</li>
<li>NAND-RAM programs</li>
<li>λ calculus</li>
<li>Game of life (mapping programs and inputs/outputs to starting and ending configurations)</li>
<li>Programming languages such as Python/C/Javascript/OCaml… (allowing for unbounded storage)</li>
</ul>
<h2 id="cellularautomatasec" data-number="7.4">Cellular automata</h2>
<p>Many physical systems can be described as consisting of a large number of elementary components that interact with one another. One way to model such systems is using <em>cellular automata</em>. This is a system that consists of a large (or even infinite) number of cells. Each cell only has a constant number of possible states. At each time step, a cell updates to a new state by applying some simple rule to the state of itself and its neighbors.</p>
<figure>
<img src="../figure/conwaysgrids.png" alt="7.9: Rules for Conway’s Game of Life. Image from this blog post." id="gameofliferulesfig" /><figcaption>7.9: Rules for Conway’s Game of Life. Image from <a href="https://mblogscode.wordpress.com/2017/06/07/python-simulation-coding-conways-game-of-life/">this blog post</a>.</figcaption>
</figure>
<p>A canonical example of a cellular automaton is <a href="https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life">Conway’s Game of Life</a>. In this automata the cells are arranged in an infinite two dimensional grid. Each cell has only two states: “dead” (which we can encode as <span><span class="math inline">\(0\)</span></span> and identify with <span><span class="math inline">\(\varnothing\)</span></span>) or “alive” (which we can encode as <span><span class="math inline">\(1\)</span></span>). The next state of a cell depends on its previous state and the states of its 8 vertical, horizontal and diagonal neighbors (see <a href='#gameofliferulesfig'>Figure 7.9</a>). A dead cell becomes alive only if exactly three of its neighbors are alive. A live cell continues to live if it has two or three live neighbors. Even though the number of cells is potentially infinite, we can encode the state using a finite-length string by only keeping track of the live cells. If we initialize the system in a configuration with a finite number of live cells, then the number of live cells will stay finite in all future steps. The <a href="https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life">Wikipedia page</a> for the Game of Life contains some beautiful figures and animations of configurations that produce very interesting evolutions.</p>
<figure>
<img src="../figure/onetwodimensionalca.png" alt="7.10: In a two dimensional cellular automaton every cell is in position i,j for some integers i,j \in \Z. The state of a cell is some value A_{i,j} \in \Sigma for some finite alphabet \Sigma. At a given time step, the state of the cell is adjusted according to some function applied to the state of (i,j) and all its neighbors (i \pm 1, j\pm 1). In a one dimensional cellular automaton every cell is in position i\in \Z and the state A_i of i at the next time step depends on its current state and the state of its two neighbors i-1 and i+1." id="onetwodimcellularautomatafig" /><figcaption>7.10: In a <em>two dimensional cellular automaton</em> every cell is in position <span><span class="math inline">\(i,j\)</span></span> for some integers <span><span class="math inline">\(i,j \in \Z\)</span></span>. The <em>state</em> of a cell is some value <span><span class="math inline">\(A_{i,j} \in \Sigma\)</span></span> for some finite alphabet <span><span class="math inline">\(\Sigma\)</span></span>. At a given time step, the state of the cell is adjusted according to some function applied to the state of <span><span class="math inline">\((i,j)\)</span></span> and all its neighbors <span><span class="math inline">\((i \pm 1, j\pm 1)\)</span></span>. In a <em>one dimensional cellular automaton</em> every cell is in position <span><span class="math inline">\(i\in \Z\)</span></span> and the state <span><span class="math inline">\(A_i\)</span></span> of <span><span class="math inline">\(i\)</span></span> at the next time step depends on its current state and the state of its two neighbors <span><span class="math inline">\(i-1\)</span></span> and <span><span class="math inline">\(i+1\)</span></span>.</figcaption>
</figure>
<p>Since the cells in the game of life are are arranged in an infinite two-dimensional grid, it is an example of a <em>two dimensional cellular automaton</em>. We can also consider the even simpler setting of a <em>one dimensional cellular automaton</em>, where the cells are arranged in an infinite line, see <a href='#onetwodimcellularautomatafig'>Figure 7.10</a>. It turns out that even this simple model is enough to achieve Turing-completeness. We will now formally define one-dimensional cellular automata and then prove their Turing completeness.</p>
<div id="cellautomatadef" class="definition" title="One dimensional cellular automata" name="Definition 7.6 (One dimensional cellular automata) ">
<p>Let <span><span class="math inline">\(\Sigma\)</span></span> be a finite set containing the symbol <span><span class="math inline">\(\varnothing\)</span></span>. A <em>one dimensional cellular automation</em> over alphabet <span><span class="math inline">\(\Sigma\)</span></span> is described by a <em>transition rule</em> <span><span class="math inline">\(r:\Sigma^3 \rightarrow \Sigma\)</span></span>, which satisfies <span><span class="math inline">\(r(\varnothing,\varnothing,\varnothing) = \varnothing\)</span></span>.</p>
<p>A <em>configuration</em> of the automaton <span><span class="math inline">\(r\)</span></span> is a function <span><span class="math inline">\(A:\Z \rightarrow \Sigma\)</span></span>. If an automaton with rule <span><span class="math inline">\(r\)</span></span> is in configuration <span><span class="math inline">\(A\)</span></span>, then its next configuration, denoted by <span><span class="math inline">\(A&#39; = \ensuremath{\mathit{NEXT}}_r(A)\)</span></span>, is the function <span><span class="math inline">\(A&#39;\)</span></span> such that <span><span class="math inline">\(A&#39;(i) = r(A(i-1),A(i),A(i+1))\)</span></span> for every <span><span class="math inline">\(i\in \Z\)</span></span>. In other words, the next state of the automaton <span><span class="math inline">\(r\)</span></span> at point <span><span class="math inline">\(i\)</span></span> obtained by applying the rule <span><span class="math inline">\(r\)</span></span> to the values of <span><span class="math inline">\(A\)</span></span> at <span><span class="math inline">\(i\)</span></span> and its two neighbors.</p>
</div>
<p><strong>Finite configuration.</strong> We say that a configuration of an automaton <span><span class="math inline">\(r\)</span></span> is <em>finite</em> if there is only some finite number of indices <span><span class="math inline">\(i_0,\ldots,i_{j-1}\)</span></span> in <span><span class="math inline">\(\Z\)</span></span> such that <span><span class="math inline">\(A(i_j) \neq \varnothing\)</span></span>. (That is, for every <span><span class="math inline">\(i \not\in \{ i_0, \ldots, i_{j-1}\}\)</span></span>, <span><span class="math inline">\(A(i)=\varnothing\)</span></span>.) Such a configuration can be represented using a finite string that encodes the indices <span><span class="math inline">\(i_0,\ldots,i_{n-1}\)</span></span> and the values <span><span class="math inline">\(A(i_0),\ldots,A(i_{n-1})\)</span></span>. Since <span><span class="math inline">\(R(\varnothing,\varnothing,\varnothing)=\varnothing\)</span></span>, if <span><span class="math inline">\(A\)</span></span> is a finite configuration then <span><span class="math inline">\(\ensuremath{\mathit{NEXT}}_r(A)\)</span></span> is finite as well. We will only be interested in studying cellular automata that are initialized in finite configurations, and hence remain in a finite configuration throughout their evolution.</p>
<h3 id="one-dimensional-cellular-automata-are-turing-complete" data-number="7.4.1">One dimensional cellular automata are Turing complete</h3>
<p>We can write a program (for example using NAND-RAM) that simulates the evolution of any cellular automaton from an initial finite configuration by simply storing the values of the cells with state not equal to <span><span class="math inline">\(\varnothing\)</span></span> and repeatedly applying the rule <span><span class="math inline">\(r\)</span></span>. Hence cellular automata can be simulated by Turing Machines. What is more surprising that the other direction holds as well. For example, as simple as its rules seem, we can simulate a Turing machine using the game of life (see <a href='#golfig'>Figure 7.11</a>).</p>
<figure>
<img src="../figure/turing_gol.jpg" alt="7.11: A Game-of-Life configuration simulating a Turing Machine. Figure by Paul Rendell." id="golfig" class="margin" /><figcaption>7.11: A Game-of-Life configuration simulating a Turing Machine. Figure by <a href="http://rendell-attic.org/gol/tm.htm">Paul Rendell</a>.</figcaption>
</figure>
<p>In fact, even <a href="https://en.wikipedia.org/wiki/Rule_110">one dimensional cellular automata</a> can be Turing complete:</p>
<div id="onedimcathm" class="theorem" title="One dimensional automata are Turing complete" name="Theorem 7.7 (One dimensional automata are Turing complete) ">
<p>For every Turing machine <span><span class="math inline">\(M\)</span></span>, there is a one dimension cellular automaton that can simulate <span><span class="math inline">\(M\)</span></span> on every input <span><span class="math inline">\(x\)</span></span>.</p>
</div>
<p>To make the notion of “simulating a Turing machine” more precise we will need to define <em>configurations</em> of Turing machines. We will do so in <a href='#turingmachinesconfigsec'>Subsection 7.4.2</a> below, but at a high level a <em>configuration</em> of a Turing machine is a string that encodes its full state at a given step in its computation. That is, the contents of all (non empty) cells of its tape, its current state, as well as the head position.</p>
<p>The key idea in the proof of <a href='#onedimcathm'>Theorem 7.7</a> is that at every point in the computation of a Turing machine <span><span class="math inline">\(M\)</span></span>, the only cell in <span><span class="math inline">\(M\)</span></span>’s tape that can change is the one where the head is located, and the value this cell changes to is a function of its current state and the finite state of <span><span class="math inline">\(M\)</span></span>. This observation allows us to encode the configuration of a Turing machine <span><span class="math inline">\(M\)</span></span> as a finite configuration of a cellular automaton <span><span class="math inline">\(r\)</span></span>, and ensure that a one-step evolution of this encoded configuration under the rules of <span><span class="math inline">\(r\)</span></span> corresponds to one step in the execution of the Turing machine <span><span class="math inline">\(M\)</span></span>.</p>
<h3 id="turingmachinesconfigsec" data-number="7.4.2">Configurations of Turing machines and the next-step function</h3>
<p>To turn the above ideas into a rigorous proof (and even statement!) of <a href='#onedimcathm'>Theorem 7.7</a> we will need to precisely define the notion of <em>configurations</em> of Turing machines. This notion will be useful for us in later chapters as well.</p>
<figure>
<img src="../figure/turingmachineconf.png" alt="7.12: A configuration of a Turing machine M with alphabet \Sigma and state space [k] encodes the state of M at a particular step in its execution as a string \alpha over the alphabet \overline{\Sigma} = \Sigma \times (\{\cdot \} \times [k]). The string is of length t where t is such that M’s tape contains \varnothing in all positions t and larger and M’s head is in a position smaller than t. If M’s head is in the i-th position, then for j \neq i, \alpha_j encodes the value of the j-th cell of M’s tape, while \alpha_i encodes both this value as well as the current state of M. If the machine writes the value \tau, changes state to t, and moves right, then in the next configuration will contain at position i the value (\tau,\cdot) and at position i+1 the value (\alpha_{i+1},t)." id="turingconfigfig" /><figcaption>7.12: A <em>configuration</em> of a Turing machine <span><span class="math inline">\(M\)</span></span> with alphabet <span><span class="math inline">\(\Sigma\)</span></span> and state space <span><span class="math inline">\([k]\)</span></span> encodes the state of <span><span class="math inline">\(M\)</span></span> at a particular step in its execution as a string <span><span class="math inline">\(\alpha\)</span></span> over the alphabet <span><span class="math inline">\(\overline{\Sigma} = \Sigma \times (\{\cdot \} \times [k])\)</span></span>. The string is of length <span><span class="math inline">\(t\)</span></span> where <span><span class="math inline">\(t\)</span></span> is such that <span><span class="math inline">\(M\)</span></span>’s tape contains <span><span class="math inline">\(\varnothing\)</span></span> in all positions <span><span class="math inline">\(t\)</span></span> and larger and <span><span class="math inline">\(M\)</span></span>’s head is in a position smaller than <span><span class="math inline">\(t\)</span></span>. If <span><span class="math inline">\(M\)</span></span>’s head is in the <span><span class="math inline">\(i\)</span></span>-th position, then for <span><span class="math inline">\(j \neq i\)</span></span>, <span><span class="math inline">\(\alpha_j\)</span></span> encodes the value of the <span><span class="math inline">\(j\)</span></span>-th cell of <span><span class="math inline">\(M\)</span></span>’s tape, while <span><span class="math inline">\(\alpha_i\)</span></span> encodes both this value as well as the current state of <span><span class="math inline">\(M\)</span></span>. If the machine writes the value <span><span class="math inline">\(\tau\)</span></span>, changes state to <span><span class="math inline">\(t\)</span></span>, and moves right, then in the next configuration will contain at position <span><span class="math inline">\(i\)</span></span> the value <span><span class="math inline">\((\tau,\cdot)\)</span></span> and at position <span><span class="math inline">\(i+1\)</span></span> the value <span><span class="math inline">\((\alpha_{i+1},t)\)</span></span>.</figcaption>
</figure>
<div id="configtmdef" class="definition" title="Configuration of Turing Machines." name="Definition 7.8 (Configuration of Turing Machines.) ">
<p>Let <span><span class="math inline">\(M\)</span></span> be a Turing machine with tape alphabet <span><span class="math inline">\(\Sigma\)</span></span> and state space <span><span class="math inline">\([k]\)</span></span>. A <em>configuration of <span><span class="math inline">\(M\)</span></span></em> is a string <span><span class="math inline">\(\alpha \in \overline{\Sigma}^*\)</span></span> where <span><span class="math inline">\(\overline{\Sigma} = \Sigma \times \left( \{\cdot\} \cup [k] \right)\)</span></span> that satisfies that there is exactly one coordinate <span><span class="math inline">\(i\)</span></span> for which <span><span class="math inline">\(\alpha_i = (\sigma,s)\)</span></span> for some <span><span class="math inline">\(\sigma \in \Sigma\)</span></span> and <span><span class="math inline">\(s\in [k]\)</span></span>. For all other coordinates <span><span class="math inline">\(j\)</span></span>, <span><span class="math inline">\(\alpha_j = (\sigma&#39;,\cdot)\)</span></span> for some <span><span class="math inline">\(\sigma&#39;\in \Sigma\)</span></span>.</p>
<p>A configuration <span><span class="math inline">\(\alpha \in \overline{\Sigma}^*\)</span></span> of <span><span class="math inline">\(M\)</span></span> corresponds to the following state of its execution:</p>
<ul>
<li><p><span><span class="math inline">\(M\)</span></span>’s tape contains <span><span class="math inline">\(\alpha_{j,0}\)</span></span> for all <span><span class="math inline">\(j&lt;|\alpha|\)</span></span> and contains <span><span class="math inline">\(\varnothing\)</span></span> for all positions that are at least <span><span class="math inline">\(|\alpha|\)</span></span>, where we let <span><span class="math inline">\(\alpha_{j,0}\)</span></span> be the value <span><span class="math inline">\(\sigma\)</span></span> such that <span><span class="math inline">\(\alpha_j = (\sigma,t)\)</span></span> with <span><span class="math inline">\(\sigma \in \Sigma\)</span></span> and <span><span class="math inline">\(t \in \{\cdot \} \cup [k]\)</span></span>. (In other words, since <span><span class="math inline">\(\alpha_j\)</span></span> is a pair of an alphabet symbol <span><span class="math inline">\(\sigma\)</span></span> and either a state in <span><span class="math inline">\([k]\)</span></span> or the symbol <span><span class="math inline">\(\cdot\)</span></span>, <span><span class="math inline">\(\alpha_{j,0}\)</span></span> is the first component <span><span class="math inline">\(\sigma\)</span></span> of this pair.)</p></li>
<li><p><span><span class="math inline">\(M\)</span></span>’s head is in the unique position <span><span class="math inline">\(i\)</span></span> for which <span><span class="math inline">\(\alpha_i\)</span></span> has the form <span><span class="math inline">\((\sigma,s)\)</span></span> for <span><span class="math inline">\(s\in [k]\)</span></span>, and <span><span class="math inline">\(M\)</span></span>’s state is equal to <span><span class="math inline">\(s\)</span></span>.</p></li>
</ul>
</div>
<div class="pause" name="Pause">
<p><a href='#configtmdef'>Definition 7.8</a> below has some technical details, but is not actually that deep or complicated. Try to take a moment to stop and think how <em>you</em> would encode as a string the state of a Turing machine at a given point in an execution.</p>
<p>Think what are all the components that you need to know in order to be able to continue the execution from this point onwards, and what is a simple way to encode them using a list of finite symbols. In particular, with an eye towards our future applications, try to think of an encoding which will make it as simple as possible to map a configuration at step <span><span class="math inline">\(t\)</span></span> to the configuration at step <span><span class="math inline">\(t+1\)</span></span>.</p>
</div>
<p><a href='#configtmdef'>Definition 7.8</a> is a little cumbersome, but ultimately a configuration is simply a string that encodes a <em>snapshot</em> of the Turing machine at a given point in the execution. (In operating-systems lingo, it is a <a href="https://goo.gl/AsccXh">“core dump”</a>.) Such a snapshot needs to encode the following components:</p>
<ol type="1">
<li><p>The current head position.</p></li>
<li><p>The full contents of the large scale memory, that is the tape.</p></li>
<li><p>The contents of the “local registers”, that is the state of the machine.</p></li>
</ol>
<p>The precise details of how we encode a configuration are not important, but we do want to record the following simple fact:</p>
<div id="nextstepfunctionlem" class="lemma" name="Lemma 7.9">
<p>Let <span><span class="math inline">\(M\)</span></span> be a Turing machine and let <span><span class="math inline">\(\ensuremath{\mathit{NEXT}}_M:\overline{\Sigma}^* \rightarrow \overline{\Sigma}^*\)</span></span> be the function that maps a configuration of <span><span class="math inline">\(M\)</span></span> to the configuration at the next step of the execution. Then for every <span><span class="math inline">\(i \in \N\)</span></span>, the value of <span><span class="math inline">\(\ensuremath{\mathit{NEXT}}_M(\alpha)_i\)</span></span> only depends on the coordinates <span><span class="math inline">\(\alpha_{i-1},\alpha_i,\alpha_{i+1}\)</span></span>.</p>
</div>
<p>(For simplicity of notation, above we use the convention that if <span><span class="math inline">\(i\)</span></span> is “out of bounds”, such as <span><span class="math inline">\(i&lt;0\)</span></span> or <span><span class="math inline">\(i&gt;|\alpha|\)</span></span>, then we assume that <span><span class="math inline">\(\alpha_i = (\varnothing,\cdot)\)</span></span>.) We leave proving <a href='#nextstepfunctionlem'>Lemma 7.9</a> as <a href='#nextstepfunctionlemex'>Exercise 7.7</a>. The idea behind the proof is simple: if the head is neither in position <span><span class="math inline">\(i\)</span></span> nor positions <span><span class="math inline">\(i-1\)</span></span> and <span><span class="math inline">\(i+1\)</span></span>, then the next-step configuration at <span><span class="math inline">\(i\)</span></span> will be the same as it was before. Otherwise, we can “read off” the state of the Turing machine and the value of the tape at the head location from the configuration at <span><span class="math inline">\(i\)</span></span> or one of its neighbors and use that to update what the new state at <span><span class="math inline">\(i\)</span></span> should be. Completing the full proof is not hard, but doing it is a great way to ensure that you are comfortable with the definition of configurations.</p>
<p><strong>Completing the proof of <a href='#onedimcathm'>Theorem 7.7</a>.</strong> We can now restate <a href='#onedimcathm'>Theorem 7.7</a> more formally, and complete its proof:</p>
<div id="onedimcathmformal" class="theorem" title="One dimensional automata are Turing complete (formal statement)" name="Theorem 7.10 (One dimensional automata are Turing complete (formal statement)) ">
<p>For every Turing Machine <span><span class="math inline">\(M\)</span></span>, if we denote by <span><span class="math inline">\(\overline{\Sigma}\)</span></span> the alphabet of its configuration strings, then there is a one-dimensional cellular automaton <span><span class="math inline">\(r\)</span></span> over the alphabet <span><span class="math inline">\(\overline{\Sigma}^*\)</span></span> such that <span>
<div class='myequationbox'><span class="math display">\[\left( \ensuremath{\mathit{NEXT}}_M(\alpha) \right)  = \ensuremath{\mathit{NEXT}}_r \left( \alpha \right)\]</span></div></span> for every configuration <span><span class="math inline">\(\alpha \in \overline{\Sigma}^*\)</span></span> of <span><span class="math inline">\(M\)</span></span> (again using the convention that we consider <span><span class="math inline">\(\alpha_i=\varnothing\)</span></span> if <span><span class="math inline">\(i\)</span></span> is "out of bounds).</p>
</div>
<div class="proof" data-ref="onedimcathmformal" name="Proof 7.4.2">
<p>We consider the element <span><span class="math inline">\((\varnothing,\cdot)\)</span></span> of <span><span class="math inline">\(\overline{\Sigma}\)</span></span> to correspond to the <span><span class="math inline">\(\varnothing\)</span></span> element of the automaton <span><span class="math inline">\(r\)</span></span>. In this case, by <a href='#nextstepfunctionlem'>Lemma 7.9</a>, the function <span><span class="math inline">\(\ensuremath{\mathit{NEXT}}_M\)</span></span> that maps a configuration of <span><span class="math inline">\(M\)</span></span> into the next one is in fact a valid rule for a one dimensional automata.</p>
</div>
<p>The automaton arising from the proof of <a href='#onedimcathmformal'>Theorem 7.10</a> has a large alphabet, and furthermore one whose size that depends on the machine <span><span class="math inline">\(M\)</span></span> that is being simulated. It turns out that one can obtain an automaton with an alphabet of fixed size that is independent of the program being simulated, and in fact the alphabet of the automaton can be the minimal set <span><span class="math inline">\(\{0,1\}\)</span></span>! See <a href='#onedimautfig'>Figure 7.13</a> for an example of such an Turing-complete automaton.</p>
<figure>
<img src="../figure/Rule110Big.jpg" alt="7.13: Evolution of a one dimensional automata. Each row in the figure corresponds to the configuration. The initial configuration corresponds to the top row and contains only a single “live” cell. This figure corresponds to the “Rule 110” automaton of Stephen Wolfram which is Turing Complete. Figure taken from Wolfram MathWorld." id="onedimautfig" class="margin" /><figcaption>7.13: Evolution of a one dimensional automata. Each row in the figure corresponds to the configuration. The initial configuration corresponds to the top row and contains only a single “live” cell. This figure corresponds to the “Rule 110” automaton of Stephen Wolfram which is Turing Complete. Figure taken from <a href="http://mathworld.wolfram.com/Rule110.html">Wolfram MathWorld</a>.</figcaption>
</figure>
<div id="nandtmprogconfig" class="remark" title="Configurations of NAND-TM programs" name="Remark 7.11 (Configurations of NAND-TM programs) ">
<p>We can use the same approach as <a href='#configtmdef'>Definition 7.8</a> to define configurations of a <em>NAND-TM program</em>. Such a configuration will need to encode:</p>
<ol type="1">
<li><p>The current value of the variable <code>i</code>.</p></li>
<li><p>For every scalar variable <code>foo</code>, the value of <code>foo</code>.</p></li>
<li><p>For every array variable <code>Bar</code>, the value <code>Bar[</code><span><span class="math inline">\(j\)</span></span><code>]</code> for every <span><span class="math inline">\(j \in \{0,\ldots, t-1\}\)</span></span> where <span><span class="math inline">\(t-1\)</span></span> is the largest value that the index variable <code>i</code> ever achieved in the computation.</p></li>
</ol>
</div>
<h2 id="lambdacalculussec" data-number="7.5">Lambda calculus and functional programming languages</h2>
<p>The <a href="https://goo.gl/B9HwT8">λ calculus</a> is another way to define computable functions. It was proposed by Alonzo Church in the 1930’s around the same time as Alan Turing’s proposal of the Turing Machine. Interestingly, while Turing Machines are not used for practical computation, the λ calculus has inspired functional programming languages such as LISP, ML and Haskell, and indirectly the development of many other programming languages as well. In this section we will present the λ calculus and show that its power is equivalent to NAND-TM programs (and hence also to Turing machines). Our <a href="https://github.com/boazbk/tcscode">Github repository</a> contains a Jupyter notebook with a Python implementation of the λ calculus that you can experiment with to get a better feel for this topic.</p>
<p><strong>The λ operator.</strong> At the core of the λ calculus is a way to define “anonymous” functions. For example, instead of giving a name <span><span class="math inline">\(f\)</span></span> to a function and defining it as</p>
<p><span>
<div class='myequationbox'><span class="math display">\[
f(x) = x\times x
\]</span></div></span></p>
<p>we can write it as</p>
<p><span>
<div class='myequationbox'><span class="math display">\[
\lambda x. x\times x
\]</span></div></span></p>
<p>and so <span><span class="math inline">\((\lambda x.x\times x)(7)=49\)</span></span>. That is, you can think of <span><span class="math inline">\(\lambda x. exp(x)\)</span></span>, where <span><span class="math inline">\(exp\)</span></span> is some expression as a way of specifying the anonymous function <span><span class="math inline">\(x \mapsto exp(x)\)</span></span>. Anonymous functions, using either <span><span class="math inline">\(\lambda x.f(x)\)</span></span>, <span><span class="math inline">\(x \mapsto f(x)\)</span></span> or other closely related notation, appear in many programming languages. For example, in <em>Python</em> we can define the squaring function using <code>lambda x: x*x</code> while in <em>JavaScript</em> we can use <code>x =&gt; x*x</code> or <code>(x) =&gt; x*x</code>. In <em>Scheme</em> we would define it as <code>(lambda (x) (* x x))</code>. Clearly, the name of the argument to a function doesn’t matter, and so <span><span class="math inline">\(\lambda y.y\times y\)</span></span> is the same as <span><span class="math inline">\(\lambda x.x \times x\)</span></span>, as both correspond to the squaring function.</p>
<p><em>Dropping parenthesis.</em> To reduce notational clutter, when writing <span><span class="math inline">\(\lambda\)</span></span> calculus expressions we often drop the parentheses for function evaluation. Hence instead of writing <span><span class="math inline">\(f(x)\)</span></span> for the result of applying the function <span><span class="math inline">\(f\)</span></span> to the input <span><span class="math inline">\(x\)</span></span>, we can also write this as simply <span><span class="math inline">\(f\; x\)</span></span>. Therefore we can write <span><span class="math inline">\((\lambda x.x\times x) 7=49\)</span></span>. In this chapter, we will use both the <span><span class="math inline">\(f(x)\)</span></span> and <span><span class="math inline">\(f\; x\)</span></span> notations for function application. Function evaluations are associative and bind from left to right, and hence <span><span class="math inline">\(f\;g\;h\)</span></span> is the same as <span><span class="math inline">\((f g) h\)</span></span>.</p>
<h3 id="applying-functions-to-functions" data-number="7.5.1">Applying functions to functions</h3>
<p>A key feature of the λ calculus is that functions are “first-class objects” in the sense that we can use functions as arguments to other functions. For example, can you guess what number is the following expression equal to?</p>
<p><span>
<div class='myequationbox'><span class="math display">\[(((\lambda f.(\lambda y.(f \;(f\; y)))) (\lambda x. x\times x))\; 3) \;\;(7.5)\]</span><a id='lambdaexampleeq'></a></div></span></p>
<div class="pause" name="Pause">
<p>The expression <a href='#lambdaexampleeq'>Equation 7.5</a> might seem daunting, but before you look at the solution below, try to break it apart to its components, and evaluate each component at a time. Working out this example would go a long way toward understanding the λ calculus.</p>
</div>
<p>Let’s evaluate <a href='#lambdaexampleeq'>Equation 7.5</a> one step at a time. As nice as it is for the λ calculus to allow anonymous functions, adding names can be very helpful for understanding complicated expressions. So, let us write <span><span class="math inline">\(F = \lambda f.(\lambda y.(f (f y)))\)</span></span> and <span><span class="math inline">\(g = \lambda x.x\times x\)</span></span>.</p>
<p>Therefore <a href='#lambdaexampleeq'>Equation 7.5</a> becomes <span>
<div class='myequationbox'><span class="math display">\[
((F \; g)\;  3) \;.
\]</span></div></span></p>
<p>On input a function <span><span class="math inline">\(f\)</span></span>, <span><span class="math inline">\(F\)</span></span> outputs the function <span><span class="math inline">\(\lambda y.(f (f\; y))\)</span></span>, or in other words <span><span class="math inline">\(F f\)</span></span> is the function <span><span class="math inline">\(y \mapsto f(f(y))\)</span></span>. Our function <span><span class="math inline">\(g\)</span></span> is simply <span><span class="math inline">\(g(x)=x^2\)</span></span> and so <span><span class="math inline">\((F g)\)</span></span> is the function that maps <span><span class="math inline">\(y\)</span></span> to <span><span class="math inline">\((y^2)^2= y^4\)</span></span>. Hence <span><span class="math inline">\(((F g) 3) = 3^4 = 81\)</span></span>.</p>
<div id="lambdaexptwoex" class="solvedexercise" name="Solvedexercise 7.1">
<p>What number does the following expression equal to?</p>
<p><span>
<div class='myequationbox'><span class="math display">\[((\lambda x.(\lambda y.x)) \; 2)\; 9) \;. \;\;(7.7)\]</span><a id='lambdaexptwoeq'></a></div></span></p>
</div>
<div class="solution" data-ref="lambdaexptwoex" name="Solution 7.5.1">
<p><span><span class="math inline">\(\lambda y.x\)</span></span> is the function that on input <span><span class="math inline">\(y\)</span></span> ignores its input and outputs <span><span class="math inline">\(x\)</span></span>. Hence <span><span class="math inline">\((\lambda x.(\lambda y.x)) 2\)</span></span> yields the function <span><span class="math inline">\(y \mapsto 2\)</span></span> (or, using <span><span class="math inline">\(\lambda\)</span></span> notation, the function <span><span class="math inline">\(\lambda y. 2\)</span></span>). Hence <a href='#lambdaexptwoeq'>Equation 7.7</a> is equivalent to <span><span class="math inline">\((\lambda y. 2) 9 = 2\)</span></span>.</p>
</div>
<h3 id="curryingsec" data-number="7.5.2">Obtaining multi-argument functions via Currying</h3>
<p>In a λ expression of the form <span><span class="math inline">\(\lambda x. e\)</span></span>, the expression <span><span class="math inline">\(e\)</span></span> can itself involve the λ operator. Thus for example the function</p>
<p><span>
<div class='myequationbox'><span class="math display">\[
\lambda x. (\lambda y. x+y) \;\;(7.8)
\]</span><a id='eqlambdaexampleone'></a></div></span></p>
<p>maps <span><span class="math inline">\(x\)</span></span> to the function <span><span class="math inline">\(y \mapsto x+y\)</span></span>.</p>
<p>In particular, if we invoke the function <a href='#eqlambdaexampleone'>Equation 7.8</a> on <span><span class="math inline">\(a\)</span></span> to obtain some function <span><span class="math inline">\(f\)</span></span>, and then invoke <span><span class="math inline">\(f\)</span></span> on <span><span class="math inline">\(b\)</span></span>, we obtain the value <span><span class="math inline">\(a+b\)</span></span>. We can see that the one-argument function <a href='#eqlambdaexampleone'>Equation 7.8</a> corresponding to <span><span class="math inline">\(a \mapsto (b \mapsto a+b)\)</span></span> can also be thought of as the two-argument function <span><span class="math inline">\((a,b) \mapsto a+b\)</span></span>. Generally, we can use the λ expression <span><span class="math inline">\(\lambda x.(\lambda y.f(x,y))\)</span></span> to simulate the effect of a two argument function <span><span class="math inline">\((x,y) \mapsto f(x,y)\)</span></span>. This technique is known as <a href="https://en.wikipedia.org/wiki/Currying">Currying</a>. We will use the shorthand <span><span class="math inline">\(\lambda x,y. e\)</span></span> for <span><span class="math inline">\(\lambda x. (\lambda y. e)\)</span></span>. If <span><span class="math inline">\(f= \lambda x.(\lambda y.e)\)</span></span> then <span><span class="math inline">\((f a) b\)</span></span> corresponds to applying <span><span class="math inline">\(f a\)</span></span> and then invoking the resulting function on <span><span class="math inline">\(b\)</span></span>, obtaining the result of replacing in <span><span class="math inline">\(e\)</span></span> the occurrences of <span><span class="math inline">\(x\)</span></span> with <span><span class="math inline">\(a\)</span></span> and occurrences of <span><span class="math inline">\(b\)</span></span> with <span><span class="math inline">\(y\)</span></span>. By our rules of associativity, this is the same as <span><span class="math inline">\((f a b)\)</span></span> which we’ll sometimes also write as <span><span class="math inline">\(f(a,b)\)</span></span>.</p>
<figure>
<img src="../figure/currying.png" alt="7.14: In the “currying” transformation, we can create the effect of a two parameter function f(x,y) with the λ expression \lambda x.(\lambda y. f(x,y)) which on input x outputs a one-parameter function f_x that has x “hardwired” into it and such that f_x(y)=f(x,y). This can be illustrated by a circuit diagram; see Chelsea Voss’s site." id="currying" class="margin" /><figcaption>7.14: In the “currying” transformation, we can create the effect of a two parameter function <span><span class="math inline">\(f(x,y)\)</span></span> with the λ expression <span><span class="math inline">\(\lambda x.(\lambda y. f(x,y))\)</span></span> which on input <span><span class="math inline">\(x\)</span></span> outputs a one-parameter function <span><span class="math inline">\(f_x\)</span></span> that has <span><span class="math inline">\(x\)</span></span> “hardwired” into it and such that <span><span class="math inline">\(f_x(y)=f(x,y)\)</span></span>. This can be illustrated by a circuit diagram; see <a href="https://tromp.github.io/cl/diagrams.html">Chelsea Voss’s site</a>.</figcaption>
</figure>
<h3 id="formal-description-of-the-λ-calculus." data-number="7.5.3">Formal description of the λ calculus.</h3>
<p>We now provide a formal description of the λ calculus. We start with “basic expressions” that contain a single variable such as <span><span class="math inline">\(x\)</span></span> or <span><span class="math inline">\(y\)</span></span> and build more complex expressions of the form <span><span class="math inline">\((e \; e&#39;)\)</span></span> and <span><span class="math inline">\(\lambda x.e\)</span></span> where <span><span class="math inline">\(e,e&#39;\)</span></span> are expressions and <span><span class="math inline">\(x\)</span></span> is a variable idenifier. Formally λ expressions are defined as follows:</p>
<div id="lambdaexpdef" class="definition" title="λ expression." name="Definition 7.12 (λ expression.) ">
<p>A <em>λ expression</em> is either a single variable identifier or an expression <span><span class="math inline">\(e\)</span></span> of the one of the following forms:</p>
<ul>
<li><p><strong>Application:</strong> <span><span class="math inline">\(e = (e&#39; \; e&#39;&#39;)\)</span></span>, where <span><span class="math inline">\(e&#39;\)</span></span> and <span><span class="math inline">\(e&#39;&#39;\)</span></span> are λ expressions.</p></li>
<li><p><strong>Abstraction:</strong> <span><span class="math inline">\(e = \lambda x.(e&#39;)\)</span></span> where <span><span class="math inline">\(e&#39;\)</span></span> is a λ expression.</p></li>
</ul>
</div>
<p><a href='#lambdaexpdef'>Definition 7.12</a> is a <em>recursive definition</em> since we defined the concept of λ expressions in terms of itself. This might seem confusing at first, but in fact you have known recursive definitions since you were an elementary school student. Consider how we define an <em>arithmetic expression</em>: it is an expression that is either just a number, or has one of the forms <span><span class="math inline">\((e + e&#39;)\)</span></span>, <span><span class="math inline">\((e - e&#39;)\)</span></span>, <span><span class="math inline">\((e \times e&#39;)\)</span></span>, or <span><span class="math inline">\((e \div e&#39;)\)</span></span>, where <span><span class="math inline">\(e\)</span></span> and <span><span class="math inline">\(e&#39;\)</span></span> are other arithmetic expressions.</p>
<p><em>Free and bound variables.</em> Variables in a λ expression can either be <em>free</em> or <em>bound</em> to a <span><span class="math inline">\(\lambda\)</span></span> operator (in the sense of <a href='lec_00_1_math_background.html#boundvarsec'>Subsection 1.4.7</a>). In a single-variable λ expression <span><span class="math inline">\(var\)</span></span>, the variable <span><span class="math inline">\(var\)</span></span> is free. The set of free and bound variables in an application expression <span><span class="math inline">\(e = (e&#39; \; e&#39;&#39;)\)</span></span> is the same as that of the underlying expressions <span><span class="math inline">\(e&#39;\)</span></span> and <span><span class="math inline">\(e&#39;&#39;\)</span></span>. In an abstraction expression <span><span class="math inline">\(e = \lambda var.(e&#39;)\)</span></span>, all free occurences of <span><span class="math inline">\(var\)</span></span> in <span><span class="math inline">\(e&#39;\)</span></span> are bound to the <span><span class="math inline">\(\lambda\)</span></span> operator of <span><span class="math inline">\(e\)</span></span>. If you find the notion of free and bound variables confusing, you can avoid all these issues by using unique identifiers for all variables.</p>
<p><em>Precedence and parenthesis.</em> We will use the following rules to allow us to drop some parenthesis. Function application associates from left to right, and so <span><span class="math inline">\(fgh\)</span></span> is the same as <span><span class="math inline">\((fg)h\)</span></span>. Function application has a higher precedence than the λ operator, and so <span><span class="math inline">\(\lambda x.fgx\)</span></span> is the same as <span><span class="math inline">\(\lambda x.((fg)x)\)</span></span>. This is similar to how we use the precedence rules in arithmetic operations to allow us to use fewer parenthesis and so write the expression <span><span class="math inline">\((7 \times 3) + 2\)</span></span> as <span><span class="math inline">\(7\times 3 + 2\)</span></span>. As mentioned in <a href='#curryingsec'>Subsection 7.5.2</a>, we also use the shorthand <span><span class="math inline">\(\lambda x,y.e\)</span></span> for <span><span class="math inline">\(\lambda x.(\lambda y.e)\)</span></span> and the shorthand <span><span class="math inline">\(f(x,y)\)</span></span> for <span><span class="math inline">\((f\; x)\; y\)</span></span>. This plays nicely with the “Currying” transformation of simulating multi-input functions using λ expressions.</p>
<p><strong>Equivalence of λ expressions.</strong> As we have seen in <a href='#lambdaexptwoex'>Solvedexercise 7.1</a>, the rule that <span><span class="math inline">\((\lambda x. exp) exp&#39;\)</span></span> is equivalent to <span><span class="math inline">\(exp[x \rightarrow exp&#39;]\)</span></span> enables us to modify λ expressions and obtain simpler <em>equivalent form</em> for them. Another rule that we can use is that the parameter does not matter and hence for example <span><span class="math inline">\(\lambda y.y\)</span></span> is the same as <span><span class="math inline">\(\lambda z.z\)</span></span>. Together these rules define the notion of <em>equivalence</em> of λ expressions:</p>
<div id="lambdaequivalence" class="definition" title="Equivalence of λ expressions" name="Definition 7.13 (Equivalence of λ expressions) ">
<p>Two λ expressions are <em>equivalent</em> if they can be made into the same expression by repeated applications of the following rules:</p>
<ol type="1">
<li><p><strong>Evaluation (aka <span><span class="math inline">\(\beta\)</span></span> reduction):</strong> The expression <span><span class="math inline">\((\lambda x.exp) exp&#39;\)</span></span> is equivalent to <span><span class="math inline">\(exp[x \rightarrow exp&#39;]\)</span></span>.</p></li>
<li><p><strong>Variable renaming (aka <span><span class="math inline">\(\alpha\)</span></span> conversion):</strong> The expression <span><span class="math inline">\(\lambda x.exp\)</span></span> is equivalent to <span><span class="math inline">\(\lambda y.exp[x \rightarrow y]\)</span></span>.</p></li>
</ol>
</div>
<p>If <span><span class="math inline">\(exp\)</span></span> is a λ expression of the form <span><span class="math inline">\(\lambda x.exp&#39;\)</span></span> then it naturally corresponds to the function that maps any input <span><span class="math inline">\(z\)</span></span> to <span><span class="math inline">\(exp&#39;[x \rightarrow z]\)</span></span>. Hence the λ calculus naturally implies a computational model. Since in the λ calculus the inputs can themselves be functions, we need to decide in what order we evaluate an expression such as</p>
<p><span>
<div class='myequationbox'><span class="math display">\[
(\lambda x.f)(\lambda y.g z) \;. \;\;(7.9)
\]</span><a id='lambdaexpeq'></a></div></span> There are two natural conventions for this:</p>
<ul>
<li><p><em>Call by name</em> (aka <em>“lazy evaluation”</em>): We evaluate <a href='#lambdaexpeq'>Equation 7.9</a> by first plugging in the righthand expression <span><span class="math inline">\((\lambda y.g z)\)</span></span> as input to the lefthand side function, obtaining <span><span class="math inline">\(f[x \rightarrow (\lambda y.g z)]\)</span></span> and then continue from there.</p></li>
<li><p><em>Call by value</em> (aka <em>“eager evaluation”</em>): We evaluate <a href='#lambdaexpeq'>Equation 7.9</a> by first evaluating the righthand side and obtaining <span><span class="math inline">\(h=g[y \rightarrow z]\)</span></span>, and then plugging this into the lefthandside to obtain <span><span class="math inline">\(f[x \rightarrow h]\)</span></span>.</p></li>
</ul>
<p>Because the λ calculus has only <em>pure</em> functions, that do not have “side effects”, in many cases the order does not matter. In fact, it can be shown that if we obtain a definite irreducible expression (for example, a number) in both strategies, then it will be the same one. However, for concreteness we will always use the “call by name” (i.e., lazy evaluation) order. (The same choice is made in the programming language Haskell, though many other programming languages use eager evaluation.) Formally, the evaluation of a λ expression using “call by name” is captured by the following process:</p>
<div id="simplifylambdadef" class="definition" title="Simplification of λ expressions" name="Definition 7.14 (Simplification of λ expressions) ">
<p>Let <span><span class="math inline">\(e\)</span></span> be a λ expression. The <em>simplification</em> of <span><span class="math inline">\(e\)</span></span> is the result of the following recursive process:</p>
<ol type="1">
<li><p>If <span><span class="math inline">\(e\)</span></span> is a single variable <span><span class="math inline">\(x\)</span></span> then the simplification of <span><span class="math inline">\(e\)</span></span> is <span><span class="math inline">\(e\)</span></span>.</p></li>
<li><p>If <span><span class="math inline">\(e\)</span></span> has the form <span><span class="math inline">\(e= \lambda x.e&#39;\)</span></span> then the simplification of <span><span class="math inline">\(e\)</span></span> is <span><span class="math inline">\(\lambda x.f&#39;\)</span></span> where <span><span class="math inline">\(f&#39;\)</span></span> is the simplification of <span><span class="math inline">\(e&#39;\)</span></span>.</p></li>
<li><p><em>(Evaluation / <span><span class="math inline">\(\beta\)</span></span> reduction.)</em> If <span><span class="math inline">\(e\)</span></span> has the form <span><span class="math inline">\(e=(\lambda x.e&#39; \; e&#39;&#39;)\)</span></span> then the simplification of <span><span class="math inline">\(e\)</span></span> is the simplification of <span><span class="math inline">\(e&#39;[x \rightarrow e&#39;&#39;]\)</span></span>, which denotes replacing all copies of <span><span class="math inline">\(x\)</span></span> in <span><span class="math inline">\(e&#39;\)</span></span> bound to the <span><span class="math inline">\(\lambda\)</span></span> operator with <span><span class="math inline">\(e&#39;&#39;\)</span></span></p></li>
<li><p><em>(Renaming / <span><span class="math inline">\(\alpha\)</span></span> conversion.)</em> The <em>canonical simplification</em> of <span><span class="math inline">\(e\)</span></span> is obtained by taking the simplification of <span><span class="math inline">\(e\)</span></span> and renaming the variables so that the first bound variable in the expression is <span><span class="math inline">\(v_0\)</span></span>, the second one is <span><span class="math inline">\(v_1\)</span></span>, and so on and so forth.</p></li>
</ol>
<p>We say that two λ expressions <span><span class="math inline">\(e\)</span></span> and <span><span class="math inline">\(e&#39;\)</span></span> are <em>equivalent</em>, denoted by <span><span class="math inline">\(e \cong e&#39;\)</span></span>, if they have the same canonical simplification.</p>
</div>
<div id="lambdaeuivexer" class="solvedexercise" title="Equivalence of λ expressions" name="Solvedexercise 7.2 (Equivalence of λ expressions) ">
<p>Prove that the following two expressions <span><span class="math inline">\(e\)</span></span> and <span><span class="math inline">\(f\)</span></span> are equivalent:</p>
<p><span>
<div class='myequationbox'><span class="math display">\[e = \lambda x.x\]</span></div></span></p>
<p><span>
<div class='myequationbox'><span class="math display">\[f = (\lambda a.(\lambda b.b)) (\lambda z.zz)\]</span></div></span></p>
</div>
<div class="solution" data-ref="lambdaeuivexer" name="Solution 7.5.3">
<p>The canonical simplification of <span><span class="math inline">\(e\)</span></span> is simply <span><span class="math inline">\(\lambda v_0.v_0\)</span></span>. To do the canonical simplification of <span><span class="math inline">\(f\)</span></span> we first use <span><span class="math inline">\(\beta\)</span></span> reduction to plug in <span><span class="math inline">\(\lambda z.zz\)</span></span> instead of <span><span class="math inline">\(a\)</span></span> in <span><span class="math inline">\((\lambda b.b)\)</span></span> but since <span><span class="math inline">\(a\)</span></span> is not used in this function at all, we simply obtained <span><span class="math inline">\(\lambda b.b\)</span></span> which simplifies to <span><span class="math inline">\(\lambda v_0.v_0\)</span></span> as well.</p>
</div>
<h3 id="infiniteloopslambda" data-number="7.5.4">Infinite loops in the λ calculus</h3>
<p>Like Turing machines and NAND-TM programs, the simplification process in the λ calculus can also enter into an infinite loop. For example, consider the λ expression</p>
<p><span>
<div class='myequationbox'><span class="math display">\[
\lambda x.xx \; \lambda x.xx \;\;(7.12)
\]</span><a id='lambdainfloopeq'></a></div></span></p>
<p>If we try to simplify <a href='#lambdainfloopeq'>Equation 7.12</a> by invoking the lefthand function on the righthand one, then we get another copy of <a href='#lambdainfloopeq'>Equation 7.12</a> and hence this never ends. There are examples where the order of evaluation can matter for whether or not an expression can be simplified, see <a href='#evalorderlambdaex'>Exercise 7.9</a>.</p>
<h2 id="the-enhanced-λ-calculus" data-number="7.6">The “Enhanced” λ calculus</h2>
<p>We now discuss the λ calculus as a computational model. We will start by describing an “enhanced” version of the λ calculus that contains some “superfluous features” but is easier to wrap your head around. We will first show how the enhanced λ calculus is equivalent to Turing machines in computational power. Then we will show how all the features of “enhanced λ calculus” can be implemented as “syntactic sugar” on top of the “pure” (i.e., non enhanced) λ calculus. Hence the pure λ calculus is equivalent in power to Turing machines (and hence also to RAM machines and all other Turing-equivalent models).</p>
<p>The <em>enhanced λ calculus</em> includes the following set of objects and operations:</p>
<ul>
<li><p><strong>Boolean constants and IF function:</strong> There are λ expressions <span><span class="math inline">\(0\)</span></span>, <span><span class="math inline">\(1\)</span></span> and <span><span class="math inline">\(\ensuremath{\mathit{IF}}\)</span></span> that satisfy the following conditions: for every λ expression <span><span class="math inline">\(e\)</span></span> and <span><span class="math inline">\(f\)</span></span>, <span><span class="math inline">\(\ensuremath{\mathit{IF}}\; 1\;e\;f = e\)</span></span> and <span><span class="math inline">\(\ensuremath{\mathit{IF}}\;0\;e\;f = f\)</span></span>. That is, <span><span class="math inline">\(\ensuremath{\mathit{IF}}\)</span></span> is the function that given three arguments <span><span class="math inline">\(a,e,f\)</span></span> outputs <span><span class="math inline">\(e\)</span></span> if <span><span class="math inline">\(a=1\)</span></span> and <span><span class="math inline">\(f\)</span></span> if <span><span class="math inline">\(a=0\)</span></span>.</p></li>
<li><p><strong>Pairs:</strong> There is a λ expression <span><span class="math inline">\(\ensuremath{\mathit{PAIR}}\)</span></span> which we will think of as the <em>pairing</em> function. For every λ expressions <span><span class="math inline">\(e,f\)</span></span>, <span><span class="math inline">\(\ensuremath{\mathit{PAIR}}\; e\; f\)</span></span> is the pair <span><span class="math inline">\(\langle e,f \rangle\)</span></span> that contains <span><span class="math inline">\(e\)</span></span> as its first member and <span><span class="math inline">\(f\)</span></span> as its second member. We also have λ expressions <span><span class="math inline">\(\ensuremath{\mathit{HEAD}}\)</span></span> and <span><span class="math inline">\(\ensuremath{\mathit{TAIL}}\)</span></span> that extract the first and second member of a pair respectively. Hence, for every λ expressions <span><span class="math inline">\(e,f\)</span></span>, <span><span class="math inline">\(\ensuremath{\mathit{HEAD}}\; (\ensuremath{\mathit{PAIR}} \; e\; f) = e\)</span></span> and <span><span class="math inline">\(\ensuremath{\mathit{TAIL}} \; (\ensuremath{\mathit{PAIR}} \; e\; f) = f\)</span></span>.<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup></p></li>
<li><p><strong>Lists and strings:</strong> There is λ expression <span><span class="math inline">\(\ensuremath{\mathit{NIL}}\)</span></span> that corresponds to the <em>empty list</em>, which we also denote by <span><span class="math inline">\(\langle \bot \rangle\)</span></span>. Using <span><span class="math inline">\(\ensuremath{\mathit{PAIR}}\)</span></span> and <span><span class="math inline">\(\ensuremath{\mathit{NIL}}\)</span></span> we construct <em>lists</em>. The idea is that if <span><span class="math inline">\(L\)</span></span> is a <span><span class="math inline">\(k\)</span></span> element list of the form <span><span class="math inline">\(\langle e_1, e_2, \ldots, e_k, \bot \rangle\)</span></span> then for every λ expression <span><span class="math inline">\(e_0\)</span></span> we can obtain the <span><span class="math inline">\(k+1\)</span></span> element list <span><span class="math inline">\(\langle e_0,e_1, e_2, \ldots, e_k, \bot \rangle\)</span></span> using the expression <span><span class="math inline">\(\ensuremath{\mathit{PAIR}}\; e_0 \; L\)</span></span>. For example, for every three λ expressions <span><span class="math inline">\(e,f,g\)</span></span>, the following corresponds to the three element list <span><span class="math inline">\(\langle e,f,g,\bot \rangle\)</span></span>: <span>
<div class='myequationbox'><span class="math display">\[
\ensuremath{\mathit{PAIR}} \; e \; \left(\ensuremath{\mathit{PAIR}}\; f \; \left( \ensuremath{\mathit{PAIR}}\; g \; \ensuremath{\mathit{NIL}} \right) \right) \;.
\]</span></div></span></p></li>
</ul>
<p>The λ expression <span><span class="math inline">\(\ensuremath{\mathit{ISEMPTY}}\)</span></span> returns <span><span class="math inline">\(1\)</span></span> on <span><span class="math inline">\(\ensuremath{\mathit{NIL}}\)</span></span> and returns <span><span class="math inline">\(0\)</span></span> on every other list. A <em>string</em> is simply a list of bits.</p>
<ul>
<li><strong>List operations:</strong> The enhanced λ calculus also contains the <em>list-processing functions</em> <span><span class="math inline">\(\ensuremath{\mathit{MAP}}\)</span></span>, <span><span class="math inline">\(\ensuremath{\mathit{REDUCE}}\)</span></span>, and <span><span class="math inline">\(\ensuremath{\mathit{FILTER}}\)</span></span>. Given a list <span><span class="math inline">\(L= \langle x_0,\ldots,x_{n-1}, \bot \rangle\)</span></span> and a function <span><span class="math inline">\(f\)</span></span>, <span><span class="math inline">\(\ensuremath{\mathit{MAP}}\; L \; f\)</span></span> applies <span><span class="math inline">\(f\)</span></span> on every member of the list to obtain the new list <span><span class="math inline">\(L&#39;= \langle f(x_0),\ldots,f(x_{n-1}), \bot \rangle\)</span></span>. Given a list <span><span class="math inline">\(L\)</span></span> as above and an expression <span><span class="math inline">\(f\)</span></span> whose output is either <span><span class="math inline">\(0\)</span></span> or <span><span class="math inline">\(1\)</span></span>, <span><span class="math inline">\(\ensuremath{\mathit{FILTER}}\; L\; f\)</span></span> returns the list <span><span class="math inline">\(\langle x_i \rangle_{f x_i = 1}\)</span></span> containing all the elements of <span><span class="math inline">\(L\)</span></span> for which <span><span class="math inline">\(f\)</span></span> outputs <span><span class="math inline">\(1\)</span></span>. The function <span><span class="math inline">\(\ensuremath{\mathit{REDUCE}}\)</span></span> applies a “combining” operation to a list. For example, <span><span class="math inline">\(\ensuremath{\mathit{REDUCE}}\; L \; + \; 0\)</span></span> will return the sum of all the elements in the list <span><span class="math inline">\(L\)</span></span>. More generally, <span><span class="math inline">\(\ensuremath{\mathit{REDUCE}}\)</span></span> takes a list <span><span class="math inline">\(L\)</span></span>, an operation <span><span class="math inline">\(f\)</span></span> (which we think of as taking two arguments) and a λ expression <span><span class="math inline">\(z\)</span></span> (which we think of as the “neutral element” for the operation <span><span class="math inline">\(f\)</span></span>, such as <span><span class="math inline">\(0\)</span></span> for addition and <span><span class="math inline">\(1\)</span></span> for multiplication). The output is defined via</li>
</ul>
<p><span>
<div class='myequationbox'><span class="math display">\[\ensuremath{\mathit{REDUCE}}\;L\;f\;z = \begin{cases}z &amp; L=\ensuremath{\mathit{NIL}} \\ f\;(\ensuremath{\mathit{HEAD}}\; L) \; (\ensuremath{\mathit{REDUCE}}\;(\ensuremath{\mathit{TAIL}}\; L)\;f\;z)  &amp; \text{otherwise}\end{cases}\;.\]</span></div></span> See <a href='#reduceetalfig'>Figure 7.16</a> for an illustration of the three list-processing operations.</p>
<ul>
<li><strong>Recursion:</strong> Finally, we want to be able to execute <em>recursive functions</em>. Since in λ calculus functions are <em>anonymous</em>, we can’t write a definition of the form <span><span class="math inline">\(f(x) = blah\)</span></span> where <span><span class="math inline">\(blah\)</span></span> includes calls to <span><span class="math inline">\(f\)</span></span>. Instead we use functions <span><span class="math inline">\(f\)</span></span> that take an additional input <span><span class="math inline">\(me\)</span></span> as a parameter. The operator <span><span class="math inline">\(\ensuremath{\mathit{RECURSE}}\)</span></span> will take such a function <span><span class="math inline">\(f\)</span></span> as input and return a “recursive version” of <span><span class="math inline">\(f\)</span></span> where all the calls to <span><span class="math inline">\(me\)</span></span> are replaced by recursive calls to this function. That is, if we have a function <span><span class="math inline">\(F\)</span></span> taking two parameters <span><span class="math inline">\(me\)</span></span> and <span><span class="math inline">\(x\)</span></span>, then <span><span class="math inline">\(\ensuremath{\mathit{RECURSE}}\; F\)</span></span> will be the function <span><span class="math inline">\(f\)</span></span> taking one parameter <span><span class="math inline">\(x\)</span></span> such that <span><span class="math inline">\(f(x) = F(f,x)\)</span></span> for every <span><span class="math inline">\(x\)</span></span>.</li>
</ul>
<div id="NANDlambdaex" class="solvedexercise" title="Compute NAND using λ calculus" name="Solvedexercise 7.3 (Compute NAND using λ calculus) ">
<p>Give a λ expression <span><span class="math inline">\(N\)</span></span> such that <span><span class="math inline">\(N\;x\;y = \ensuremath{\mathit{NAND}}(x,y)\)</span></span> for every <span><span class="math inline">\(x,y \in \{0,1\}\)</span></span>.</p>
</div>
<div class="solution" data-ref="NANDlambdaex" name="Section 14.4">
<p>The <span><span class="math inline">\(\ensuremath{\mathit{NAND}}\)</span></span> of <span><span class="math inline">\(x,y\)</span></span> is equal to <span><span class="math inline">\(1\)</span></span> unless <span><span class="math inline">\(x=y=1\)</span></span>. Hence we can write</p>
<p><span>
<div class='myequationbox'><span class="math display">\[
N = \lambda x,y.\ensuremath{\mathit{IF}}(x,\ensuremath{\mathit{IF}}(y,0,1),1)
\]</span></div></span></p>
</div>
<div id="XORlambdaex" class="solvedexercise" title="Compute XOR using λ calculus" name="Solvedexercise 7.4 (Compute XOR using λ calculus) ">
<p>Give a λ expression <span><span class="math inline">\(\ensuremath{\mathit{XOR}}\)</span></span> such that for every list <span><span class="math inline">\(L=\langle x_0, \ldots, x_{n-1}, \bot \rangle\)</span></span> where <span><span class="math inline">\(x_i \in \{0,1\}\)</span></span> for <span><span class="math inline">\(i\in [n]\)</span></span>, <span><span class="math inline">\(\ensuremath{\mathit{XOR}} L\)</span></span> evaluates to <span><span class="math inline">\(\sum x_i \mod 2\)</span></span>.</p>
</div>
<div class="solution" data-ref="XORlambdaex" name="Solution 7.6">
<p>First, we note that we can compute XOR of two bits as follows: <span>
<div class='myequationbox'><span class="math display">\[
\ensuremath{\mathit{NOT}} = \lambda a. \ensuremath{\mathit{IF}}(a,0,1) \;\;(7.16)
\]</span><a id='lambdanot'></a></div></span> and <span>
<div class='myequationbox'><span class="math display">\[
\ensuremath{\mathit{XOR}}_2 = \lambda a,b. \ensuremath{\mathit{IF}}(b,\ensuremath{\mathit{NOT}}(a),a) \;\;(7.17)
\]</span><a id='lambdaxor'></a></div></span></p>
<p>(We are using here a bit of syntactic sugar to describe the functions. To obtain the λ expression for XOR we will simply replace the expression <a href='#lambdanot'>Equation 7.16</a> in <a href='#lambdaxor'>Equation 7.17</a>.) Now recursively we can define the XOR of a list as follows:</p>
<p><span>
<div class='myequationbox'><span class="math display">\[
\ensuremath{\mathit{XOR}}(L) = \begin{cases} 0 &amp; \text{$L$ is empty} \\
\ensuremath{\mathit{XOR}}_2(\ensuremath{\mathit{HEAD}}(L),\ensuremath{\mathit{XOR}}(\ensuremath{\mathit{TAIL}}(L))) &amp; \text{otherwise}
\end{cases}
\]</span></div></span></p>
<p>This means that <span><span class="math inline">\(\ensuremath{\mathit{XOR}}\)</span></span> is equal to</p>
<p><span>
<div class='myequationbox'><span class="math display">\[
\ensuremath{\mathit{RECURSE}} \;  \bigl(\lambda me,L. \ensuremath{\mathit{IF}}(\ensuremath{\mathit{ISEMPTY}}(L),0,\ensuremath{\mathit{XOR}}_2(\ensuremath{\mathit{HEAD}}\;L\;\;,\;\;me(\ensuremath{\mathit{TAIL}} \; L)))\bigr) \;.
\]</span></div></span></p>
<p>That is, <span><span class="math inline">\(\ensuremath{\mathit{XOR}}\)</span></span> is obtained by applying the <span><span class="math inline">\(\ensuremath{\mathit{RECURSE}}\)</span></span> operator to the function that on inputs <span><span class="math inline">\(me\)</span></span>, <span><span class="math inline">\(L\)</span></span>, returns <span><span class="math inline">\(0\)</span></span> if <span><span class="math inline">\(\ensuremath{\mathit{ISEMPTY}}(L)\)</span></span> and otherwise returns <span><span class="math inline">\(\ensuremath{\mathit{XOR}}_2\)</span></span> applied to <span><span class="math inline">\(\ensuremath{\mathit{HEAD}}(L)\)</span></span> and to <span><span class="math inline">\(me(\ensuremath{\mathit{TAIL}}(L))\)</span></span>.</p>
<p>We could have also computed <span><span class="math inline">\(\ensuremath{\mathit{XOR}}\)</span></span> using the <span><span class="math inline">\(\ensuremath{\mathit{REDUCE}}\)</span></span> operation, we leave working this out as an exercise to the reader.</p>
</div>
<figure>
<img src="../figure/lambdalist.png" alt="7.15: A list \langle x_0,x_1,x_2 \rangle in the λ calculus is constructed from the tail up, building the pair \langle x_2,\ensuremath{\mathit{NIL}}\rangle, then the pair \langle x_1, \langle x_2,\ensuremath{\mathit{NIL}}\rangle \rangle and finally the pair \langle x_0,\langle x_1,\langle x_2,\ensuremath{\mathit{NIL}} \rangle\rangle\rangle. That is, a list is a pair where the first element of the pair is the first element of the list and the second element is the rest of the list. The figure on the left renders this “pairs inside pairs” construction, though it is often easier to think of a list as a “chain”, as in the figure on the right, where the second element of each pair is thought of as a link, pointer or reference to the remainder of the list." id="lambdalistfig" /><figcaption>7.15: A list <span><span class="math inline">\(\langle x_0,x_1,x_2 \rangle\)</span></span> in the λ calculus is constructed from the tail up, building the pair <span><span class="math inline">\(\langle x_2,\ensuremath{\mathit{NIL}}\rangle\)</span></span>, then the pair <span><span class="math inline">\(\langle x_1, \langle x_2,\ensuremath{\mathit{NIL}}\rangle \rangle\)</span></span> and finally the pair <span><span class="math inline">\(\langle x_0,\langle x_1,\langle x_2,\ensuremath{\mathit{NIL}} \rangle\rangle\rangle\)</span></span>. That is, a list is a pair where the first element of the pair is the first element of the list and the second element is the rest of the list. The figure on the left renders this “pairs inside pairs” construction, though it is often easier to think of a list as a “chain”, as in the figure on the right, where the second element of each pair is thought of as a <em>link</em>, <em>pointer</em> or <em>reference</em> to the remainder of the list.</figcaption>
</figure>
<figure>
<img src="../figure/reducemapfilter.png" alt="7.16: Illustration of the \ensuremath{\mathit{MAP}}, \ensuremath{\mathit{FILTER}} and \ensuremath{\mathit{REDUCE}} operations." id="reduceetalfig" /><figcaption>7.16: Illustration of the <span><span class="math inline">\(\ensuremath{\mathit{MAP}}\)</span></span>, <span><span class="math inline">\(\ensuremath{\mathit{FILTER}}\)</span></span> and <span><span class="math inline">\(\ensuremath{\mathit{REDUCE}}\)</span></span> operations.</figcaption>
</figure>
<h3 id="computing-a-function-in-the-enhanced-λ-calculus" data-number="7.6.1">Computing a function in the enhanced λ calculus</h3>
<p>An <em>enhanced λ expression</em> is obtained by composing the objects above with the <em>application</em> and <em>abstraction</em> rules. The result of simplifying a λ expression is an equivalent expression, and hence if two expressions have the same simplification then they are equivalent.</p>
<div id="lambdacomputedef" class="definition" title="Computing a function via λ calculus" name="Definition 7.15 (Computing a function via λ calculus) ">
<p>Let <span><span class="math inline">\(F:\{0,1\}^* \rightarrow \{0,1\}^*\)</span></span></p>
<p>We say that <em><span><span class="math inline">\(exp\)</span></span> computes <span><span class="math inline">\(F\)</span></span></em> if for every <span><span class="math inline">\(x\in \{0,1\}^*\)</span></span>,</p>
<p><span>
<div class='myequationbox'><span class="math display">\[exp \langle x_0,\ldots,x_{n-1},\bot \rangle \cong \langle y_0,\ldots, y_{m-1}, \bot \rangle\]</span></div></span></p>
<p>where <span><span class="math inline">\(n=|x|\)</span></span>, <span><span class="math inline">\(y=F(x)\)</span></span>, and <span><span class="math inline">\(m=|y|\)</span></span>, and the notion of equivalence is defined as per <a href='#simplifylambdadef'>Definition 7.14</a>.</p>
</div>
<h3 id="enhanced-λ-calculus-is-turing-complete" data-number="7.6.2">Enhanced λ calculus is Turing-complete</h3>
<p>The basic operations of the enhanced λ calculus more or less amount to the Lisp or Scheme programming languages. Given that, it is perhaps not surprising that the enhanced λ-calculus is equivalent to Turing machines:</p>
<div id="lambdaturing-thm" class="theorem" title="Lambda calculus and NAND-TM" name="Exercise 7.11 (Lambda calculus and NAND-TM) ">
<p>For every function <span><span class="math inline">\(F:\{0,1\}^* \rightarrow \{0,1\}^*\)</span></span>, <span><span class="math inline">\(F\)</span></span> is computable in the enhanced λ calculus if and only if it is computable by a Turing machine.</p>
</div>
<div class="proofidea" data-ref="lambdaturing-thm" name="Proof 13.3">
<p>To prove the theorem, we need to show that <strong>(1)</strong> if <span><span class="math inline">\(F\)</span></span> is computable by a λ calculus expression then it is computable by a Turing machine, and <strong>(2)</strong> if <span><span class="math inline">\(F\)</span></span> is computable by a Turing machine, then it is computable by an enhanced λ calculus expression.</p>
<p>Showing <strong>(1)</strong> is fairly straightforward. Applying the simplification rules to a λ expression basically amounts to “search and replace” which we can implement easily in, say, NAND-RAM, or for that matter Python (both of which are equivalent to Turing machines in power). Showing <strong>(2)</strong> essentially amounts to simulating a Turing machine (or writing a NAND-TM interpreter) in a functional programming language such as LISP or Scheme. We give the details below but how this can be done is a good exercise in mastering some functional programming techniques that are useful in their own right.</p>
</div>
<div class="proof" data-ref="lambdaturing-thm" name="Solution 12.6.2">
<p>We only sketch the proof. The “if” direction is simple. As mentioned above, evaluating λ expressions basically amounts to “search and replace”. It is also a fairly straightforward programming exercise to implement all the above basic operations in an imperative language such as Python or C, and using the same ideas we can do so in NAND-RAM as well, which we can then transform to a NAND-TM program.</p>
<p>For the “only if” direction we need to simulate a Turing machine using a λ expression. We will do so by first showing that showing for every Turing machine <span><span class="math inline">\(M\)</span></span> a λ expression to compute the next-step function <span><span class="math inline">\(\ensuremath{\mathit{NEXT}}_M:\overline{\Sigma}^* \rightarrow \overline{\Sigma}^*\)</span></span> that maps a configuration of <span><span class="math inline">\(M\)</span></span> to the next one (see <a href='#turingmachinesconfigsec'>Subsection 7.4.2</a>).</p>
<p>A configuration of <span><span class="math inline">\(M\)</span></span> is a string <span><span class="math inline">\(\alpha \in \overline{\Sigma}^*\)</span></span> for a finite set <span><span class="math inline">\(\overline{\Sigma}\)</span></span>. We can encode every symbol <span><span class="math inline">\(\sigma \in \overline{\Sigma}\)</span></span> by a finite string <span><span class="math inline">\(\{0,1\}^\ell\)</span></span>, and so we will encode a configuration <span><span class="math inline">\(\alpha\)</span></span> in the λ calculus as a list <span><span class="math inline">\(\langle \alpha_0, \alpha_1, \ldots, \alpha_{m-1}, \bot \rangle\)</span></span> where <span><span class="math inline">\(\alpha_i\)</span></span> is an <span><span class="math inline">\(\ell\)</span></span>-length string (i.e., an <span><span class="math inline">\(\ell\)</span></span>-length list of <span><span class="math inline">\(0\)</span></span>’s and <span><span class="math inline">\(1\)</span></span>’s) encoding a symbol in <span><span class="math inline">\(\overline{\Sigma}\)</span></span>.</p>
<p>By <a href='#nextstepfunctionlem'>Lemma 7.9</a>, for every <span><span class="math inline">\(\alpha \in \overline{\Sigma}^*\)</span></span>, <span><span class="math inline">\(\ensuremath{\mathit{NEXT}}_M(\alpha)_i\)</span></span> is equal to <span><span class="math inline">\(r(\alpha_{i-1},\alpha_i,\alpha_{i+1})\)</span></span> for some finite function <span><span class="math inline">\(r:\overline{\Sigma}^3 \rightarrow \overline{\Sigma}\)</span></span>. Using our encoding of <span><span class="math inline">\(\overline{\Sigma}\)</span></span> as <span><span class="math inline">\(\{0,1\}^\ell\)</span></span>, we can also think of <span><span class="math inline">\(r\)</span></span> as mapping <span><span class="math inline">\(\{0,1\}^{3\ell}\)</span></span> to <span><span class="math inline">\(\{0,1\}^\ell\)</span></span>. By <a href='#NANDlambdaex'>Solvedexercise 7.3</a>, we can compute the <span><span class="math inline">\(\ensuremath{\mathit{NAND}}\)</span></span> function, and hence <em>every</em> finite function, including <span><span class="math inline">\(r\)</span></span>, using the λ calculus. Using this insight, we can compute <span><span class="math inline">\(\ensuremath{\mathit{NEXT}}_M\)</span></span> using the λ calculus as follows. Given a list <span><span class="math inline">\(L\)</span></span> encoding the configuration <span><span class="math inline">\(\alpha_0\cdots \alpha_{m-1}\)</span></span>, we define the lists <span><span class="math inline">\(L_{prev}\)</span></span> and <span><span class="math inline">\(L_{next}\)</span></span> encoding the configuration <span><span class="math inline">\(\alpha\)</span></span> shifted by one step to the right and left respectively. The next configuration <span><span class="math inline">\(\alpha&#39;\)</span></span> is defined as <span><span class="math inline">\(\alpha&#39;_i = r(L_{prev}[i],L[i],L_{next}[i])\)</span></span> where we let <span><span class="math inline">\(L&#39;[i]\)</span></span> denote the <span><span class="math inline">\(i\)</span></span>-th element of <span><span class="math inline">\(L&#39;\)</span></span>. This can be computed by recursion (and hence using the enhanced λ calculus’ <span><span class="math inline">\(\ensuremath{\mathit{RECURSE}}\)</span></span> operator) as follows:</p>
<div  class="pseudocodeoutput">
<div class="ps-root">
<div class="ps-algorithm with-caption" id = nextmlambdacalc>
<p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;">
<span class="ps-keyword">Algorithm 17 </span>$NEXT_M$ using the λ calculus</p>
<div class="ps-algorithmic"><p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"><span class="ps-keyword">Input:</span>  List \(L = \langle \alpha_0,\alpha_1,\ldots, \alpha_{m-1}, \bot \rangle\) encoding a configuration \(\alpha\).<p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"><span class="ps-keyword">Output:</span>  List \(L'\) encoding \(NEXT_M(\alpha)\)<p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"><span class="ps-keyword">Procedure</span> \(\mathsf{ComputeNext}(\(L_{prev)\),L,L_{next}\)} 
                <div class="ps-block" style="margin-left:1.2em;">
                <p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"><span class="ps-keyword">if</span> \(ISEMPTY \; L_{prev}\) 
                <div class="ps-block" style="margin-left:1.2em;">
                <p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"><span class="ps-keyword">return</span> \(NIL\)
</div><p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"><span class="ps-keyword">endif</span><p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"> \(a \leftarrow HEAD \; L_{prev}\)<p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"><span class="ps-keyword">if</span> \(ISEMPTY\; L\) 
                <div class="ps-block" style="margin-left:1.2em;">
                <p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"> \(b \leftarrow \varnothing\) <span class="ps-comment"><i>#  Encoding of \(\varnothing\) in \(\{0,1\</i></span><p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"><span class="ps-keyword">else</span><p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"> \(b \leftarrow HEAD\;L\)
</div><p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"><span class="ps-keyword">endif</span><p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"><span class="ps-keyword">if</span> \(ISEMPTY\; L_{next}\) 
                <div class="ps-block" style="margin-left:1.2em;">
                <p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"> \(c \leftarrow \varnothing\)<p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"><span class="ps-keyword">else</span><p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"> \(c \leftarrow HEAD \; L_{next}\)
</div><p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"><span class="ps-keyword">endif</span><p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"><span class="ps-keyword">return</span> \(PAIR \; r(a,b,c) \;\) \mathsf{ComputeNext}(\(TAIL\; L_{prev)\;,\;TAIL\; L\;,\;TAIL\; L_{next}\)}
</div><p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"><span class="ps-keyword">endproc</span><p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"> \(L_{prev} \leftarrow PAIR \; \varnothing \; L\) <span class="ps-comment"><i>#  \(L_{prev</i></span><p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"> \(L_{next} \leftarrow  TAIL\; L\) <span class="ps-comment"><i>#  \(L_{next</i></span><p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"><span class="ps-keyword">return</span> \mathsf{ComputeNext}(\(L_{prev),L,L_{next}\)}<p class="ps-line" style="text-indent:-1.2em;padding-left:1.2em;"></div>
</div>
</div>
</div>
<p>Once we can compute <span><span class="math inline">\(\ensuremath{\mathit{NEXT}}_M\)</span></span>, we can simulate the execution of <span><span class="math inline">\(M\)</span></span> on input <span><span class="math inline">\(x\)</span></span> using the following recursion. Define <span><span class="math inline">\(\ensuremath{\mathit{FINAL}}(\alpha)\)</span></span> to be the final configuration of <span><span class="math inline">\(M\)</span></span> when initialized at configuration <span><span class="math inline">\(\alpha\)</span></span>. The function <span><span class="math inline">\(\ensuremath{\mathit{FINAL}}\)</span></span> can be defined recursively as follows:</p>
<p><span>
<div class='myequationbox'><span class="math display">\[
\ensuremath{\mathit{FINAL}}(\alpha) = \begin{cases}\alpha &amp; \text{$\alpha$ is halting configuration} \\ \ensuremath{\mathit{NEXT}}_M(\alpha) &amp; \text{otherwise}\end{cases}\;.
\]</span></div></span></p>
<p>Checking whether a configuration is halting (i.e., whether it is one in which the transition function would output <span><span class="math inline">\(\mathsf{H}\)</span></span>alt) can be easily implemented in the <span><span class="math inline">\(\lambda\)</span></span> calculus, and hence we can use the <span><span class="math inline">\(\ensuremath{\mathit{RECURSE}}\)</span></span> to compute <span><span class="math inline">\(\ensuremath{\mathit{FINAL}}\)</span></span>. If we let <span><span class="math inline">\(\alpha^0\)</span></span> be the <em>initial configuration</em> of <span><span class="math inline">\(M\)</span></span> on input <span><span class="math inline">\(x\)</span></span> then we can obtain the output <span><span class="math inline">\(M(x)\)</span></span> from <span><span class="math inline">\(\ensuremath{\mathit{FINAL}}(\alpha^0)\)</span></span>, hence completing the proof.</p>
</div>
<h2 id="lambdacacluluspuresec" data-number="7.7">From enhanced to pure λ calculus</h2>
<p>While the collection of “basic” functions we allowed for the enhanced λ calculus is smaller than what’s provided by most Lisp dialects, coming from NAND-TM it still seems a little “bloated”. Can we make do with less? In other words, can we find a subset of these basic operations that can implement the rest?</p>
<p>It turns out that there is in fact a proper subset of the operations of the enhanced λ calculus that can be used to implement the rest. That subset is the empty set. That is, we can implement <em>all</em> the operations above using the λ formalism only, even without using <span><span class="math inline">\(0\)</span></span>’s and <span><span class="math inline">\(1\)</span></span>’s. It’s λ’s all the way down!</p>
<div class="pause" name="Pause 7.7">
<p>This is a good point to pause and think how you would implement these operations yourself. For example, start by thinking how you could implement <span><span class="math inline">\(\ensuremath{\mathit{MAP}}\)</span></span> using <span><span class="math inline">\(\ensuremath{\mathit{REDUCE}}\)</span></span>, and then <span><span class="math inline">\(\ensuremath{\mathit{REDUCE}}\)</span></span> using <span><span class="math inline">\(\ensuremath{\mathit{RECURSE}}\)</span></span> combined with <span><span class="math inline">\(0,1,\ensuremath{\mathit{IF}},\ensuremath{\mathit{PAIR}},\ensuremath{\mathit{HEAD}},\ensuremath{\mathit{TAIL}},\ensuremath{\mathit{NIL}},\ensuremath{\mathit{ISEMPTY}}\)</span></span>. You can also <span><span class="math inline">\(\ensuremath{\mathit{PAIR}}\)</span></span>, <span><span class="math inline">\(\ensuremath{\mathit{HEAD}}\)</span></span> and <span><span class="math inline">\(\ensuremath{\mathit{TAIL}}\)</span></span> based on <span><span class="math inline">\(0,1,\ensuremath{\mathit{IF}}\)</span></span>. The most challenging part is to implement <span><span class="math inline">\(\ensuremath{\mathit{RECURSE}}\)</span></span> using only the operations of the pure λ calculus.</p>
</div>
<div id="enhancedvanillalambdathm" class="theorem" title="Enhanced λ calculus equivalent to pure λ calculus." name="Theorem 7.18 (Enhanced λ calculus equivalent to pure λ calculus.) ">
<p>There are λ expressions that implement the functions <span><span class="math inline">\(0\)</span></span>,<span><span class="math inline">\(1\)</span></span>,<span><span class="math inline">\(\ensuremath{\mathit{IF}}\)</span></span>,<span><span class="math inline">\(\ensuremath{\mathit{PAIR}}\)</span></span>, <span><span class="math inline">\(\ensuremath{\mathit{HEAD}}\)</span></span>, <span><span class="math inline">\(\ensuremath{\mathit{TAIL}}\)</span></span>, <span><span class="math inline">\(\ensuremath{\mathit{NIL}}\)</span></span>, <span><span class="math inline">\(\ensuremath{\mathit{ISEMPTY}}\)</span></span>, <span><span class="math inline">\(\ensuremath{\mathit{MAP}}\)</span></span>, <span><span class="math inline">\(\ensuremath{\mathit{REDUCE}}\)</span></span>, and <span><span class="math inline">\(\ensuremath{\mathit{RECURSE}}\)</span></span>.</p>
</div>
<p>The idea behind <a href='#enhancedvanillalambdathm'>Theorem 7.18</a> is that we encode <span><span class="math inline">\(0\)</span></span> and <span><span class="math inline">\(1\)</span></span> themselves as λ expressions, and build things up from there. This is known as <a href="https://goo.gl/QZKM9M">Church encoding</a>, as it was originated by Church in his effort to show that the λ calculus can be a basis for all computation. We will not write the full formal proof of <a href='#enhancedvanillalambdathm'>Theorem 7.18</a> but outline the ideas involved in it:</p>
<ul>
<li><p>We define <span><span class="math inline">\(0\)</span></span> to be the function that on two inputs <span><span class="math inline">\(x,y\)</span></span> outputs <span><span class="math inline">\(y\)</span></span>, and <span><span class="math inline">\(1\)</span></span> to be the function that on two inputs <span><span class="math inline">\(x,y\)</span></span> outputs <span><span class="math inline">\(x\)</span></span>. We use Currying to achieve the effect of two-input functions and hence <span><span class="math inline">\(0 = \lambda x. \lambda y.y\)</span></span> and <span><span class="math inline">\(1 = \lambda x.\lambda y.x\)</span></span>. (This representation scheme is the common convention for representing <code>false</code> and <code>true</code> but there are many other alternative representations for <span><span class="math inline">\(0\)</span></span> and <span><span class="math inline">\(1\)</span></span> that would have worked just as well.)</p></li>
<li><p>The above implementation makes the <span><span class="math inline">\(\ensuremath{\mathit{IF}}\)</span></span> function trivial: <span><span class="math inline">\(\ensuremath{\mathit{IF}}(cond,a,b)\)</span></span> is simply <span><span class="math inline">\(cond \; a\; b\)</span></span> since <span><span class="math inline">\(0ab = b\)</span></span> and <span><span class="math inline">\(1ab = a\)</span></span>. We can write <span><span class="math inline">\(\ensuremath{\mathit{IF}} = \lambda x.x\)</span></span> to achieve <span><span class="math inline">\(\ensuremath{\mathit{IF}}(cond,a,b) = (((\ensuremath{\mathit{IF}} cond) a) b) = cond \; a \; b\)</span></span>.</p></li>
<li><p>To encode a pair <span><span class="math inline">\((x,y)\)</span></span> we will produce a function <span><span class="math inline">\(f_{x,y}\)</span></span> that has <span><span class="math inline">\(x\)</span></span> and <span><span class="math inline">\(y\)</span></span> “in its belly” and satisfies <span><span class="math inline">\(f_{x,y}g = g x y\)</span></span> for every function <span><span class="math inline">\(g\)</span></span>. That is, <span><span class="math inline">\(\ensuremath{\mathit{PAIR}} = \lambda x,y. \left(\lambda g. gxy\right)\)</span></span>. We can extract the first element of a pair <span><span class="math inline">\(p\)</span></span> by writing <span><span class="math inline">\(p1\)</span></span> and the second element by writing <span><span class="math inline">\(p0\)</span></span>, and so <span><span class="math inline">\(\ensuremath{\mathit{HEAD}} = \lambda p. p1\)</span></span> and <span><span class="math inline">\(\ensuremath{\mathit{TAIL}} = \lambda p. p0\)</span></span>.</p></li>
<li><p>We define <span><span class="math inline">\(\ensuremath{\mathit{NIL}}\)</span></span> to be the function that ignores its input and always outputs <span><span class="math inline">\(1\)</span></span>. That is, <span><span class="math inline">\(\ensuremath{\mathit{NIL}} = \lambda x.1\)</span></span>. The <span><span class="math inline">\(\ensuremath{\mathit{ISEMPTY}}\)</span></span> function checks, given an input <span><span class="math inline">\(p\)</span></span>, whether we get <span><span class="math inline">\(1\)</span></span> if we apply <span><span class="math inline">\(p\)</span></span> to the function <span><span class="math inline">\(zero = \lambda x,y.0\)</span></span> that ignores both its inputs and always outputs <span><span class="math inline">\(0\)</span></span>. For every valid pair of the form <span><span class="math inline">\(p = \ensuremath{\mathit{PAIR}} x y\)</span></span>, <span><span class="math inline">\(p zero = p x y = 0\)</span></span> while <span><span class="math inline">\(\ensuremath{\mathit{NIL}} zero=1\)</span></span>. Formally, <span><span class="math inline">\(\ensuremath{\mathit{ISEMPTY}} = \lambda p. p (\lambda x,y.0)\)</span></span>.</p></li>
</ul>
<div id="Churchnumrem" class="remark" title="Church numerals (optional)" name="Remark 7.19 (Church numerals (optional)) ">
<p>There is nothing special about Boolean values. You can use similar tricks to implement <em>natural numbers</em> using λ terms. The standard way to do so is to represent the number <span><span class="math inline">\(n\)</span></span> by the function <span><span class="math inline">\(\ensuremath{\mathit{ITER}}_n\)</span></span> that on input a function <span><span class="math inline">\(f\)</span></span> outputs the function <span><span class="math inline">\(x \mapsto f(f(\cdots f(x)))\)</span></span> (<span><span class="math inline">\(n\)</span></span> times). That is, we represent the natural number <span><span class="math inline">\(1\)</span></span> as <span><span class="math inline">\(\lambda f.f\)</span></span>, the number <span><span class="math inline">\(2\)</span></span> as <span><span class="math inline">\(\lambda f.(\lambda x.f(fx))\)</span></span>, the number <span><span class="math inline">\(3\)</span></span> as <span><span class="math inline">\(\lambda f.(\lambda x.f(f(fx)))\)</span></span>, and so on and so forth. (Note that this is not the same representation we used for <span><span class="math inline">\(1\)</span></span> in the Boolean context: this is fine; we already know that the same object can be represented in more than one way.) The number <span><span class="math inline">\(0\)</span></span> is represented by the function that maps any function <span><span class="math inline">\(f\)</span></span> to the identity function <span><span class="math inline">\(\lambda x.x\)</span></span>. (That is, <span><span class="math inline">\(0 = \lambda f.(\lambda x.x)\)</span></span>.)</p>
<p>In this representation, we can compute <span><span class="math inline">\(\ensuremath{\mathit{PLUS}}(n,m)\)</span></span> as <span><span class="math inline">\(\lambda f.\lambda x.(n f)((m f)x)\)</span></span> and <span><span class="math inline">\(\ensuremath{\mathit{TIMES}}(n,m)\)</span></span> as <span><span class="math inline">\(\lambda f.n(m f)\)</span></span>. Subtraction and division are trickier, but can be achieved using recursion. (Working this out is a great exercise.)</p>
</div>
<h3 id="list-processing" data-number="7.7.1">List processing</h3>
<p>Now we come to a bigger hurdle, which is how to implement <span><span class="math inline">\(\ensuremath{\mathit{MAP}}\)</span></span>, <span><span class="math inline">\(\ensuremath{\mathit{FILTER}}\)</span></span>, <span><span class="math inline">\(\ensuremath{\mathit{REDUCE}}\)</span></span> and <span><span class="math inline">\(\ensuremath{\mathit{RECURSE}}\)</span></span> in the pure λ calculus. It turns out that we can build <span><span class="math inline">\(\ensuremath{\mathit{MAP}}\)</span></span> and <span><span class="math inline">\(\ensuremath{\mathit{FILTER}}\)</span></span> from <span><span class="math inline">\(\ensuremath{\mathit{REDUCE}}\)</span></span>, and <span><span class="math inline">\(\ensuremath{\mathit{REDUCE}}\)</span></span> from <span><span class="math inline">\(\ensuremath{\mathit{RECURSE}}\)</span></span>. For example <span><span class="math inline">\(\ensuremath{\mathit{MAP}}(L,f)\)</span></span> is the same as <span><span class="math inline">\(\ensuremath{\mathit{REDUCE}}(L,g)\)</span></span> where <span><span class="math inline">\(g\)</span></span> is the operation that on input <span><span class="math inline">\(x\)</span></span> and <span><span class="math inline">\(y\)</span></span>, outputs <span><span class="math inline">\(\ensuremath{\mathit{PAIR}}(f(x),\ensuremath{\mathit{NIL}})\)</span></span> if <span><span class="math inline">\(y\)</span></span> is NIL and otherwise outputs <span><span class="math inline">\(\ensuremath{\mathit{PAIR}}(f(x),y)\)</span></span>. (I leave checking this as a (recommended!) exercise for you, the reader.)</p>
<p>We can define <span><span class="math inline">\(\ensuremath{\mathit{REDUCE}}(L,g)\)</span></span> recursively, by setting <span><span class="math inline">\(\ensuremath{\mathit{REDUCE}}(\ensuremath{\mathit{NIL}},g)=\ensuremath{\mathit{NIL}}\)</span></span> and stipulating that given a non-empty list <span><span class="math inline">\(L\)</span></span>, which we can think of as a pair <span><span class="math inline">\((head,rest)\)</span></span>, <span><span class="math inline">\(\ensuremath{\mathit{REDUCE}}(L,g) = g(head, \ensuremath{\mathit{REDUCE}}(rest,g)))\)</span></span>. Thus, we might try to write a recursive λ expression for <span><span class="math inline">\(\ensuremath{\mathit{REDUCE}}\)</span></span> as follows</p>
<p><span>
<div class='myequationbox'><span class="math display">\[
\ensuremath{\mathit{REDUCE}} = \lambda L,g. \ensuremath{\mathit{IF}}(\ensuremath{\mathit{ISEMPTY}}(L),\ensuremath{\mathit{NIL}},g \ensuremath{\mathit{HEAD}}(L) \ensuremath{\mathit{REDUCE}}(\ensuremath{\mathit{TAIL}}(L),g)) \;\;(7.22) \;.
\]</span><a id='reducereceq'></a></div></span></p>
<p>The only fly in this ointment is that the λ calculus does not have the notion of recursion, and so this is an invalid definition. But of course we can use our <span><span class="math inline">\(\ensuremath{\mathit{RECURSE}}\)</span></span> operator to solve this problem. We will replace the recursive call to “<span><span class="math inline">\(\ensuremath{\mathit{REDUCE}}\)</span></span>” with a call to a function <span><span class="math inline">\(me\)</span></span> that is given as an extra argument, and then apply <span><span class="math inline">\(\ensuremath{\mathit{RECURSE}}\)</span></span> to this. Thus <span><span class="math inline">\(\ensuremath{\mathit{REDUCE}} = \ensuremath{\mathit{RECURSE}}\;myREDUCE\)</span></span> where</p>
<p><span>
<div class='myequationbox'><span class="math display">\[
myREDUCE = \lambda me,L,g. \ensuremath{\mathit{IF}}(\ensuremath{\mathit{ISEMPTY}}(L),\ensuremath{\mathit{NIL}},g \ensuremath{\mathit{HEAD}}(L) me(\ensuremath{\mathit{TAIL}}(L),g)) \;\;(7.23) \;.
\]</span><a id='myreducereceq'></a></div></span></p>
<h3 id="ycombinatorsec" data-number="7.7.2">The Y combinator, or recursion without recursion</h3>
<p><a href='#myreducereceq'>Equation 7.23</a> means that implementing <span><span class="math inline">\(\ensuremath{\mathit{MAP}}\)</span></span>, <span><span class="math inline">\(\ensuremath{\mathit{FILTER}}\)</span></span>, and <span><span class="math inline">\(\ensuremath{\mathit{REDUCE}}\)</span></span> boils down to implementing the <span><span class="math inline">\(\ensuremath{\mathit{RECURSE}}\)</span></span> operator in the pure λ calculus. This is what we do now.</p>
<p>How can we implement recursion without recursion? We will illustrate this using a simple example - the <span><span class="math inline">\(\ensuremath{\mathit{XOR}}\)</span></span> function. As shown in <a href='#XORlambdaex'>Solvedexercise 7.4</a>, we can write the <span><span class="math inline">\(\ensuremath{\mathit{XOR}}\)</span></span> function of a list recursively as follows:</p>
<p><span>
<div class='myequationbox'><span class="math display">\[
\ensuremath{\mathit{XOR}}(L) = \begin{cases} 0 &amp; L \text{ is empty} \\ \ensuremath{\mathit{XOR}}_2(\ensuremath{\mathit{HEAD}}(L),\ensuremath{\mathit{XOR}}(\ensuremath{\mathit{TAIL}}(L))) &amp; \text{otherwise}
\end{cases}
\]</span></div></span></p>
<p>where <span><span class="math inline">\(\ensuremath{\mathit{XOR}}_2:\{0,1\}^2 \rightarrow \{0,1\}\)</span></span> is the XOR on two bits. In <em>Python</em> we would write this as</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb5-1" title="1"><span class="kw">def</span> xor2(a,b): <span class="cf">return</span> <span class="dv">1</span><span class="op">-</span>b <span class="cf">if</span> a <span class="cf">else</span> b</a>
<a class="sourceLine" id="cb5-2" title="2"><span class="kw">def</span> head(L): <span class="cf">return</span> L[<span class="dv">0</span>]</a>
<a class="sourceLine" id="cb5-3" title="3"><span class="kw">def</span> tail(L): <span class="cf">return</span> L[<span class="dv">1</span>:]</a>
<a class="sourceLine" id="cb5-4" title="4"></a>
<a class="sourceLine" id="cb5-5" title="5"><span class="kw">def</span> xor(L): <span class="cf">return</span> xor2(head(L),xor(tail(L))) <span class="cf">if</span> L <span class="cf">else</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb5-6" title="6"></a>
<a class="sourceLine" id="cb5-7" title="7"><span class="bu">print</span>(xor([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>]))</a>
<a class="sourceLine" id="cb5-8" title="8"><span class="co"># 1</span></a></code></pre></div>
<p>Now, how could we eliminate this recursive call? The main idea is that since functions can take other functions as input, it is perfectly legal in Python (and the λ calculus of course) to give a function <em>itself</em> as input. So, our idea is to try to come up with a <em>non recursive</em> function <code>tempxor</code> that takes <em>two inputs</em>: a function and a list, and such that <code>tempxor(tempxor,L)</code> will output the XOR of <code>L</code>!</p>
<div class="pause" name="Pause 19.4.1">
<p>At this point you might want to stop and try to implement this on your own in Python or any other programming language of your choice (as long as it allows functions as inputs).</p>
</div>
<p>Our first attempt might be to simply use the idea of replacing the recursive call by <code>me</code>. Let’s define this function as <code>myxor</code></p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb6-1" title="1"><span class="kw">def</span> myxor(me,L): <span class="cf">return</span> xor2(head(L),me(tail(L))) <span class="cf">if</span> L <span class="cf">else</span> <span class="dv">0</span></a></code></pre></div>
<p>Let’s test this out:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb7-1" title="1">myxor(myxor,[<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>])</a></code></pre></div>
<p>If you do this, you will get the following complaint from the interpreter:</p>
<p><code>TypeError: myxor() missing 1 required positional argument</code></p>
<p>The problem is that <code>myxor</code> expects <em>two</em> inputs- a function and a list- while in the call to <code>me</code> we only provided a list. To correct this, we modify the call to also provide the function itself:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb8-1" title="1"><span class="kw">def</span> tempxor(me,L): <span class="cf">return</span> xor2(head(L),me(me,tail(L))) <span class="cf">if</span> L <span class="cf">else</span> <span class="dv">0</span></a></code></pre></div>
<p>Note the call <code>me(me,..)</code> in the definition of <code>tempxor</code>: given a function <code>me</code> as input, <code>tempxor</code> will actually call the function <code>me</code> with itself as the first input. If we test this out now, we see that we actually get the right result!</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb9-1" title="1">tempxor(tempxor,[<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>])</a>
<a class="sourceLine" id="cb9-2" title="2"><span class="co"># 0</span></a>
<a class="sourceLine" id="cb9-3" title="3">tempxor(tempxor,[<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>])</a>
<a class="sourceLine" id="cb9-4" title="4"><span class="co"># 1</span></a></code></pre></div>
<p>and so we can define <code>xor(L)</code> as simply <code>return tempxor(tempxor,L)</code>.</p>
<p>The approach above is not specific to XOR. Given a recursive function <code>f</code> that takes an input <code>x</code>, we can obtain a non recursive version as follows:</p>
<ol type="1">
<li><p>Create the function <code>myf</code> that takes a pair of inputs <code>me</code> and <code>x</code>, and replaces recursive calls to <code>f</code> with calls to <code>me</code>.</p></li>
<li><p>Create the function <code>tempf</code> that converts calls in <code>myf</code> of the form <code>me(x)</code> to calls of the form <code>me(me,x)</code>.</p></li>
<li><p>The function <code>f(x)</code> will be defined as <code>tempf(tempf,x)</code></p></li>
</ol>
<p>Here is the way we implement the <code>RECURSE</code> operator in Python. It will take a function <code>myf</code> as above, and replace it with a function <code>g</code> such that <code>g(x)=myf(g,x)</code> for every <code>x</code>.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb10-1" title="1"><span class="kw">def</span> RECURSE(myf):</a>
<a class="sourceLine" id="cb10-2" title="2">    <span class="kw">def</span> tempf(me,x): <span class="cf">return</span> myf(<span class="kw">lambda</span> y: me(me,y),x)</a>
<a class="sourceLine" id="cb10-3" title="3"></a>
<a class="sourceLine" id="cb10-4" title="4">    <span class="cf">return</span> <span class="kw">lambda</span> x: tempf(tempf,x)</a>
<a class="sourceLine" id="cb10-5" title="5"></a>
<a class="sourceLine" id="cb10-6" title="6"></a>
<a class="sourceLine" id="cb10-7" title="7">xor <span class="op">=</span> RECURSE(myxor)</a>
<a class="sourceLine" id="cb10-8" title="8"></a>
<a class="sourceLine" id="cb10-9" title="9"><span class="bu">print</span>(xor([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>]))</a>
<a class="sourceLine" id="cb10-10" title="10"><span class="co"># 1</span></a>
<a class="sourceLine" id="cb10-11" title="11"></a>
<a class="sourceLine" id="cb10-12" title="12"><span class="bu">print</span>(xor([<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>]))</a>
<a class="sourceLine" id="cb10-13" title="13"><span class="co"># 0</span></a></code></pre></div>
<p><strong>From Python to the <span><span class="math inline">\(\lambda\)</span></span> calculus.</strong> In the λ calculus, a two input function <span><span class="math inline">\(g\)</span></span> that takes a pair of inputs <span><span class="math inline">\(me,y\)</span></span> is written as <span><span class="math inline">\(\lambda me.(\lambda y. g)\)</span></span>. So the function <span><span class="math inline">\(y \mapsto me(me,y)\)</span></span> is simply written as <span><span class="math inline">\(me\;me\)</span></span> and similarly the function <span><span class="math inline">\(x \mapsto tempf(tempf,x)\)</span></span> is simply <span><span class="math inline">\(tempf\; tempf\)</span></span>. (Can you see why?) Therefore the function <code>tempf</code> defined above can be written as <code>λ me. myf(me me)</code>. This means that if we denote the input of <code>RECURSE</code> by <span><span class="math inline">\(f\)</span></span>, then <span><span class="math inline">\(\ensuremath{\mathit{RECURSE}}\; myf = tempf \; tempf\)</span></span> where <span><span class="math inline">\(tempf = \lambda m. f(m\; m)\)</span></span> or in other words <span>
<div class='myequationbox'><span class="math display">\[
\ensuremath{\mathit{RECURSE}} =  \lambda f.\bigl( (\lambda m. f(m\; m))\;\; (\lambda m. f(m \;m)) \bigr)
\]</span></div></span></p>
<p>The <a href="https://github.com/boazbk/nandnotebooks/blob/master/lambda.ipynb">online appendix</a> contains an implementation of the λ calculus using Python. Here is an implementation of the recursive XOR function from that appendix:<sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup></p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb11-1" title="1"><span class="co"># XOR of two bits</span></a>
<a class="sourceLine" id="cb11-2" title="2">XOR2 <span class="op">=</span> λ(a,b)(IF(a,IF(b,_<span class="dv">0</span>,_<span class="dv">1</span>),b))</a>
<a class="sourceLine" id="cb11-3" title="3"></a>
<a class="sourceLine" id="cb11-4" title="4"><span class="co"># Recursive XOR with recursive calls replaced by m parameter</span></a>
<a class="sourceLine" id="cb11-5" title="5">myXOR <span class="op">=</span> λ(m,l)(IF(ISEMPTY(l),_<span class="dv">0</span>,XOR2(HEAD(l),m(TAIL(l)))))</a>
<a class="sourceLine" id="cb11-6" title="6"></a>
<a class="sourceLine" id="cb11-7" title="7"><span class="co"># Recurse operator (aka Y combinator)</span></a>
<a class="sourceLine" id="cb11-8" title="8">RECURSE <span class="op">=</span> λf((λm(f(m<span class="op">*</span>m)))(λm(f(m<span class="op">*</span>m))))</a>
<a class="sourceLine" id="cb11-9" title="9"></a>
<a class="sourceLine" id="cb11-10" title="10"><span class="co"># XOR function</span></a>
<a class="sourceLine" id="cb11-11" title="11">XOR <span class="op">=</span> RECURSE(myXOR)</a>
<a class="sourceLine" id="cb11-12" title="12"></a>
<a class="sourceLine" id="cb11-13" title="13"><span class="co">#TESTING:</span></a>
<a class="sourceLine" id="cb11-14" title="14"></a>
<a class="sourceLine" id="cb11-15" title="15">XOR(PAIR(_<span class="dv">1</span>,NIL)) <span class="co"># List [1]</span></a>
<a class="sourceLine" id="cb11-16" title="16"><span class="co"># equals 1</span></a>
<a class="sourceLine" id="cb11-17" title="17"></a>
<a class="sourceLine" id="cb11-18" title="18">XOR(PAIR(_<span class="dv">1</span>,PAIR(_<span class="dv">0</span>,PAIR(_<span class="dv">1</span>,NIL)))) <span class="co"># List [1,0,1]</span></a>
<a class="sourceLine" id="cb11-19" title="19"><span class="co"># equals 0</span></a></code></pre></div>
<div id="Ycombinator" class="remark" title="The Y combinator" name="Remark 7.20 (The Y combinator) ">
<p>The <span><span class="math inline">\(\ensuremath{\mathit{RECURSE}}\)</span></span> operator above is better known as the <a href="https://en.wikipedia.org/wiki/Fixed-point_combinator#Fixed_point_combinators_in_lambda_calculus">Y combinator</a>.</p>
<p>It is one of a family of a <em>fixed point operators</em> that given a lambda expression <span><span class="math inline">\(F\)</span></span>, find a <em>fixed point</em> <span><span class="math inline">\(f\)</span></span> of <span><span class="math inline">\(F\)</span></span> such that <span><span class="math inline">\(f = F f\)</span></span>. If you think about it, <span><span class="math inline">\(\ensuremath{\mathit{XOR}}\)</span></span> is the fixed point of <span><span class="math inline">\(myXOR\)</span></span> above. <span><span class="math inline">\(\ensuremath{\mathit{XOR}}\)</span></span> is the function such that for every <span><span class="math inline">\(x\)</span></span>, if plug in <span><span class="math inline">\(\ensuremath{\mathit{XOR}}\)</span></span> as the first argument of <span><span class="math inline">\(myXOR\)</span></span> then we get back <span><span class="math inline">\(\ensuremath{\mathit{XOR}}\)</span></span>, or in other words <span><span class="math inline">\(\ensuremath{\mathit{XOR}} = myXOR\; \ensuremath{\mathit{XOR}}\)</span></span>. Hence finding a <em>fixed point</em> for <span><span class="math inline">\(myXOR\)</span></span> is the same as applying <span><span class="math inline">\(\ensuremath{\mathit{RECURSE}}\)</span></span> to it.</p>
</div>
<h2 id="churchturingdiscussionsec" data-number="7.8">The Church-Turing Thesis (discussion)</h2>
<blockquote>
<p><em>“[In 1934], Church had been speculating, and finally definitely proposed, that the λ-definable functions are all the effectively calculable functions …. When Church proposed this thesis, I sat down to disprove it … but, quickly realizing that [my approach failed], I became overnight a supporter of the thesis.”</em>, Stephen Kleene, 1979.</p>
</blockquote>
<blockquote>
<p><em>“[The thesis is] not so much a definition or to an axiom but … a natural law.”</em>, Emil Post, 1936.</p>
</blockquote>
<p>We have defined functions to be <em>computable</em> if they can be computed by a NAND-TM program, and we’ve seen that the definition would remain the same if we replaced NAND-TM programs by Python programs, Turing machines, λ calculus, cellular automata, and many other computational models. The <em>Church-Turing thesis</em> is that this is the only sensible definition of “computable” functions. Unlike the “Physical Extended Church Turing Thesis” (PECTT) which we saw before, the Church Turing thesis does not make a concrete physical prediction that can be experimentally tested, but it certainly motivates predictions such as the PECTT. One can think of the Church-Turing Thesis as either advocating a definitional choice, making some prediction about all potential computing devices, or suggesting some laws of nature that constrain the natural world. In Scott Aaronson’s words, “whatever it is, the Church-Turing thesis can only be regarded as extremely successful”. No candidate computing device (including quantum computers, and also much less reasonable models such as the hypothetical “closed time curve” computers we mentioned before) has so far mounted a serious challenge to the Church Turing thesis. These devices might potentially make some computations more <em>efficient</em>, but they do not change the difference between what is finitely computable and what is not. (The <em>extended</em> Church Turing thesis, which we discuss in <a href='lec_11_running_time.html#ECTTsec'>Section 12.3</a>, stipulates that Turing machines capture also the limit of what can be <em>efficiently</em> computable. Just like its physical version, quantum computing presents the main challenge to this thesis.)</p>
<h3 id="different-models-of-computation" data-number="7.8.1">Different models of computation</h3>
<p>We can summarize the models we have seen in the following table:</p>
<table>
<caption>Different models for computing finite functions and functions with arbitrary input length.</caption>
<colgroup>
<col style="width: 33%" />
<col style="width: 29%" />
<col style="width: 36%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>Computational problems</strong></th>
<th><strong>Type of model</strong></th>
<th><strong>Examples</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Finite functions <span><span class="math inline">\(f:\{0,1\}^n \rightarrow \{0,1\}^m\)</span></span></td>
<td>Non uniform computation (algorithm depends on input length)</td>
<td>Boolean circuits, NAND circuits, straight-line programs (e.g., NAND-CIRC)</td>
</tr>
<tr class="even">
<td>Functions with unbounded inputs <span><span class="math inline">\(F:\{0,1\}^* \rightarrow \{0,1\}^*\)</span></span></td>
<td>Sequential access to memory</td>
<td>Turing machines, NAND-TM programs</td>
</tr>
<tr class="odd">
<td>–</td>
<td>Indexed access / RAM</td>
<td>RAM machines, NAND-RAM, modern programming languages</td>
</tr>
<tr class="even">
<td>–</td>
<td>Other</td>
<td>Lambda calculus, cellular automata</td>
</tr>
</tbody>
</table>
<p>Later on in <a href='lec_14a_space_complexity.html#spacechap'>Chapter 16</a> we will study <em>memory bounded</em> computation. It turns out that NAND-TM programs with a constant amount of memory are equivalent to the model of <em>finite automata</em> (the adjectives “deterministic” or “nondeterministic” are sometimes added as well, this model is also known as <em>finite state machines</em>) which in turn captures the notion of <em>regular languages</em> (those that can be described by <a href="https://en.wikipedia.org/wiki/Regular_expression">regular expressions</a>), which is a concept we will see in <a href='lec_08a_restricted_models.html#restrictedchap'>Chapter 9</a>.</p>
<div id="section-1" class="recap" name="Recap">
<ul>
<li>While we defined computable functions using Turing machines, we could just as well have done so using many other models, including not just NAND-TM programs but also RAM machines, NAND-RAM, the λ-calculus, cellular automata and many other models.</li>
<li>Very simple models turn out to be “Turing complete” in the sense that they can simulate arbitrarily complex computation.</li>
</ul>
</div>
<h2 id="exercises" data-number="7.9">Exercises</h2>
<div id="RAMTMalternativeex" class="exercise" title="Alternative proof for TM/RAM equivalence" name="Exercise 7.1 (Alternative proof for TM/RAM equivalence) ">
<p>Let <span><span class="math inline">\(\ensuremath{\mathit{SEARCH}}:\{0,1\}^* \rightarrow \{0,1\}^*\)</span></span> be the following function. The input is a pair <span><span class="math inline">\((L,k)\)</span></span> where <span><span class="math inline">\(k\in \{0,1\}^*\)</span></span>, <span><span class="math inline">\(L\)</span></span> is an encoding of a list of <em>key value</em> pairs <span><span class="math inline">\((k_0,v_1),\ldots,(k_{m-1},v_{m-1})\)</span></span> where <span><span class="math inline">\(k_0,\ldots,k_{m-1}\)</span></span>, <span><span class="math inline">\(v_0,\ldots,v_{m-1}\)</span></span> are binary strings. The output is <span><span class="math inline">\(v_i\)</span></span> for the smallest <span><span class="math inline">\(i\)</span></span> such that <span><span class="math inline">\(k_i=k\)</span></span>, if such <span><span class="math inline">\(i\)</span></span> exists, and otherwise the empty string.</p>
<ol type="1">
<li><p>Prove that <span><span class="math inline">\(\ensuremath{\mathit{SEARCH}}\)</span></span> is computable by a Turing machine.</p></li>
<li><p>Let <span><span class="math inline">\(\ensuremath{\mathit{UPDATE}}(L,k,v)\)</span></span> be the function whose input is a list <span><span class="math inline">\(L\)</span></span> of pairs, and whose output is the list <span><span class="math inline">\(L&#39;\)</span></span> obtained by prepending the pair <span><span class="math inline">\((k,v)\)</span></span> to the beginning of <span><span class="math inline">\(L\)</span></span>. Prove that <span><span class="math inline">\(\ensuremath{\mathit{UPDATE}}\)</span></span> is computable by a Turing machine.</p></li>
<li><p>Suppose we encode the configuration of a NAND-RAM program by a list <span><span class="math inline">\(L\)</span></span> of key/value pairs where the key is either the name of a scalar variable <code>foo</code> or of the form <code>Bar[&lt;num&gt;]</code> for some number <code>&lt;num&gt;</code> and it contains all the nonzero values of variables. Let <span><span class="math inline">\(\ensuremath{\mathit{NEXT}}(L)\)</span></span> be the function that maps a configuration of a NAND-RAM program at one step to the configuration in the next step. Prove that <span><span class="math inline">\(\ensuremath{\mathit{NEXT}}\)</span></span> is computable by a Turing machine (you don’t have to implement each one of the arithmetic operations: it is enough to implement addition and multiplication).</p></li>
<li><p>Prove that for every <span><span class="math inline">\(F:\{0,1\}^* \rightarrow \{0,1\}^*\)</span></span> that is computable by a NAND-RAM program, <span><span class="math inline">\(F\)</span></span> is computable by a Turing machine.</p></li>
</ol>
</div>
<div id="lookup" class="exercise" title="NAND-TM lookup" name="Exercise 7.2 (NAND-TM lookup) ">
<p>This exercise shows part of the proof that NAND-TM can simulate NAND-RAM. Produce the code of a NAND-TM program that computes the function <span><span class="math inline">\(\ensuremath{\mathit{LOOKUP}}:\{0,1\}^* \rightarrow \{0,1\}\)</span></span> that is defined as follows. On input <span><span class="math inline">\(pf(i)x\)</span></span>, where <span><span class="math inline">\(pf(i)\)</span></span> denotes a prefix-free encoding of an integer <span><span class="math inline">\(i\)</span></span>, <span><span class="math inline">\(\ensuremath{\mathit{LOOKUP}}(pf(i)x)=x_i\)</span></span> if <span><span class="math inline">\(i&lt;|x|\)</span></span> and <span><span class="math inline">\(\ensuremath{\mathit{LOOKUP}}(pf(i)x)=0\)</span></span> otherwise. (We don’t care what <span><span class="math inline">\(\ensuremath{\mathit{LOOKUP}}\)</span></span> outputs on inputs that are not of this form.) You can choose any prefix-free encoding of your choice, and also can use your favorite programming language to produce this code.</p>
</div>
<div id="pair-ex" class="exercise" title="Pairing" name="Exercise 7.3 (Pairing) ">
<p>Let <span><span class="math inline">\(embed:\N^2 \rightarrow \N\)</span></span> be the function defined as <span><span class="math inline">\(embed(x_0,x_1)= \tfrac{1}{2}(x_0+x_1)(x_0+x_1+1) + x_1\)</span></span>.<br />
</p>
<ol type="1">
<li><p>Prove that for every <span><span class="math inline">\(x^0,x^1 \in \N\)</span></span>, <span><span class="math inline">\(embed(x^0,x^1)\)</span></span> is indeed a natural number.<br />
</p></li>
<li><p>Prove that <span><span class="math inline">\(embed\)</span></span> is one-to-one<br />
</p></li>
<li><p>Construct a NAND-TM program <span><span class="math inline">\(P\)</span></span> such that for every <span><span class="math inline">\(x^0,x^1 \in \N\)</span></span>, <span><span class="math inline">\(P(pf(x^0)pf(x^1))=pf(embed(x^0,x^1))\)</span></span>, where <span><span class="math inline">\(pf\)</span></span> is the prefix-free encoding map defined above. You can use the syntactic sugar for inner loops, conditionals, and incrementing/decrementing the counter.<br />
</p></li>
<li><p>Construct NAND-TM programs <span><span class="math inline">\(P_0,P_1\)</span></span> such that for for every <span><span class="math inline">\(x^0,x^1 \in \N\)</span></span> and <span><span class="math inline">\(i \in N\)</span></span>, <span><span class="math inline">\(P_i(pf(embed(x^0,x^1)))=pf(x^i)\)</span></span>. You can use the syntactic sugar for inner loops, conditionals, and incrementing/decrementing the counter.</p></li>
</ol>
</div>
<div id="shortestpathcomputableex" class="exercise" title="Shortest Path" name="Exercise 7.4 (Shortest Path) ">
<p>Let <span><span class="math inline">\(\ensuremath{\mathit{SHORTPATH}}:\{0,1\}^* \rightarrow \{0,1\}^*\)</span></span> be the function that on input a string encoding a triple <span><span class="math inline">\((G,u,v)\)</span></span> outputs a string encoding <span><span class="math inline">\(\infty\)</span></span> if <span><span class="math inline">\(u\)</span></span> and <span><span class="math inline">\(v\)</span></span> are disconnected in <span><span class="math inline">\(G\)</span></span> or a string encoding the length <span><span class="math inline">\(k\)</span></span> of the shortest path from <span><span class="math inline">\(u\)</span></span> to <span><span class="math inline">\(v\)</span></span>. Prove that <span><span class="math inline">\(\ensuremath{\mathit{SHORTPATH}}\)</span></span> is computable by a Turing machine. See footnote for hint.<sup id="fnref:4"><a href="#fn:4" rel="footnote">4</a></sup></p>
</div>
<div id="longestpathcomputableex" class="exercise" title="Longest Path" name="Exercise 7.5 (Longest Path) ">
<p>Let <span><span class="math inline">\(\ensuremath{\mathit{LONGPATH}}:\{0,1\}^* \rightarrow \{0,1\}^*\)</span></span> be the function that on input a string encoding a triple <span><span class="math inline">\((G,u,v)\)</span></span> outputs a string encoding <span><span class="math inline">\(\infty\)</span></span> if <span><span class="math inline">\(u\)</span></span> and <span><span class="math inline">\(v\)</span></span> are disconnected in <span><span class="math inline">\(G\)</span></span> or a string encoding the length <span><span class="math inline">\(k\)</span></span> of the <em>longest simple path</em> from <span><span class="math inline">\(u\)</span></span> to <span><span class="math inline">\(v\)</span></span>. Prove that <span><span class="math inline">\(\ensuremath{\mathit{LONGPATH}}\)</span></span> is computable by a Turing machine. See footnote for hint.<sup id="fnref:5"><a href="#fn:5" rel="footnote">5</a></sup></p>
</div>
<div id="shortestpathlambda" class="exercise" title="Shortest path λ expression" name="Exercise 7.6 (Shortest path λ expression) ">
<p>Let <span><span class="math inline">\(\ensuremath{\mathit{SHORTPATH}}\)</span></span> be as in <a href='#shortestpathcomputableex'>Exercise 7.4</a>. Prove that there exists a <span><span class="math inline">\(\lambda\)</span></span> expression that computes <span><span class="math inline">\(\ensuremath{\mathit{SHORTPATH}}\)</span></span>. You can use <a href='#shortestpathcomputableex'>Exercise 7.4</a></p>
</div>
<div id="nextstepfunctionlemex" class="exercise" title="Next-step function is local" name="Exercise 7.7 (Next-step function is local) ">
<p>Prove <a href='#nextstepfunctionlem'>Lemma 7.9</a> and use it to complete the proof of <a href='#onedimcathm'>Theorem 7.7</a>.</p>
</div>
<div id="lambda-calc-ex" class="exercise" title="λ calculus requires at most three variables" name="Exercise 7.8 (λ calculus requires at most three variables) ">
<p>Prove that for every λ-expression <span><span class="math inline">\(e\)</span></span> with no free variables there is an equivalent λ-expression <span><span class="math inline">\(f\)</span></span> that only uses the variables <span><span class="math inline">\(x\)</span></span>,<span><span class="math inline">\(y\)</span></span>, and <span><span class="math inline">\(z\)</span></span>.<sup id="fnref:6"><a href="#fn:6" rel="footnote">6</a></sup></p>
</div>
<div id="evalorderlambdaex" class="exercise" title="Evaluation order example in λ calculus" name="Exercise 7.9 (Evaluation order example in λ calculus) ">
<ol type="1">
<li><p>Let <span><span class="math inline">\(e = \lambda x.7 \left( (\lambda x.xx) (\lambda x.xx) \right)\)</span></span>. Prove that the simplification process of <span><span class="math inline">\(e\)</span></span> ends in a definite number if we use the “call by name” evaluation order while it never ends if we use the “call by value” order.</p></li>
<li><p>(bonus, challenging) Let <span><span class="math inline">\(e\)</span></span> be any λ expression. Prove that if the simplification process ends in a definite number if we use the “call by value” order then it also ends in such a number if we use the “call by name” order. See footnote for hint.<sup id="fnref:7"><a href="#fn:7" rel="footnote">7</a></sup></p></li>
</ol>
</div>
<div id="zipfunctionex" class="exercise" title="Zip function" name="Exercise 7.10 (Zip function) ">
<p>Give an enhanced λ calculus expression to compute the function <span><span class="math inline">\(zip\)</span></span> that on input a pair of lists <span><span class="math inline">\(I\)</span></span> and <span><span class="math inline">\(L\)</span></span> of the same length <span><span class="math inline">\(n\)</span></span>, outputs a <em>list of <span><span class="math inline">\(n\)</span></span> pairs</em> <span><span class="math inline">\(M\)</span></span> such that the <span><span class="math inline">\(j\)</span></span>-th element of <span><span class="math inline">\(M\)</span></span> (which we denote by <span><span class="math inline">\(M_j\)</span></span>) is the pair <span><span class="math inline">\((I_j,L_j)\)</span></span>. Thus <span><span class="math inline">\(zip\)</span></span> “zips together” these two lists of elements into a single list of pairs.<sup id="fnref:8"><a href="#fn:8" rel="footnote">8</a></sup></p>
</div>
<div id="lambdaturing-thm" class="exercise" title="Next-step function without $RECURSE$" name="Exercise 7.11 (Next-step function without $RECURSE$) ">
<p>Let <span><span class="math inline">\(M\)</span></span> be a Turing machine. Give an enhanced λ calculus expression to compute the next-step function <span><span class="math inline">\(\ensuremath{\mathit{NEXT}}_M\)</span></span> of <span><span class="math inline">\(M\)</span></span> (as in the proof of <a href='#lambdaturing-thm'>Exercise 7.11</a>) <em>without using <span><span class="math inline">\(\ensuremath{\mathit{RECURSE}}\)</span></span></em>. See footnote for hint.<sup id="fnref:9"><a href="#fn:9" rel="footnote">9</a></sup></p>
</div>
<div id="lambdacompiler" class="exercise" title="λ calculus to NAND-TM compiler (challenging)" name="Exercise 7.12 (λ calculus to NAND-TM compiler (challenging)) ">
<p>Give a program in the programming language of your choice that takes as input a λ expression <span><span class="math inline">\(e\)</span></span> and outputs a NAND-TM program <span><span class="math inline">\(P\)</span></span> that computes the same function as <span><span class="math inline">\(e\)</span></span>. For partial credit you can use the <code>GOTO</code> and all NAND-CIRC syntactic sugar in your output program. You can use any encoding of λ expressions as binary string that is convenient for you. See footnote for hint.<sup id="fnref:10"><a href="#fn:10" rel="footnote">10</a></sup></p>
</div>
<div id="altlambdaex" class="exercise" title="At least two in $\lambda$ calculus" name="Exercise 7.13 (At least two in $\lambda$ calculus) ">
<p>Let <span><span class="math inline">\(1 = \lambda x,y.x\)</span></span> and <span><span class="math inline">\(0 = \lambda x,y.y\)</span></span> as before. Define</p>
<p><span>
<div class='myequationbox'><span class="math display">\[
\ensuremath{\mathit{ALT}} = \lambda a,b,c.(a (b 1 (c 1 0)) (b c 0))
\]</span></div></span></p>
<p>Prove that <span><span class="math inline">\(\ensuremath{\mathit{ALT}}\)</span></span> is a <span><span class="math inline">\(\lambda\)</span></span> expression that computes the <em>at least two</em> function. That is, for every <span><span class="math inline">\(a,b,c \in \{0,1\}\)</span></span> (as encoded above) <span><span class="math inline">\(\ensuremath{\mathit{ALT}} a b c = 1\)</span></span> if and only at least two of <span><span class="math inline">\(\{a,b,c\}\)</span></span> are equal to <span><span class="math inline">\(1\)</span></span>.</p>
</div>
<div id="stringsprogramex" class="exercise" title="Locality of next-step function" name="Exercise 7.14 (Locality of next-step function) ">
<p>This question will help you get a better sense of the notion of <em>locality of the next step function</em> of Turing Machines. This locality plays an important role in results such as the Turing completeness of <span><span class="math inline">\(\lambda\)</span></span> calculus and one dimensional cellular automata, as well as results such as Godel’s Incompleteness Theorem and the Cook Levin theorem that we will see later in this course. Define <code>STRINGS</code> to be the a programming language that has the following semantics:</p>
<ul>
<li><p>A <code>STRINGS</code> program <span><span class="math inline">\(Q\)</span></span> has a single string variable <code>str</code> that is both the input and the output of <span><span class="math inline">\(Q\)</span></span>. The program has no loops and no other variables, but rather consists of a sequence of conditional search and replace operations that modify <code>str</code>.</p></li>
<li><p>The operations of a <code>STRINGS</code> program are:</p>
<ul>
<li><code>REPLACE(pattern1,pattern2)</code> where <code>pattern1</code> and <code>pattern2</code> are fixed strings. This replaces the first occurrence of <code>pattern1</code> in <code>str</code> with <code>pattern2</code></li>
<li><code>if search(pattern) { code }</code> executes <code>code</code> if <code>pattern</code> is a substring of <code>str</code>. The code <code>code</code> can itself include nested <code>if</code>’s. (One can also add an <code>else { ... }</code> to execute if <code>pattern</code> is <em>not</em> a substring of condf).</li>
<li>the returned value is <code>str</code></li>
</ul></li>
<li><p>A <code>STRING</code> program <span><span class="math inline">\(Q\)</span></span> computes a function <span><span class="math inline">\(F:\{0,1\}^* \rightarrow \{0,1\}^*\)</span></span> if for every <span><span class="math inline">\(x\in \{0,1\}^*\)</span></span>, if we initialize <code>str</code> to <span><span class="math inline">\(x\)</span></span> and then execute the sequence of instructions in <span><span class="math inline">\(Q\)</span></span>, then at the end of the execution <code>str</code> equals <span><span class="math inline">\(F(x)\)</span></span>.</p></li>
</ul>
<p>For example, the following is a <code>STRINGS</code> program that computes the function <span><span class="math inline">\(F:\{0,1\}^* \rightarrow \{0,1\}^*\)</span></span> such that for every <span><span class="math inline">\(x\in \{0,1\}^*\)</span></span>, if <span><span class="math inline">\(x\)</span></span> contains a substring of the form <span><span class="math inline">\(y=11ab11\)</span></span> where <span><span class="math inline">\(a,b \in \{0,1\}\)</span></span>, then <span><span class="math inline">\(F(x)=x&#39;\)</span></span> where <span><span class="math inline">\(x&#39;\)</span></span> is obtained by replacing the first occurrence of <span><span class="math inline">\(y\)</span></span> in <span><span class="math inline">\(x\)</span></span> with <span><span class="math inline">\(00\)</span></span>.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb12-1" title="1"><span class="cf">if</span> search(<span class="st">&#39;110011&#39;</span>) {</a>
<a class="sourceLine" id="cb12-2" title="2">    replace(<span class="st">&#39;110011&#39;</span>,<span class="st">&#39;00&#39;</span>)</a>
<a class="sourceLine" id="cb12-3" title="3">} <span class="cf">else</span> <span class="cf">if</span> search(<span class="st">&#39;110111&#39;</span>) {</a>
<a class="sourceLine" id="cb12-4" title="4">    replace(<span class="st">&#39;110111&#39;</span>,<span class="st">&#39;00&#39;</span>)</a>
<a class="sourceLine" id="cb12-5" title="5">} <span class="cf">else</span> <span class="cf">if</span> search(<span class="st">&#39;111011&#39;</span>) {</a>
<a class="sourceLine" id="cb12-6" title="6">    replace(<span class="st">&#39;111011&#39;</span>,<span class="st">&#39;00&#39;</span>)</a>
<a class="sourceLine" id="cb12-7" title="7">} <span class="cf">else</span> <span class="cf">if</span> search(<span class="st">&#39;111111&#39;</span>) {</a>
<a class="sourceLine" id="cb12-8" title="8">    replace(<span class="st">&#39;1111111&#39;</span>,<span class="st">&#39;00&#39;</span>)</a>
<a class="sourceLine" id="cb12-9" title="9">}</a></code></pre></div>
<p>Prove that for every Turing Machine program <span><span class="math inline">\(M\)</span></span>, there exists a <code>STRINGS</code> program <span><span class="math inline">\(Q\)</span></span> that computes the <span><span class="math inline">\(\ensuremath{\mathit{NEXT}}_M\)</span></span> function that maps every string encoding a valid <em>configuration</em> of <span><span class="math inline">\(M\)</span></span> to the string encoding the configuration of the next step of <span><span class="math inline">\(M\)</span></span>’s computation. (We don’t care what the function will do on strings that do not encode a valid configuration.) You don’t have to write the <code>STRINGS</code> program fully, but you do need to give a convincing argument that such a program exists.</p>
</div>
<h2 id="othermodelsbibnotes" data-number="7.10">Bibliographical notes</h2>
<p>Chapters 7 in the wonderful book of Moore and Mertens  (<a href="https://scholar.google.com/scholar?hl=en&q=Moore,+Mertens+The+nature+of+computation" target="_blank">Moore, Mertens, 2011</a>)  contains a great exposition much of this material. .</p>
<p>The RAM model can be very useful in studying the concrete complexity of practical algorithms. Its theoretical study was initiated in  (<a href="https://scholar.google.com/scholar?hl=en&q=Cook,+Reckhow+Time+bounded+random+access+machines" target="_blank">Cook, Reckhow, 1973</a>) . However, the exact set of operations that are allowed in the RAM model and their costs vary between texts and contexts. One needs to be careful in making such definitions, especially if the word size grows, as was already shown by Shamir  (<a href="https://scholar.google.com/scholar?hl=en&q=Shamir+Factoring+numbers+in+O+(logn)+arithmetic+steps" target="_blank">Shamir, 1979</a>) . Chapter 3 in Savage’s book  (<a href="https://scholar.google.com/scholar?hl=en&q=Savage+Models+of+computation" target="_blank">Savage, 1998</a>)  contains a more formal description of RAM machines, see also the paper  (<a href="https://scholar.google.com/scholar?hl=en&q=Hagerup+Sorting+and+searching+on+the+word+RAM" target="_blank">Hagerup, 1998</a>) . A study of RAM algorithms that are independent of the input size (known as the “transdichotomous RAM model”) was initiated by  (<a href="https://scholar.google.com/scholar?hl=en&q=Fredman,+Willard+Surpassing+the+information+theoretic+bound+with+fusion+trees" target="_blank">Fredman, Willard, 1993</a>) </p>
<p>The models of computation we considered so far are inherently sequential, but these days much computation happens in parallel, whether using multi-core processors or in massively parallel distributed computation in data centers or over the Internet. Parallel computing is important in practice, but it does not really make much difference for the question of what can and can’t be computed. After all, if a computation can be performed using <span><span class="math inline">\(m\)</span></span> machines in <span><span class="math inline">\(t\)</span></span> time, then it can be computed by a single machine in time <span><span class="math inline">\(mt\)</span></span>.</p>
<p>The λ-calculus was described by Church in  (<a href="https://scholar.google.com/scholar?hl=en&q=Church+The+calculi+of+lambda-conversion" target="_blank">Church, 1941</a>) . Pierce’s book  (<a href="https://scholar.google.com/scholar?hl=en&q=Pierce+Types+and+programming+languages" target="_blank">Pierce, 2002</a>)  is a canonical textbook, see also  (<a href="https://scholar.google.com/scholar?hl=en&q=Barendregt+The+lambda+calculus+:+its+syntax+and+semantics" target="_blank">Barendregt, 1984</a>) . The “Currying technique” is named after the logician <a href="https://goo.gl/C9hKz1">Haskell Curry</a> (the <em>Haskell</em> programming language is named after Haskell Curry as well). Curry himself attributed this concept to <a href="https://goo.gl/qJqd47">Moses Schönfinkel</a>, though for some reason the term “Schönfinkeling” never caught on.</p>
<p>Unlike most programming languages, the pure λ-calculus doesn’t have the notion of <em>types</em>. Every object in the λ calculus can also be thought of as a λ expression and hence as a function that takes one input and returns one output. All functions take one input and return one output, and if you feed a function an input of a form it didn’t expect, it still evaluates the λ expression via “search and replace”, replacing all instances of its parameter with copies of the input expression you fed it. Typed variants of the λ calculus are objects of intense research, and are strongly related to type systems for programming language and computer-verifiable proof systems, see  (<a href="https://scholar.google.com/scholar?hl=en&q=Pierce+Types+and+programming+languages" target="_blank">Pierce, 2002</a>) . Some of the typed variants of the λ calculus do not have infinite loops, which makes them very useful as ways of enabling static analysis of programs as well as computer-verifiable proofs. We will come back to this point in <a href='lec_08a_restricted_models.html#restrictedchap'>Chapter 9</a> and <a href='lec_24_proofs.html#chapproofs'>Chapter 21</a>.</p>
<p>Tao has <a href="https://terrytao.wordpress.com/2014/02/04/finite-time-blowup-for-an-averaged-three-dimensional-navier-stokes-equation/">proposed</a> showing the Turing completeness of fluid dynamics (a “water computer”) as a way of settling the question of the behavior of the Navier-Stokes equations, see this <a href="https://www.quantamagazine.org/terence-tao-proposes-fluid-new-path-in-navier-stokes-problem-20140224/">popular article</a>.</p>
<div id="footnotediv" class="footnotes">
<ol>
<li class="footnote" id="fn:1"><p>
<div>
<p>Some programming language have fixed (even if extremely large) bounds on the amount of memory they can access, which formally prevent them from being applicable to computing infinite functions and hence simulating Turing machines. We ignore such issues in this discussion and assume access to some storage device without a fixed upper bound on its capacity.</p>
</div>
<a href="#fnref:1" title="return to article"> ↩</a><p></li>
<li class="footnote" id="fn:2"><p>
<div>
<p>In Lisp, the <span><span class="math inline">\(\ensuremath{\mathit{PAIR}}\)</span></span>, <span><span class="math inline">\(\ensuremath{\mathit{HEAD}}\)</span></span> and <span><span class="math inline">\(\ensuremath{\mathit{TAIL}}\)</span></span> functions are <a href="https://goo.gl/BLRd6S">traditionally called</a> <code>cons</code>, <code>car</code> and <code>cdr</code>.</p>
</div>
<a href="#fnref:2" title="return to article"> ↩</a><p></li>
<li class="footnote" id="fn:3"><p>
<div>
<p>Because of specific issues of Python syntax, in this implementation we use <code>f * g</code> for applying <code>f</code> to <code>g</code> rather than <code>fg</code>, and use <code>λx(exp)</code> rather than <code>λx.exp</code> for abstraction. We also use <code>_0</code> and <code>_1</code> for the λ terms for <span><span class="math inline">\(0\)</span></span> and <span><span class="math inline">\(1\)</span></span> so as not to confuse with the Python constants.</p>
</div>
<a href="#fnref:3" title="return to article"> ↩</a><p></li>
<li class="footnote" id="fn:4"><p>
<div>
<p>You don’t have to give a full description of a Turing machine: use our “eat the cake and have it too” paradigm to show the existence of such a machine by arguing from more powerful equivalent models.</p>
</div>
<a href="#fnref:4" title="return to article"> ↩</a><p></li>
<li class="footnote" id="fn:5"><p>
<div>
<p>Same hint as <a href='#longestpathcomputableex'>Exercise 7.5</a> applies. Note that for showing that <span><span class="math inline">\(\ensuremath{\mathit{LONGPATH}}\)</span></span> is computable you don’t have to give an <em>efficient</em> algorithm.</p>
</div>
<a href="#fnref:5" title="return to article"> ↩</a><p></li>
<li class="footnote" id="fn:6"><p>
<div>
<p><strong>Hint:</strong> You can reduce the number of variables a function takes by “pairing them up”. That is, define a λ expression <span><span class="math inline">\(\ensuremath{\mathit{PAIR}}\)</span></span> such that for every <span><span class="math inline">\(x,y\)</span></span> <span><span class="math inline">\(\ensuremath{\mathit{PAIR}} xy\)</span></span> is some function <span><span class="math inline">\(f\)</span></span> such that <span><span class="math inline">\(f0=x\)</span></span> and <span><span class="math inline">\(f1=y\)</span></span>. Then use <span><span class="math inline">\(\ensuremath{\mathit{PAIR}}\)</span></span> to iteratively reduce the number of variables used.</p>
</div>
<a href="#fnref:6" title="return to article"> ↩</a><p></li>
<li class="footnote" id="fn:7"><p>
<div>
<p>Use structural induction on the expression <span><span class="math inline">\(e\)</span></span>.</p>
</div>
<a href="#fnref:7" title="return to article"> ↩</a><p></li>
<li class="footnote" id="fn:8"><p>
<div>
<p>The name <span><span class="math inline">\(zip\)</span></span> is a common name for this operation, for example in Python. It should not be confused with the <code>zip</code> compression file format.</p>
</div>
<a href="#fnref:8" title="return to article"> ↩</a><p></li>
<li class="footnote" id="fn:9"><p>
<div>
<p>Use <span><span class="math inline">\(\ensuremath{\mathit{MAP}}\)</span></span> and <span><span class="math inline">\(\ensuremath{\mathit{REDUCE}}\)</span></span> (and potentially <span><span class="math inline">\(\ensuremath{\mathit{FILTER}}\)</span></span>). You might also find the function <span><span class="math inline">\(zip\)</span></span> of <a href='#zipfunctionex'>Exercise 7.10</a> useful.</p>
</div>
<a href="#fnref:9" title="return to article"> ↩</a><p></li>
<li class="footnote" id="fn:10"><p>
<div>
<p>Try to set up a procedure such that if array <code>Left</code> contains an encoding of a λ expression <span><span class="math inline">\(\lambda x.e\)</span></span> and array <code>Right</code> contains an encoding of another λ expression <span><span class="math inline">\(e&#39;\)</span></span>, then the array <code>Result</code> will contain <span><span class="math inline">\(e[x \rightarrow e&#39;]\)</span></span>.</p>
</div>
<a href="#fnref:10" title="return to article"> ↩</a><p></li>
</ol>
</div>
<!--bookdown:body:end-->


<!-- end of  actual content -->

<!-- start of comments -->


<a name="commentform"></a>
<h2 id="comments" class="nocount">Comments</h2>

<p>Comments are posted on the <a href="https://github.com/boazbk/tcs/issues">GitHub repository</a> using the <a href="https://utteranc.es">utteranc.es</a> app.
A GitHub login is required to comment.
If you don't want to authorize the app to post on your behalf, you can also comment directly on the <a href="https://github.com/boazbk/tcs/issues?q=Defining Computation+in%3Atitle">GitHub issue for this page</a>.


<p>


<script src="https://utteranc.es/client.js" 
repo="boazbk/tcs" 
issue-term="title" 
label="comments"
theme="github-light" 
crossorigin="anonymous" async>
  </script>


<!-- end of comments -->

<p>Compiled on 12/02/2019 19:02:03</p>

<p>Copyright 2019, Boaz Barak.


<a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/"><img alt="Creative Commons License"
    style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png" /></a><br />This work is
licensed under a <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/">Creative Commons
  Attribution-NonCommercial-NoDerivatives 4.0 International License</a>.

<p>Produced using <a href="https://pandoc.org/">pandoc</a> and <a href="http://scorreia.com/software/panflute/">panflute</a> with templates derived from <a href="https://www.gitbook.com/">gitbook</a> and <a href="https://bookdown.org/">bookdown</a>.</p>



</div>


            </section>

          </div>
        </div>
      </div>
<!--bookdown:link_prev-->
<!--bookdown:link_next-->



    </div>
  </div>
<!--bookdown:config-->
<script src="js/app.min.js"></script>
<script src="js/lunr.js"></script>
<script src="js/plugin-search.js"></script>
<script src="js/plugin-sharing.js"></script>
<script src="js/plugin-fontsettings.js"></script>
<script src="js/fullscreen.js"></script>
<script src="js/plugin-bookdown.js"></script>
<script src="js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"history": {
"link": null,
"text": null
},
"download": ["https://files.boazbarak.org/introtcs/lec_07_other_models.pdf"],
"toc": {
"collapse": "section"
}
});
});
</script>


</body>

</html>
