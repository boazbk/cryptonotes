<!DOCTYPE html>
<html  lang="en">

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Introduction to Theoretical Computer Science: Probability theory 101</title>
  <meta name="description" content="Textbook on Theoretical Computer Science by Boaz Barak">

  <meta property="og:title" content="Introduction to Theoretical Computer Science: Probability theory 101" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://introtcs.org/" />
  <meta property="og:image" content="icons/cover.png" />
  <meta property="og:description" content="Textbook on Theoretical Computer Science by Boaz Barak" />
  <meta name="github-repo" content="boazbk/tcs" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Introduction to Theoretical Computer Science" />
  <meta name="twitter:description" content="Textbook on Theoretical Computer Science by Boaz Barak" />
  <meta name="twitter:image" content="https://introtcs.org/icons/cover.png" />

<meta name="author" content="Boaz Barak">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <link rel="apple-touch-icon-precomposed" sizes="120x120" href="icons/apple-touch-icon-120x120.png">
  <link rel="shortcut icon" href="icons/favicon.ico" type="image/x-icon">

<!-- Boaz: resources -->

<!-- <script src="https://kit.fontawesome.com/ab08ce82a8.js"></script> -->

<link rel="stylesheet" src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.9.0/css/all.min.css">


<!-- KaTeX -->


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css"
  integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">

<!-- The loading of KaTeX is deferred to speed up page rendering -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js"
  integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>

<!-- To automatically render math in text elements, include the auto-render extension: -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js"
  integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
  onload='renderMathInElement(document.body, {  throwOnError: false, macros: { "\\N": "\\mathbb{N}", "\\R": "\\mathbb{R}", "\\Z": "\\mathbb{Z}","\\E": "\\mathbb{E}","\\val": "\\mathrm{val}", "\\label": "\\;\\;\\;\\;\\;\\;\\;\\;","\\floor": "\\lfloor #1 \\rfloor","\\ceil": "\\lceil #1 \\rceil", "\\ensuremath": "#1"}});'>
</script>




<!-- KaTeX -->
<!-- pseudocode -->
<link rel="stylesheet" href="css/pseudocode.css">
<!-- <script src="js/pseudocode.min.js"></script> -->


<!-- Gitbook resources -->

  <script src="js/jquery.min.js"></script>
  <link href="css/style.css" rel="stylesheet" />
  
  <link href="css/plugin-table.css" rel="stylesheet" />
  <link href="css/plugin-bookdown.css" rel="stylesheet" />
  <link href="css/plugin-highlight.css" rel="stylesheet" />
  <link href="css/plugin-search.css" rel="stylesheet" />
  <link href="css/plugin-fontsettings.css" rel="stylesheet" />
  <link href="css/moregitbook.css" rel="stylesheet" />

  <link href="css/resmisc.css" rel="stylesheet" />





<!-- Boaz: end resources -->



<!--bookdown:link_prev-->
<!--bookdown:link_next-->




<!-- bigfoot-->

<link href="css/bigfoot-default.css" rel="stylesheet" />
<script type="text/javascript" src="js/bigfoot.js"></script>

<script type="text/javascript">
    var bigfoot = jQuery.bigfoot(
        {
            deleteOnUnhover: false,
            preventPageScroll: false,
            hoverDelay: 250
        }
    );
</script>

<!-- end bigfoot -->


</head>

<body>



<!--bookdown:title:start-->
<!--bookdown:title:end-->


<!--bookdown:toc:start-->
  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">
<!--bookdown:toc2:start-->
<ul class="summary">
<li><a href="./">Introduction to Theoretical Computer Science</a></li>
<li class="divider"></li><li class="chapter" data-level="p" data-path="lec_00_0_preface.html"><a href="lec_00_0_preface.html"><i class="fa fa-check"></i><b>p</b> Preface</a><ul><li class="chapter" data-level="p.1" data-path="lec_00_0_preface.html"><a href="lec_00_0_preface.html#to-the-student"><i class="fa fa-check"></i><b>p.1</b> To the student</a><ul><li class="chapter" data-level="p.1.1" data-path="lec_00_0_preface.html"><a href="lec_00_0_preface.html#is-the-effort-worth-it"><i class="fa fa-check"></i><b>p.1.1</b> Is the effort worth it?</a></li></ul></li><li class="chapter" data-level="p.2" data-path="lec_00_0_preface.html"><a href="lec_00_0_preface.html#to-potential-instructors"><i class="fa fa-check"></i><b>p.2</b> To potential instructors</a></li><li class="chapter" data-level="p.3" data-path="lec_00_0_preface.html"><a href="lec_00_0_preface.html#acknowledgements"><i class="fa fa-check"></i><b>p.3</b> Acknowledgements</a></li></ul></li><li class="chapter" data-level="0" data-path="lec_01_introduction.html"><a href="lec_01_introduction.html"><i class="fa fa-check"></i><b>0</b> Introduction</a><ul><li class="chapter" data-level="0.1" data-path="lec_01_introduction.html"><a href="lec_01_introduction.html#integer-multiplication-an-example-of-an-algorithm"><i class="fa fa-check"></i><b>0.1</b> Integer multiplication: an example of an algorithm</a></li><li class="chapter" data-level="0.2" data-path="lec_01_introduction.html"><a href="lec_01_introduction.html#karatsubasec"><i class="fa fa-check"></i><b>0.2</b> Extended Example: A faster way to multiply (optional)</a></li><li class="chapter" data-level="0.3" data-path="lec_01_introduction.html"><a href="lec_01_introduction.html#algsbeyondarithmetic"><i class="fa fa-check"></i><b>0.3</b> Algorithms beyond arithmetic</a></li><li class="chapter" data-level="0.4" data-path="lec_01_introduction.html"><a href="lec_01_introduction.html#on-the-importance-of-negative-results."><i class="fa fa-check"></i><b>0.4</b> On the importance of negative results.</a></li><li class="chapter" data-level="0.5" data-path="lec_01_introduction.html"><a href="lec_01_introduction.html#roadmapsec"><i class="fa fa-check"></i><b>0.5</b> Roadmap to the rest of this book</a><ul><li class="chapter" data-level="0.5.1" data-path="lec_01_introduction.html"><a href="lec_01_introduction.html#dependencies-between-chapters"><i class="fa fa-check"></i><b>0.5.1</b> Dependencies between chapters</a></li></ul></li><li class="chapter" data-level="0.6" data-path="lec_01_introduction.html"><a href="lec_01_introduction.html#exercises"><i class="fa fa-check"></i><b>0.6</b> Exercises</a></li><li class="chapter" data-level="0.7" data-path="lec_01_introduction.html"><a href="lec_01_introduction.html#bnotesintrosec"><i class="fa fa-check"></i><b>0.7</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="1" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html"><i class="fa fa-check"></i><b>1</b> Mathematical Background</a><ul><li class="chapter" data-level="1.1" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#manualbackground"><i class="fa fa-check"></i><b>1.1</b> This chapter: a reader’s manual</a></li><li class="chapter" data-level="1.2" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#secmathoverview"><i class="fa fa-check"></i><b>1.2</b> A quick overview of mathematical prerequisites</a></li><li class="chapter" data-level="1.3" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#reading-mathematical-texts"><i class="fa fa-check"></i><b>1.3</b> Reading mathematical texts</a><ul><li class="chapter" data-level="1.3.1" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#definitions"><i class="fa fa-check"></i><b>1.3.1</b> Definitions</a></li><li class="chapter" data-level="1.3.2" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#assertions-theorems-lemmas-claims"><i class="fa fa-check"></i><b>1.3.2</b> Assertions: Theorems, lemmas, claims</a></li><li class="chapter" data-level="1.3.3" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#proofs"><i class="fa fa-check"></i><b>1.3.3</b> Proofs</a></li></ul></li><li class="chapter" data-level="1.4" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#basic-discrete-math-objects"><i class="fa fa-check"></i><b>1.4</b> Basic discrete math objects</a><ul><li class="chapter" data-level="1.4.1" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#sets"><i class="fa fa-check"></i><b>1.4.1</b> Sets</a></li><li class="chapter" data-level="1.4.2" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#specialsets"><i class="fa fa-check"></i><b>1.4.2</b> Special sets</a></li><li class="chapter" data-level="1.4.3" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#functionsec"><i class="fa fa-check"></i><b>1.4.3</b> Functions</a></li><li class="chapter" data-level="1.4.4" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#graphsec"><i class="fa fa-check"></i><b>1.4.4</b> Graphs</a></li><li class="chapter" data-level="1.4.5" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#secquantifiers"><i class="fa fa-check"></i><b>1.4.5</b> Logic operators and quantifiers</a></li><li class="chapter" data-level="1.4.6" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#secquantifierssums"><i class="fa fa-check"></i><b>1.4.6</b> Quantifiers for summations and products</a></li><li class="chapter" data-level="1.4.7" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#boundvarsec"><i class="fa fa-check"></i><b>1.4.7</b> Parsing formulas: bound and free variables</a></li><li class="chapter" data-level="1.4.8" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#secbigohnotation"><i class="fa fa-check"></i><b>1.4.8</b> Asymptotics and Big-O notation</a></li><li class="chapter" data-level="1.4.9" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#some-rules-of-thumb-for-big-o-notation"><i class="fa fa-check"></i><b>1.4.9</b> Some rules of thumb for Big-O notation</a></li></ul></li><li class="chapter" data-level="1.5" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#proofsbackgroundsec"><i class="fa fa-check"></i><b>1.5</b> Proofs</a><ul><li class="chapter" data-level="1.5.1" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#proofs-and-programs"><i class="fa fa-check"></i><b>1.5.1</b> Proofs and programs</a></li><li class="chapter" data-level="1.5.2" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#proof-writing-style"><i class="fa fa-check"></i><b>1.5.2</b> Proof writing style</a></li><li class="chapter" data-level="1.5.3" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#patterns-in-proofs"><i class="fa fa-check"></i><b>1.5.3</b> Patterns in proofs</a></li></ul></li><li class="chapter" data-level="1.6" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#topsortsec"><i class="fa fa-check"></i><b>1.6</b> Extended example: Topological Sorting</a><ul><li class="chapter" data-level="1.6.1" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#inductionsec"><i class="fa fa-check"></i><b>1.6.1</b> Mathematical induction</a></li><li class="chapter" data-level="1.6.2" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#proving-the-result-by-induction"><i class="fa fa-check"></i><b>1.6.2</b> Proving the result by induction</a></li><li class="chapter" data-level="1.6.3" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#minimality-and-uniqueness"><i class="fa fa-check"></i><b>1.6.3</b> Minimality and uniqueness</a></li></ul></li><li class="chapter" data-level="1.7" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#notationsec"><i class="fa fa-check"></i><b>1.7</b> This book: notation and conventions</a><ul><li class="chapter" data-level="1.7.1" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#conventionsec"><i class="fa fa-check"></i><b>1.7.1</b> Variable name conventions</a></li><li class="chapter" data-level="1.7.2" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#some-idioms"><i class="fa fa-check"></i><b>1.7.2</b> Some idioms</a></li></ul></li><li class="chapter" data-level="1.8" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#exercises"><i class="fa fa-check"></i><b>1.8</b> Exercises</a></li><li class="chapter" data-level="1.9" data-path="lec_00_1_math_background.html"><a href="lec_00_1_math_background.html#notesmathchap"><i class="fa fa-check"></i><b>1.9</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="2" data-path="lec_02_representation.html"><a href="lec_02_representation.html"><i class="fa fa-check"></i><b>2</b> Computation and Representation</a><ul><li class="chapter" data-level="2.1" data-path="lec_02_representation.html"><a href="lec_02_representation.html#defining-representations"><i class="fa fa-check"></i><b>2.1</b> Defining representations</a><ul><li class="chapter" data-level="2.1.1" data-path="lec_02_representation.html"><a href="lec_02_representation.html#representing-natural-numbers"><i class="fa fa-check"></i><b>2.1.1</b> Representing natural numbers</a></li><li class="chapter" data-level="2.1.2" data-path="lec_02_representation.html"><a href="lec_02_representation.html#meaning-of-representations-discussion"><i class="fa fa-check"></i><b>2.1.2</b> Meaning of representations (discussion)</a></li></ul></li><li class="chapter" data-level="2.2" data-path="lec_02_representation.html"><a href="lec_02_representation.html#representations-beyond-natural-numbers"><i class="fa fa-check"></i><b>2.2</b> Representations beyond natural numbers</a><ul><li class="chapter" data-level="2.2.1" data-path="lec_02_representation.html"><a href="lec_02_representation.html#repnegativeintegerssec"><i class="fa fa-check"></i><b>2.2.1</b> Representing (potentially negative) integers</a></li><li class="chapter" data-level="2.2.2" data-path="lec_02_representation.html"><a href="lec_02_representation.html#twoscomplement"><i class="fa fa-check"></i><b>2.2.2</b> Two’s complement representation (optional)</a></li><li class="chapter" data-level="2.2.3" data-path="lec_02_representation.html"><a href="lec_02_representation.html#rational-numbers-and-representing-pairs-of-strings"><i class="fa fa-check"></i><b>2.2.3</b> Rational numbers, and representing pairs of strings</a></li></ul></li><li class="chapter" data-level="2.3" data-path="lec_02_representation.html"><a href="lec_02_representation.html#representing-real-numbers"><i class="fa fa-check"></i><b>2.3</b> Representing real numbers</a><ul><li class="chapter" data-level="2.3.1" data-path="lec_02_representation.html"><a href="lec_02_representation.html#cantorsec"><i class="fa fa-check"></i><b>2.3.1</b> Can we represent reals exactly?</a></li></ul></li><li class="chapter" data-level="2.4" data-path="lec_02_representation.html"><a href="lec_02_representation.html#representing-objects-beyond-numbers"><i class="fa fa-check"></i><b>2.4</b> Representing objects beyond numbers</a><ul><li class="chapter" data-level="2.4.1" data-path="lec_02_representation.html"><a href="lec_02_representation.html#finite-representations"><i class="fa fa-check"></i><b>2.4.1</b> Finite representations</a></li><li class="chapter" data-level="2.4.2" data-path="lec_02_representation.html"><a href="lec_02_representation.html#prefixfreesec"><i class="fa fa-check"></i><b>2.4.2</b> Prefix-free encoding</a></li><li class="chapter" data-level="2.4.3" data-path="lec_02_representation.html"><a href="lec_02_representation.html#making-representations-prefix-free"><i class="fa fa-check"></i><b>2.4.3</b> Making representations prefix-free</a></li><li class="chapter" data-level="2.4.4" data-path="lec_02_representation.html"><a href="lec_02_representation.html#proof-by-python-optional"><i class="fa fa-check"></i><b>2.4.4</b> Proof by Python (optional)</a></li><li class="chapter" data-level="2.4.5" data-path="lec_02_representation.html"><a href="lec_02_representation.html#representing-letters-and-text"><i class="fa fa-check"></i><b>2.4.5</b> Representing letters and text</a></li><li class="chapter" data-level="2.4.6" data-path="lec_02_representation.html"><a href="lec_02_representation.html#representing-vectors-matrices-images"><i class="fa fa-check"></i><b>2.4.6</b> Representing vectors, matrices, images</a></li><li class="chapter" data-level="2.4.7" data-path="lec_02_representation.html"><a href="lec_02_representation.html#representing-graphs"><i class="fa fa-check"></i><b>2.4.7</b> Representing graphs</a></li><li class="chapter" data-level="2.4.8" data-path="lec_02_representation.html"><a href="lec_02_representation.html#representing-lists-and-nested-lists"><i class="fa fa-check"></i><b>2.4.8</b> Representing lists and nested lists</a></li><li class="chapter" data-level="2.4.9" data-path="lec_02_representation.html"><a href="lec_02_representation.html#notation"><i class="fa fa-check"></i><b>2.4.9</b> Notation</a></li></ul></li><li class="chapter" data-level="2.5" data-path="lec_02_representation.html"><a href="lec_02_representation.html#defining-computational-tasks-as-mathematical-functions"><i class="fa fa-check"></i><b>2.5</b> Defining computational tasks as mathematical functions</a><ul><li class="chapter" data-level="2.5.1" data-path="lec_02_representation.html"><a href="lec_02_representation.html#secimplvsspec"><i class="fa fa-check"></i><b>2.5.1</b> Distinguish functions from programs!</a></li></ul></li><li class="chapter" data-level="2.6" data-path="lec_02_representation.html"><a href="lec_02_representation.html#exercises"><i class="fa fa-check"></i><b>2.6</b> Exercises</a></li><li class="chapter" data-level="2.7" data-path="lec_02_representation.html"><a href="lec_02_representation.html#bibnotesrepres"><i class="fa fa-check"></i><b>2.7</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="3" data-path="lec_03_computation.html"><a href="lec_03_computation.html"><i class="fa fa-check"></i><b>3</b> Defining computation</a><ul><li class="chapter" data-level="3.1" data-path="lec_03_computation.html"><a href="lec_03_computation.html#defining-computation"><i class="fa fa-check"></i><b>3.1</b> Defining computation</a></li><li class="chapter" data-level="3.2" data-path="lec_03_computation.html"><a href="lec_03_computation.html#computing-using-and-or-and-not."><i class="fa fa-check"></i><b>3.2</b> Computing using AND, OR, and NOT.</a><ul><li class="chapter" data-level="3.2.1" data-path="lec_03_computation.html"><a href="lec_03_computation.html#some-properties-of-and-and-or"><i class="fa fa-check"></i><b>3.2.1</b> Some properties of AND and OR</a></li><li class="chapter" data-level="3.2.2" data-path="lec_03_computation.html"><a href="lec_03_computation.html#xoraonexample"><i class="fa fa-check"></i><b>3.2.2</b> Extended example: Computing \ensuremath{\mathit{XOR}} from \ensuremath{\mathit{AND}}, \ensuremath{\mathit{OR}}, and \ensuremath{\mathit{NOT}}</a></li><li class="chapter" data-level="3.2.3" data-path="lec_03_computation.html"><a href="lec_03_computation.html#informally-defining-basic-operations-and-algorithms"><i class="fa fa-check"></i><b>3.2.3</b> Informally defining basic operations and algorithms</a></li></ul></li><li class="chapter" data-level="3.3" data-path="lec_03_computation.html"><a href="lec_03_computation.html#booleancircuitfig"><i class="fa fa-check"></i><b>3.3</b> Boolean Circuits</a><ul><li class="chapter" data-level="3.3.1" data-path="lec_03_computation.html"><a href="lec_03_computation.html#boolean-circuits-a-formal-definition"><i class="fa fa-check"></i><b>3.3.1</b> Boolean circuits: a formal definition</a></li><li class="chapter" data-level="3.3.2" data-path="lec_03_computation.html"><a href="lec_03_computation.html#equivalence-of-circuits-and-straight-line-programs"><i class="fa fa-check"></i><b>3.3.2</b> Equivalence of circuits and straight-line programs</a></li></ul></li><li class="chapter" data-level="3.4" data-path="lec_03_computation.html"><a href="lec_03_computation.html#physicalimplementationsec"><i class="fa fa-check"></i><b>3.4</b> Physical implementations of computing devices (digression)</a><ul><li class="chapter" data-level="3.4.1" data-path="lec_03_computation.html"><a href="lec_03_computation.html#transistors"><i class="fa fa-check"></i><b>3.4.1</b> Transistors</a></li><li class="chapter" data-level="3.4.2" data-path="lec_03_computation.html"><a href="lec_03_computation.html#logical-gates-from-transistors"><i class="fa fa-check"></i><b>3.4.2</b> Logical gates from transistors</a></li><li class="chapter" data-level="3.4.3" data-path="lec_03_computation.html"><a href="lec_03_computation.html#biological-computing"><i class="fa fa-check"></i><b>3.4.3</b> Biological computing</a></li><li class="chapter" data-level="3.4.4" data-path="lec_03_computation.html"><a href="lec_03_computation.html#cellular-automata-and-the-game-of-life"><i class="fa fa-check"></i><b>3.4.4</b> Cellular automata and the game of life</a></li><li class="chapter" data-level="3.4.5" data-path="lec_03_computation.html"><a href="lec_03_computation.html#neural-networks"><i class="fa fa-check"></i><b>3.4.5</b> Neural networks</a></li><li class="chapter" data-level="3.4.6" data-path="lec_03_computation.html"><a href="lec_03_computation.html#a-computer-made-from-marbles-and-pipes"><i class="fa fa-check"></i><b>3.4.6</b> A computer made from marbles and pipes</a></li></ul></li><li class="chapter" data-level="3.5" data-path="lec_03_computation.html"><a href="lec_03_computation.html#nandsec"><i class="fa fa-check"></i><b>3.5</b> The NAND function</a><ul><li class="chapter" data-level="3.5.1" data-path="lec_03_computation.html"><a href="lec_03_computation.html#nand-circuits"><i class="fa fa-check"></i><b>3.5.1</b> NAND Circuits</a></li><li class="chapter" data-level="3.5.2" data-path="lec_03_computation.html"><a href="lec_03_computation.html#more-examples-of-nand-circuits-optional"><i class="fa fa-check"></i><b>3.5.2</b> More examples of NAND circuits (optional)</a></li><li class="chapter" data-level="3.5.3" data-path="lec_03_computation.html"><a href="lec_03_computation.html#nandcircsec"><i class="fa fa-check"></i><b>3.5.3</b> The NAND-CIRC Programming language</a></li></ul></li><li class="chapter" data-level="3.6" data-path="lec_03_computation.html"><a href="lec_03_computation.html#equivalence-of-all-these-models"><i class="fa fa-check"></i><b>3.6</b> Equivalence of all these models</a><ul><li class="chapter" data-level="3.6.1" data-path="lec_03_computation.html"><a href="lec_03_computation.html#othergatessec"><i class="fa fa-check"></i><b>3.6.1</b> Circuits with other gate sets</a></li><li class="chapter" data-level="3.6.2" data-path="lec_03_computation.html"><a href="lec_03_computation.html#specvsimplrem"><i class="fa fa-check"></i><b>3.6.2</b> Specification vs. implementation (again)</a></li></ul></li><li class="chapter" data-level="3.7" data-path="lec_03_computation.html"><a href="lec_03_computation.html#exercises"><i class="fa fa-check"></i><b>3.7</b> Exercises</a></li><li class="chapter" data-level="3.8" data-path="lec_03_computation.html"><a href="lec_03_computation.html#biographical-notes"><i class="fa fa-check"></i><b>3.8</b> Biographical notes</a></li></ul></li><li class="chapter" data-level="4" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html"><i class="fa fa-check"></i><b>4</b> Syntactic sugar, and computing every function</a><ul><li class="chapter" data-level="4.1" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#secsyntacticsugar"><i class="fa fa-check"></i><b>4.1</b> Some examples of syntactic sugar</a><ul><li class="chapter" data-level="4.1.1" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#user-defined-procedures"><i class="fa fa-check"></i><b>4.1.1</b> User-defined procedures</a></li><li class="chapter" data-level="4.1.2" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#functionsynsugarthmpython"><i class="fa fa-check"></i><b>4.1.2</b> Proof by Python (optional)</a></li><li class="chapter" data-level="4.1.3" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#ifstatementsec"><i class="fa fa-check"></i><b>4.1.3</b> Conditional statements</a></li></ul></li><li class="chapter" data-level="4.2" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#addexample"><i class="fa fa-check"></i><b>4.2</b> Extended example: Addition and Multiplication (optional)</a></li><li class="chapter" data-level="4.3" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#seclookupfunc"><i class="fa fa-check"></i><b>4.3</b> The LOOKUP function</a><ul><li class="chapter" data-level="4.3.1" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#constructing-a-nand-circ-program-for-lookup"><i class="fa fa-check"></i><b>4.3.1</b> Constructing a NAND-CIRC program for \ensuremath{\mathit{LOOKUP}}</a></li></ul></li><li class="chapter" data-level="4.4" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#seccomputeallfunctions"><i class="fa fa-check"></i><b>4.4</b> Computing every function</a><ul><li class="chapter" data-level="4.4.1" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#proof-of-nands-universality"><i class="fa fa-check"></i><b>4.4.1</b> Proof of NAND’s Universality</a></li><li class="chapter" data-level="4.4.2" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#tight-upper-bound"><i class="fa fa-check"></i><b>4.4.2</b> Improving by a factor of n (optional)</a></li></ul></li><li class="chapter" data-level="4.5" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#seccomputalternative"><i class="fa fa-check"></i><b>4.5</b> Computing every function: An alternative proof</a></li><li class="chapter" data-level="4.6" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#secdefinesizeclasses"><i class="fa fa-check"></i><b>4.6</b> The class \ensuremath{\mathit{SIZE}}(T)</a></li><li class="chapter" data-level="4.7" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#exercises"><i class="fa fa-check"></i><b>4.7</b> Exercises</a></li><li class="chapter" data-level="4.8" data-path="lec_03a_computing_every_function.html"><a href="lec_03a_computing_every_function.html#computeeveryfunctionbibnotes"><i class="fa fa-check"></i><b>4.8</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="5" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html"><i class="fa fa-check"></i><b>5</b> Code as data, data as code</a><ul><li class="chapter" data-level="5.1" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#representprogramsec"><i class="fa fa-check"></i><b>5.1</b> Representing programs as strings</a></li><li class="chapter" data-level="5.2" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#countingcircuitsec"><i class="fa fa-check"></i><b>5.2</b> Counting programs, and lower bounds on the size of NAND-CIRC programs</a><ul><li class="chapter" data-level="5.2.1" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#size-hierarchy-theorem-optional"><i class="fa fa-check"></i><b>5.2.1</b> Size hierarchy theorem (optional)</a></li></ul></li><li class="chapter" data-level="5.3" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#listoftuplesrepsec"><i class="fa fa-check"></i><b>5.3</b> The tuples representation</a><ul><li class="chapter" data-level="5.3.1" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#stringrepresentationrpgoramsec"><i class="fa fa-check"></i><b>5.3.1</b> From tuples to strings</a></li></ul></li><li class="chapter" data-level="5.4" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#a-nand-circ-interpreter-in-nand-circ"><i class="fa fa-check"></i><b>5.4</b> A NAND-CIRC interpreter in NAND-CIRC</a><ul><li class="chapter" data-level="5.4.1" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#efficient-universal-programs"><i class="fa fa-check"></i><b>5.4.1</b> Efficient universal programs</a></li><li class="chapter" data-level="5.4.2" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#a-nand-circ-interpeter-in-pseudocode"><i class="fa fa-check"></i><b>5.4.2</b> A NAND-CIRC interpeter in pseudocode</a></li><li class="chapter" data-level="5.4.3" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#nandevalpythonsec"><i class="fa fa-check"></i><b>5.4.3</b> A NAND interpreter in Python</a></li><li class="chapter" data-level="5.4.4" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#constructing-the-nand-circ-interpreter-in-nand-circ"><i class="fa fa-check"></i><b>5.4.4</b> Constructing the NAND-CIRC interpreter in NAND-CIRC</a></li></ul></li><li class="chapter" data-level="5.5" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#a-python-interpreter-in-nand-circ-discussion"><i class="fa fa-check"></i><b>5.5</b> A Python interpreter in NAND-CIRC (discussion)</a></li><li class="chapter" data-level="5.6" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#PECTTsec"><i class="fa fa-check"></i><b>5.6</b> The physical extended Church-Turing thesis (discussion)</a><ul><li class="chapter" data-level="5.6.1" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#attempts-at-refuting-the-pectt"><i class="fa fa-check"></i><b>5.6.1</b> Attempts at refuting the PECTT</a></li></ul></li><li class="chapter" data-level="5.7" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#recap-of-part-i-finite-computation"><i class="fa fa-check"></i><b>5.7</b> Recap of Part I: Finite Computation</a></li><li class="chapter" data-level="5.8" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#exercises"><i class="fa fa-check"></i><b>5.8</b> Exercises</a></li><li class="chapter" data-level="5.9" data-path="lec_04_code_and_data.html"><a href="lec_04_code_and_data.html#bibnotescodeasdata"><i class="fa fa-check"></i><b>5.9</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="6" data-path="lec_06_loops.html"><a href="lec_06_loops.html"><i class="fa fa-check"></i><b>6</b> Loops and infinity</a><ul><li class="chapter" data-level="6.1" data-path="lec_06_loops.html"><a href="lec_06_loops.html#turing-machines"><i class="fa fa-check"></i><b>6.1</b> Turing Machines</a><ul><li class="chapter" data-level="6.1.1" data-path="lec_06_loops.html"><a href="lec_06_loops.html#turingmachinepalindrome"><i class="fa fa-check"></i><b>6.1.1</b> Extended example: A Turing machine for palindromes</a></li><li class="chapter" data-level="6.1.2" data-path="lec_06_loops.html"><a href="lec_06_loops.html#turing-machines-a-formal-definition"><i class="fa fa-check"></i><b>6.1.2</b> Turing machines: a formal definition</a></li><li class="chapter" data-level="6.1.3" data-path="lec_06_loops.html"><a href="lec_06_loops.html#computable-functions"><i class="fa fa-check"></i><b>6.1.3</b> Computable functions</a></li><li class="chapter" data-level="6.1.4" data-path="lec_06_loops.html"><a href="lec_06_loops.html#infinite-loops-and-partial-functions"><i class="fa fa-check"></i><b>6.1.4</b> Infinite loops and partial functions</a></li></ul></li><li class="chapter" data-level="6.2" data-path="lec_06_loops.html"><a href="lec_06_loops.html#turing-machines-as-programming-languages"><i class="fa fa-check"></i><b>6.2</b> Turing machines as programming languages</a><ul><li class="chapter" data-level="6.2.1" data-path="lec_06_loops.html"><a href="lec_06_loops.html#the-nand-tm-programming-language"><i class="fa fa-check"></i><b>6.2.1</b> The NAND-TM Programming language</a></li><li class="chapter" data-level="6.2.2" data-path="lec_06_loops.html"><a href="lec_06_loops.html#sneak-peak-nand-tm-vs-turing-machines"><i class="fa fa-check"></i><b>6.2.2</b> Sneak peak: NAND-TM vs Turing machines</a></li><li class="chapter" data-level="6.2.3" data-path="lec_06_loops.html"><a href="lec_06_loops.html#examples"><i class="fa fa-check"></i><b>6.2.3</b> Examples</a></li></ul></li><li class="chapter" data-level="6.3" data-path="lec_06_loops.html"><a href="lec_06_loops.html#equivalence-of-turing-machines-and-nand-tm-programs"><i class="fa fa-check"></i><b>6.3</b> Equivalence of Turing machines and NAND-TM programs</a><ul><li class="chapter" data-level="6.3.1" data-path="lec_06_loops.html"><a href="lec_06_loops.html#specification-vs-implementation-again"><i class="fa fa-check"></i><b>6.3.1</b> Specification vs implementation (again)</a></li></ul></li><li class="chapter" data-level="6.4" data-path="lec_06_loops.html"><a href="lec_06_loops.html#nand-tm-syntactic-sugar"><i class="fa fa-check"></i><b>6.4</b> NAND-TM syntactic sugar</a><ul><li class="chapter" data-level="6.4.1" data-path="lec_06_loops.html"><a href="lec_06_loops.html#nandtminnerloopssec"><i class="fa fa-check"></i><b>6.4.1</b> GOTO and inner loops</a></li></ul></li><li class="chapter" data-level="6.5" data-path="lec_06_loops.html"><a href="lec_06_loops.html#uniformity-and-nand-vs-nand-tm-discussion"><i class="fa fa-check"></i><b>6.5</b> Uniformity, and NAND vs NAND-TM (discussion)</a></li><li class="chapter" data-level="6.6" data-path="lec_06_loops.html"><a href="lec_06_loops.html#exercises"><i class="fa fa-check"></i><b>6.6</b> Exercises</a></li><li class="chapter" data-level="6.7" data-path="lec_06_loops.html"><a href="lec_06_loops.html#chaploopnotes"><i class="fa fa-check"></i><b>6.7</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="7" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html"><i class="fa fa-check"></i><b>7</b> Equivalent models of computation</a><ul><li class="chapter" data-level="7.1" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#ram-machines-and-nand-ram"><i class="fa fa-check"></i><b>7.1</b> RAM machines and NAND-RAM</a></li><li class="chapter" data-level="7.2" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#nandtmgorydetailssec"><i class="fa fa-check"></i><b>7.2</b> The gory details (optional)</a><ul><li class="chapter" data-level="7.2.1" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#indexed-access-in-nand-tm"><i class="fa fa-check"></i><b>7.2.1</b> Indexed access in NAND-TM</a></li><li class="chapter" data-level="7.2.2" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#two-dimensional-arrays-in-nand-tm"><i class="fa fa-check"></i><b>7.2.2</b> Two dimensional arrays in NAND-TM</a></li><li class="chapter" data-level="7.2.3" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#all-the-rest"><i class="fa fa-check"></i><b>7.2.3</b> All the rest</a></li></ul></li><li class="chapter" data-level="7.3" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#turing-equivalence-discussion"><i class="fa fa-check"></i><b>7.3</b> Turing equivalence (discussion)</a><ul><li class="chapter" data-level="7.3.1" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#the-best-of-both-worlds-paradigm"><i class="fa fa-check"></i><b>7.3.1</b> The Best of both worlds paradigm</a></li><li class="chapter" data-level="7.3.2" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#lets-talk-about-abstractions."><i class="fa fa-check"></i><b>7.3.2</b> Let’s talk about abstractions.</a></li><li class="chapter" data-level="7.3.3" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#turingcompletesec"><i class="fa fa-check"></i><b>7.3.3</b> Turing completeness and equivalence, a formal definition (optional)</a></li></ul></li><li class="chapter" data-level="7.4" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#cellularautomatasec"><i class="fa fa-check"></i><b>7.4</b> Cellular automata</a><ul><li class="chapter" data-level="7.4.1" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#one-dimensional-cellular-automata-are-turing-complete"><i class="fa fa-check"></i><b>7.4.1</b> One dimensional cellular automata are Turing complete</a></li><li class="chapter" data-level="7.4.2" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#turingmachinesconfigsec"><i class="fa fa-check"></i><b>7.4.2</b> Configurations of Turing machines and the next-step function</a></li></ul></li><li class="chapter" data-level="7.5" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#lambdacalculussec"><i class="fa fa-check"></i><b>7.5</b> Lambda calculus and functional programming languages</a><ul><li class="chapter" data-level="7.5.1" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#applying-functions-to-functions"><i class="fa fa-check"></i><b>7.5.1</b> Applying functions to functions</a></li><li class="chapter" data-level="7.5.2" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#curryingsec"><i class="fa fa-check"></i><b>7.5.2</b> Obtaining multi-argument functions via Currying</a></li><li class="chapter" data-level="7.5.3" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#formal-description-of-the-λ-calculus."><i class="fa fa-check"></i><b>7.5.3</b> Formal description of the λ calculus.</a></li><li class="chapter" data-level="7.5.4" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#infiniteloopslambda"><i class="fa fa-check"></i><b>7.5.4</b> Infinite loops in the λ calculus</a></li></ul></li><li class="chapter" data-level="7.6" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#the-enhanced-λ-calculus"><i class="fa fa-check"></i><b>7.6</b> The Enhanced λ calculus</a><ul><li class="chapter" data-level="7.6.1" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#computing-a-function-in-the-enhanced-λ-calculus"><i class="fa fa-check"></i><b>7.6.1</b> Computing a function in the enhanced λ calculus</a></li><li class="chapter" data-level="7.6.2" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#enhanced-λ-calculus-is-turing-complete"><i class="fa fa-check"></i><b>7.6.2</b> Enhanced λ calculus is Turing-complete</a></li></ul></li><li class="chapter" data-level="7.7" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#lambdacacluluspuresec"><i class="fa fa-check"></i><b>7.7</b> From enhanced to pure λ calculus</a><ul><li class="chapter" data-level="7.7.1" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#list-processing"><i class="fa fa-check"></i><b>7.7.1</b> List processing</a></li><li class="chapter" data-level="7.7.2" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#ycombinatorsec"><i class="fa fa-check"></i><b>7.7.2</b> The Y combinator, or recursion without recursion</a></li></ul></li><li class="chapter" data-level="7.8" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#churchturingdiscussionsec"><i class="fa fa-check"></i><b>7.8</b> The Church-Turing Thesis (discussion)</a><ul><li class="chapter" data-level="7.8.1" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#different-models-of-computation"><i class="fa fa-check"></i><b>7.8.1</b> Different models of computation</a></li></ul></li><li class="chapter" data-level="7.9" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#exercises"><i class="fa fa-check"></i><b>7.9</b> Exercises</a></li><li class="chapter" data-level="7.10" data-path="lec_07_other_models.html"><a href="lec_07_other_models.html#othermodelsbibnotes"><i class="fa fa-check"></i><b>7.10</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="8" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html"><i class="fa fa-check"></i><b>8</b> Universality and uncomputability</a><ul><li class="chapter" data-level="8.1" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#universality-or-a-meta-circular-evaluator"><i class="fa fa-check"></i><b>8.1</b> Universality or a meta-circular evaluator</a><ul><li class="chapter" data-level="8.1.1" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#representtmsec"><i class="fa fa-check"></i><b>8.1.1</b> Proving the existence of a universal Turing Machine</a></li><li class="chapter" data-level="8.1.2" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#implications-of-universality-discussion"><i class="fa fa-check"></i><b>8.1.2</b> Implications of universality (discussion)</a></li></ul></li><li class="chapter" data-level="8.2" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#is-every-function-computable"><i class="fa fa-check"></i><b>8.2</b> Is every function computable?</a></li><li class="chapter" data-level="8.3" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#haltingsec"><i class="fa fa-check"></i><b>8.3</b> The Halting problem</a><ul><li class="chapter" data-level="8.3.1" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#is-the-halting-problem-really-hard-discussion"><i class="fa fa-check"></i><b>8.3.1</b> Is the Halting problem really hard? (discussion)</a></li><li class="chapter" data-level="8.3.2" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#haltalternativesec"><i class="fa fa-check"></i><b>8.3.2</b> A direct proof of the uncomputability of \ensuremath{\mathit{HALT}} (optional)</a></li></ul></li><li class="chapter" data-level="8.4" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#reductionsuncompsec"><i class="fa fa-check"></i><b>8.4</b> Reductions</a><ul><li class="chapter" data-level="8.4.1" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#example-halting-on-the-zero-problem"><i class="fa fa-check"></i><b>8.4.1</b> Example: Halting on the zero problem</a></li></ul></li><li class="chapter" data-level="8.5" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#rices-theorem-and-the-impossibility-of-general-software-verification"><i class="fa fa-check"></i><b>8.5</b> Rice’s Theorem and the impossibility of general software verification</a><ul><li class="chapter" data-level="8.5.1" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#ricethmsec"><i class="fa fa-check"></i><b>8.5.1</b> Rice’s Theorem</a></li><li class="chapter" data-level="8.5.2" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#halting-and-rices-theorem-for-other-turing-complete-models"><i class="fa fa-check"></i><b>8.5.2</b> Halting and Rice’s Theorem for other Turing-complete models</a></li><li class="chapter" data-level="8.5.3" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#is-software-verification-doomed-discussion"><i class="fa fa-check"></i><b>8.5.3</b> Is software verification doomed? (discussion)</a></li></ul></li><li class="chapter" data-level="8.6" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#exercises"><i class="fa fa-check"></i><b>8.6</b> Exercises</a></li><li class="chapter" data-level="8.7" data-path="lec_08_uncomputability.html"><a href="lec_08_uncomputability.html#uncomputablebibnotes"><i class="fa fa-check"></i><b>8.7</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="9" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html"><i class="fa fa-check"></i><b>9</b> Restricted computational models</a><ul><li class="chapter" data-level="9.1" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#turing-completeness-as-a-bug"><i class="fa fa-check"></i><b>9.1</b> Turing completeness as a bug</a></li><li class="chapter" data-level="9.2" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#regular-expressions"><i class="fa fa-check"></i><b>9.2</b> Regular expressions</a></li><li class="chapter" data-level="9.3" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#deterministic-finite-automata-and-efficient-matching-of-regular-expressions-optional"><i class="fa fa-check"></i><b>9.3</b> Deterministic finite automata, and efficient matching of regular expressions (optional)</a><ul><li class="chapter" data-level="9.3.1" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#matching-regular-expressions-using-constant-memory"><i class="fa fa-check"></i><b>9.3.1</b> Matching regular expressions using constant memory</a></li><li class="chapter" data-level="9.3.2" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#secdfa"><i class="fa fa-check"></i><b>9.3.2</b> Deterministic Finite Automata</a></li><li class="chapter" data-level="9.3.3" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#regular-functions-are-closed-under-complement"><i class="fa fa-check"></i><b>9.3.3</b> Regular functions are closed under complement</a></li></ul></li><li class="chapter" data-level="9.4" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#limitations-of-regular-expressions"><i class="fa fa-check"></i><b>9.4</b> Limitations of regular expressions</a></li><li class="chapter" data-level="9.5" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#other-semantic-properties-of-regular-expressions"><i class="fa fa-check"></i><b>9.5</b> Other semantic properties of regular expressions</a></li><li class="chapter" data-level="9.6" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#seccfg"><i class="fa fa-check"></i><b>9.6</b> Context free grammars</a><ul><li class="chapter" data-level="9.6.1" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#context-free-grammars-as-a-computational-model"><i class="fa fa-check"></i><b>9.6.1</b> Context-free grammars as a computational model</a></li><li class="chapter" data-level="9.6.2" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#the-power-of-context-free-grammars"><i class="fa fa-check"></i><b>9.6.2</b> The power of context free grammars</a></li><li class="chapter" data-level="9.6.3" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#limitations-of-context-free-grammars-optional"><i class="fa fa-check"></i><b>9.6.3</b> Limitations of context-free grammars (optional)</a></li></ul></li><li class="chapter" data-level="9.7" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#semantic-properties-of-context-free-languages"><i class="fa fa-check"></i><b>9.7</b> Semantic properties of context free languages</a><ul><li class="chapter" data-level="9.7.1" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#uncomputability-of-context-free-grammar-equivalence-optional"><i class="fa fa-check"></i><b>9.7.1</b> Uncomputability of context-free grammar equivalence (optional)</a></li></ul></li><li class="chapter" data-level="9.8" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#summary-of-semantic-properties-for-regular-expressions-and-context-free-grammars"><i class="fa fa-check"></i><b>9.8</b> Summary of semantic properties for regular expressions and context-free grammars</a></li><li class="chapter" data-level="9.9" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#exercises"><i class="fa fa-check"></i><b>9.9</b> Exercises</a></li><li class="chapter" data-level="9.10" data-path="lec_08a_restricted_models.html"><a href="lec_08a_restricted_models.html#bibliographical-notes"><i class="fa fa-check"></i><b>9.10</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="10" data-path="lec_09_godel.html"><a href="lec_09_godel.html"><i class="fa fa-check"></i><b>10</b> Is every theorem provable?</a><ul><li class="chapter" data-level="10.1" data-path="lec_09_godel.html"><a href="lec_09_godel.html#godelproofdef"><i class="fa fa-check"></i><b>10.1</b> Hilbert’s Program and Gödel’s Incompleteness Theorem</a><ul><li class="chapter" data-level="10.1.1" data-path="lec_09_godel.html"><a href="lec_09_godel.html#godelproofsystemssec"><i class="fa fa-check"></i><b>10.1.1</b> Defining Proof Systems</a></li></ul></li><li class="chapter" data-level="10.2" data-path="lec_09_godel.html"><a href="lec_09_godel.html#gödels-incompleteness-theorem-computational-variant"><i class="fa fa-check"></i><b>10.2</b> Gödel’s Incompleteness Theorem: Computational variant</a></li><li class="chapter" data-level="10.3" data-path="lec_09_godel.html"><a href="lec_09_godel.html#quantified-integer-statements"><i class="fa fa-check"></i><b>10.3</b> Quantified integer statements</a></li><li class="chapter" data-level="10.4" data-path="lec_09_godel.html"><a href="lec_09_godel.html#diophantine-equations-and-the-mrdp-theorem"><i class="fa fa-check"></i><b>10.4</b> Diophantine equations and the MRDP Theorem</a></li><li class="chapter" data-level="10.5" data-path="lec_09_godel.html"><a href="lec_09_godel.html#hardness-of-quantified-integer-statements"><i class="fa fa-check"></i><b>10.5</b> Hardness of quantified integer statements</a><ul><li class="chapter" data-level="10.5.1" data-path="lec_09_godel.html"><a href="lec_09_godel.html#step-1-quantified-mixed-statements-and-computation-histories"><i class="fa fa-check"></i><b>10.5.1</b> Step 1: Quantified mixed statements and computation histories</a></li><li class="chapter" data-level="10.5.2" data-path="lec_09_godel.html"><a href="lec_09_godel.html#step-2-reducing-mixed-statements-to-integer-statements"><i class="fa fa-check"></i><b>10.5.2</b> Step 2: Reducing mixed statements to integer statements</a></li></ul></li><li class="chapter" data-level="10.6" data-path="lec_09_godel.html"><a href="lec_09_godel.html#exercises"><i class="fa fa-check"></i><b>10.6</b> Exercises</a></li><li class="chapter" data-level="10.7" data-path="lec_09_godel.html"><a href="lec_09_godel.html#bibliographical-notes"><i class="fa fa-check"></i><b>10.7</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="11" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html"><i class="fa fa-check"></i><b>11</b> Efficient computation</a><ul><li class="chapter" data-level="11.1" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#problems-on-graphs"><i class="fa fa-check"></i><b>11.1</b> Problems on graphs</a><ul><li class="chapter" data-level="11.1.1" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#finding-the-shortest-path-in-a-graph"><i class="fa fa-check"></i><b>11.1.1</b> Finding the shortest path in a graph</a></li><li class="chapter" data-level="11.1.2" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#finding-the-longest-path-in-a-graph"><i class="fa fa-check"></i><b>11.1.2</b> Finding the longest path in a graph</a></li><li class="chapter" data-level="11.1.3" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#mincutsec"><i class="fa fa-check"></i><b>11.1.3</b> Finding the minimum cut in a graph</a></li><li class="chapter" data-level="11.1.4" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#linerprogsec"><i class="fa fa-check"></i><b>11.1.4</b> Min-Cut Max-Flow and Linear programming</a></li><li class="chapter" data-level="11.1.5" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#finding-the-maximum-cut-in-a-graph"><i class="fa fa-check"></i><b>11.1.5</b> Finding the maximum cut in a graph</a></li><li class="chapter" data-level="11.1.6" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#a-note-on-convexity"><i class="fa fa-check"></i><b>11.1.6</b> A note on convexity</a></li></ul></li><li class="chapter" data-level="11.2" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#beyond-graphs"><i class="fa fa-check"></i><b>11.2</b> Beyond graphs</a><ul><li class="chapter" data-level="11.2.1" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#sat"><i class="fa fa-check"></i><b>11.2.1</b> SAT</a></li><li class="chapter" data-level="11.2.2" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#solving-linear-equations"><i class="fa fa-check"></i><b>11.2.2</b> Solving linear equations</a></li><li class="chapter" data-level="11.2.3" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#solving-quadratic-equations"><i class="fa fa-check"></i><b>11.2.3</b> Solving quadratic equations</a></li></ul></li><li class="chapter" data-level="11.3" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#more-advanced-examples"><i class="fa fa-check"></i><b>11.3</b> More advanced examples</a><ul><li class="chapter" data-level="11.3.1" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#determinant-of-a-matrix"><i class="fa fa-check"></i><b>11.3.1</b> Determinant of a matrix</a></li><li class="chapter" data-level="11.3.2" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#permanent-of-a-matrix"><i class="fa fa-check"></i><b>11.3.2</b> Permanent of a matrix</a></li><li class="chapter" data-level="11.3.3" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#finding-a-zero-sum-equilibrium"><i class="fa fa-check"></i><b>11.3.3</b> Finding a zero-sum equilibrium</a></li><li class="chapter" data-level="11.3.4" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#finding-a-nash-equilibrium"><i class="fa fa-check"></i><b>11.3.4</b> Finding a Nash equilibrium</a></li><li class="chapter" data-level="11.3.5" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#primality-testing"><i class="fa fa-check"></i><b>11.3.5</b> Primality testing</a></li><li class="chapter" data-level="11.3.6" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#integer-factoring"><i class="fa fa-check"></i><b>11.3.6</b> Integer factoring</a></li></ul></li><li class="chapter" data-level="11.4" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#our-current-knowledge"><i class="fa fa-check"></i><b>11.4</b> Our current knowledge</a></li><li class="chapter" data-level="11.5" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#exercises"><i class="fa fa-check"></i><b>11.5</b> Exercises</a></li><li class="chapter" data-level="11.6" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#effalgnotes"><i class="fa fa-check"></i><b>11.6</b> Bibliographical notes</a></li><li class="chapter" data-level="11.7" data-path="lec_10_efficient_alg.html"><a href="lec_10_efficient_alg.html#further-explorations"><i class="fa fa-check"></i><b>11.7</b> Further explorations</a></li></ul></li><li class="chapter" data-level="12" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html"><i class="fa fa-check"></i><b>12</b> Modeling running time</a><ul><li class="chapter" data-level="12.1" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#formally-defining-running-time"><i class="fa fa-check"></i><b>12.1</b> Formally defining running time</a><ul><li class="chapter" data-level="12.1.1" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#polynomial-and-exponential-time"><i class="fa fa-check"></i><b>12.1.1</b> Polynomial and Exponential Time</a></li></ul></li><li class="chapter" data-level="12.2" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#modeling-running-time-using-ram-machines-nand-ram"><i class="fa fa-check"></i><b>12.2</b> Modeling running time using RAM Machines / NAND-RAM</a></li><li class="chapter" data-level="12.3" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#ECTTsec"><i class="fa fa-check"></i><b>12.3</b> Extended Church-Turing Thesis (discussion)</a></li><li class="chapter" data-level="12.4" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#efficient-universal-machine-a-nand-ram-interpreter-in-nand-ram"><i class="fa fa-check"></i><b>12.4</b> Efficient universal machine: a NAND-RAM interpreter in NAND-RAM</a><ul><li class="chapter" data-level="12.4.1" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#timed-universal-turing-machine"><i class="fa fa-check"></i><b>12.4.1</b> Timed Universal Turing Machine</a></li></ul></li><li class="chapter" data-level="12.5" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#the-time-hierarchy-theorem"><i class="fa fa-check"></i><b>12.5</b> The time hierarchy theorem</a></li><li class="chapter" data-level="12.6" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#nonuniformcompsec"><i class="fa fa-check"></i><b>12.6</b> Non uniform computation</a><ul><li class="chapter" data-level="12.6.1" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#obliviousnandtm"><i class="fa fa-check"></i><b>12.6.1</b> Oblivious NAND-TM programs</a></li><li class="chapter" data-level="12.6.2" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#unrollloopsec"><i class="fa fa-check"></i><b>12.6.2</b> Unrolling the loop: algorithmic transformation of Turing Machines to circuits</a></li><li class="chapter" data-level="12.6.3" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#can-uniform-algorithms-simulate-non-uniform-ones"><i class="fa fa-check"></i><b>12.6.3</b> Can uniform algorithms simulate non uniform ones?</a></li><li class="chapter" data-level="12.6.4" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#uniform-vs.-nonuniform-computation-a-recap"><i class="fa fa-check"></i><b>12.6.4</b> Uniform vs. Nonuniform computation: A recap</a></li></ul></li><li class="chapter" data-level="12.7" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#exercises"><i class="fa fa-check"></i><b>12.7</b> Exercises</a></li><li class="chapter" data-level="12.8" data-path="lec_11_running_time.html"><a href="lec_11_running_time.html#bibnotesrunningtime"><i class="fa fa-check"></i><b>12.8</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="13" data-path="lec_12_NP.html"><a href="lec_12_NP.html"><i class="fa fa-check"></i><b>13</b> Polynomial-time reductions</a><ul><li class="chapter" data-level="13.1" data-path="lec_12_NP.html"><a href="lec_12_NP.html#formaldefdecisionexamplessec"><i class="fa fa-check"></i><b>13.1</b> Formal definitions of problems</a></li><li class="chapter" data-level="13.2" data-path="lec_12_NP.html"><a href="lec_12_NP.html#polytimeredsec"><i class="fa fa-check"></i><b>13.2</b> Polynomial-time reductions</a></li><li class="chapter" data-level="13.3" data-path="lec_12_NP.html"><a href="lec_12_NP.html#reducing-3sat-to-zero-one-equations"><i class="fa fa-check"></i><b>13.3</b> Reducing 3SAT to zero one equations</a><ul><li class="chapter" data-level="13.3.1" data-path="lec_12_NP.html"><a href="lec_12_NP.html#quadratic-equations"><i class="fa fa-check"></i><b>13.3.1</b> Quadratic equations</a></li></ul></li><li class="chapter" data-level="13.4" data-path="lec_12_NP.html"><a href="lec_12_NP.html#the-independent-set-problem"><i class="fa fa-check"></i><b>13.4</b> The independent set problem</a></li><li class="chapter" data-level="13.5" data-path="lec_12_NP.html"><a href="lec_12_NP.html#reducing-independent-set-to-maximum-cut"><i class="fa fa-check"></i><b>13.5</b> Reducing Independent Set to Maximum Cut</a></li><li class="chapter" data-level="13.6" data-path="lec_12_NP.html"><a href="lec_12_NP.html#reducing-3sat-to-longest-path"><i class="fa fa-check"></i><b>13.6</b> Reducing 3SAT to Longest Path</a><ul><li class="chapter" data-level="13.6.1" data-path="lec_12_NP.html"><a href="lec_12_NP.html#summary-of-relations"><i class="fa fa-check"></i><b>13.6.1</b> Summary of relations</a></li></ul></li><li class="chapter" data-level="13.7" data-path="lec_12_NP.html"><a href="lec_12_NP.html#exercises"><i class="fa fa-check"></i><b>13.7</b> Exercises</a></li><li class="chapter" data-level="13.8" data-path="lec_12_NP.html"><a href="lec_12_NP.html#reductionsbibnotes"><i class="fa fa-check"></i><b>13.8</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="14" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html"><i class="fa fa-check"></i><b>14</b> NP, NP completeness, and the Cook-Levin Theorem</a><ul><li class="chapter" data-level="14.1" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#the-class-mathbfnp"><i class="fa fa-check"></i><b>14.1</b> The class \mathbf{NP}</a><ul><li class="chapter" data-level="14.1.1" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#examples-of-functions-in-mathbfnp"><i class="fa fa-check"></i><b>14.1.1</b> Examples of functions in \mathbf{NP}</a></li><li class="chapter" data-level="14.1.2" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#basic-facts-about-mathbfnp"><i class="fa fa-check"></i><b>14.1.2</b> Basic facts about \mathbf{NP}</a></li></ul></li><li class="chapter" data-level="14.2" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#from-mathbfnp-to-3sat-the-cook-levin-theorem"><i class="fa fa-check"></i><b>14.2</b> From \mathbf{NP} to 3SAT: The Cook-Levin Theorem</a><ul><li class="chapter" data-level="14.2.1" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#what-does-this-mean"><i class="fa fa-check"></i><b>14.2.1</b> What does this mean?</a></li><li class="chapter" data-level="14.2.2" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#the-cook-levin-theorem-proof-outline"><i class="fa fa-check"></i><b>14.2.2</b> The Cook-Levin Theorem: Proof outline</a></li></ul></li><li class="chapter" data-level="14.3" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#the-nandsat-problem-and-why-it-is-mathbfnp-hard."><i class="fa fa-check"></i><b>14.3</b> The \ensuremath{\mathit{NANDSAT}} Problem, and why it is \mathbf{NP} hard.</a></li><li class="chapter" data-level="14.4" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#the-3nand-problem"><i class="fa fa-check"></i><b>14.4</b> The 3\ensuremath{\mathit{NAND}} problem</a></li><li class="chapter" data-level="14.5" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#from-3nand-to-3sat"><i class="fa fa-check"></i><b>14.5</b> From 3\ensuremath{\mathit{NAND}} to 3\ensuremath{\mathit{SAT}}</a></li><li class="chapter" data-level="14.6" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#wrapping-up"><i class="fa fa-check"></i><b>14.6</b> Wrapping up</a></li><li class="chapter" data-level="14.7" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#exercises"><i class="fa fa-check"></i><b>14.7</b> Exercises</a></li><li class="chapter" data-level="14.8" data-path="lec_13_Cook_Levin.html"><a href="lec_13_Cook_Levin.html#bibliographical-notes"><i class="fa fa-check"></i><b>14.8</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="15" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html"><i class="fa fa-check"></i><b>15</b> What if P equals NP?</a><ul><li class="chapter" data-level="15.1" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#search-to-decision-reduction"><i class="fa fa-check"></i><b>15.1</b> Search-to-decision reduction</a></li><li class="chapter" data-level="15.2" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#optimizationsection"><i class="fa fa-check"></i><b>15.2</b> Optimization</a><ul><li class="chapter" data-level="15.2.1" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#example-supervised-learning"><i class="fa fa-check"></i><b>15.2.1</b> Example: Supervised learning</a></li><li class="chapter" data-level="15.2.2" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#example-breaking-cryptosystems"><i class="fa fa-check"></i><b>15.2.2</b> Example: Breaking cryptosystems</a></li></ul></li><li class="chapter" data-level="15.3" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#finding-mathematical-proofs"><i class="fa fa-check"></i><b>15.3</b> Finding mathematical proofs</a></li><li class="chapter" data-level="15.4" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#quantifier-elimination-advanced"><i class="fa fa-check"></i><b>15.4</b> Quantifier elimination (advanced)</a><ul><li class="chapter" data-level="15.4.1" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#selfimprovingsat"><i class="fa fa-check"></i><b>15.4.1</b> Application: self improving algorithm for 3\ensuremath{\mathit{SAT}}</a></li></ul></li><li class="chapter" data-level="15.5" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#approximating-counting-problems-and-posterior-sampling-advanced-optional"><i class="fa fa-check"></i><b>15.5</b> Approximating counting problems and posterior sampling (advanced, optional)</a></li><li class="chapter" data-level="15.6" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#what-does-all-of-this-imply"><i class="fa fa-check"></i><b>15.6</b> What does all of this imply?</a></li><li class="chapter" data-level="15.7" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#can-mathbfp-neq-mathbfnp-be-neither-true-nor-false"><i class="fa fa-check"></i><b>15.7</b> Can \mathbf{P} \neq \mathbf{NP} be neither true nor false?</a></li><li class="chapter" data-level="15.8" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#is-mathbfpmathbfnp-in-practice"><i class="fa fa-check"></i><b>15.8</b> Is \mathbf{P}=\mathbf{NP} in practice?</a></li><li class="chapter" data-level="15.9" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#what-if-mathbfp-neq-mathbfnp"><i class="fa fa-check"></i><b>15.9</b> What if \mathbf{P} \neq \mathbf{NP}?</a></li><li class="chapter" data-level="15.10" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#exercises"><i class="fa fa-check"></i><b>15.10</b> Exercises</a></li><li class="chapter" data-level="15.11" data-path="lec_14_PvsNP.html"><a href="lec_14_PvsNP.html#bibliographical-notes"><i class="fa fa-check"></i><b>15.11</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="16" data-path="lec_14a_space_complexity.html"><a href="lec_14a_space_complexity.html"><i class="fa fa-check"></i><b>16</b> Space bounded computation</a><ul><li class="chapter" data-level="16.1" data-path="lec_14a_space_complexity.html"><a href="lec_14a_space_complexity.html#exercises"><i class="fa fa-check"></i><b>16.1</b> Exercises</a></li><li class="chapter" data-level="16.2" data-path="lec_14a_space_complexity.html"><a href="lec_14a_space_complexity.html#bibliographical-notes"><i class="fa fa-check"></i><b>16.2</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="17" data-path="lec_15_probability.html"><a href="lec_15_probability.html"><i class="fa fa-check"></i><b>17</b> Probability Theory 101</a><ul><li class="chapter" data-level="17.1" data-path="lec_15_probability.html"><a href="lec_15_probability.html#random-coins"><i class="fa fa-check"></i><b>17.1</b> Random coins</a><ul><li class="chapter" data-level="17.1.1" data-path="lec_15_probability.html"><a href="lec_15_probability.html#random-variables"><i class="fa fa-check"></i><b>17.1.1</b> Random variables</a></li><li class="chapter" data-level="17.1.2" data-path="lec_15_probability.html"><a href="lec_15_probability.html#distributions-over-strings"><i class="fa fa-check"></i><b>17.1.2</b> Distributions over strings</a></li><li class="chapter" data-level="17.1.3" data-path="lec_15_probability.html"><a href="lec_15_probability.html#more-general-sample-spaces."><i class="fa fa-check"></i><b>17.1.3</b> More general sample spaces.</a></li></ul></li><li class="chapter" data-level="17.2" data-path="lec_15_probability.html"><a href="lec_15_probability.html#correlations-and-independence"><i class="fa fa-check"></i><b>17.2</b> Correlations and independence</a><ul><li class="chapter" data-level="17.2.1" data-path="lec_15_probability.html"><a href="lec_15_probability.html#independent-random-variables"><i class="fa fa-check"></i><b>17.2.1</b> Independent random variables</a></li><li class="chapter" data-level="17.2.2" data-path="lec_15_probability.html"><a href="lec_15_probability.html#collections-of-independent-random-variables."><i class="fa fa-check"></i><b>17.2.2</b> Collections of independent random variables.</a></li></ul></li><li class="chapter" data-level="17.3" data-path="lec_15_probability.html"><a href="lec_15_probability.html#concentration-and-tail-bounds"><i class="fa fa-check"></i><b>17.3</b> Concentration and tail bounds</a><ul><li class="chapter" data-level="17.3.1" data-path="lec_15_probability.html"><a href="lec_15_probability.html#chebyshevs-inequality"><i class="fa fa-check"></i><b>17.3.1</b> Chebyshev’s Inequality</a></li><li class="chapter" data-level="17.3.2" data-path="lec_15_probability.html"><a href="lec_15_probability.html#the-chernoff-bound"><i class="fa fa-check"></i><b>17.3.2</b> The Chernoff bound</a></li></ul></li><li class="chapter" data-level="17.4" data-path="lec_15_probability.html"><a href="lec_15_probability.html#exercises"><i class="fa fa-check"></i><b>17.4</b> Exercises</a></li><li class="chapter" data-level="17.5" data-path="lec_15_probability.html"><a href="lec_15_probability.html#bibliographical-notes"><i class="fa fa-check"></i><b>17.5</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="18" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html"><i class="fa fa-check"></i><b>18</b> Probabilistic computation</a><ul><li class="chapter" data-level="18.1" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html#finding-approximately-good-maximum-cuts."><i class="fa fa-check"></i><b>18.1</b> Finding approximately good maximum cuts.</a><ul><li class="chapter" data-level="18.1.1" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html#amplifying-the-success-of-randomized-algorithms"><i class="fa fa-check"></i><b>18.1.1</b> Amplifying the success of randomized algorithms</a></li><li class="chapter" data-level="18.1.2" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html#success-amplification"><i class="fa fa-check"></i><b>18.1.2</b> Success amplification</a></li><li class="chapter" data-level="18.1.3" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html#two-sided-amplification"><i class="fa fa-check"></i><b>18.1.3</b> Two-sided amplification</a></li><li class="chapter" data-level="18.1.4" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html#what-does-this-mean"><i class="fa fa-check"></i><b>18.1.4</b> What does this mean?</a></li><li class="chapter" data-level="18.1.5" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html#solving-sat-through-randomization"><i class="fa fa-check"></i><b>18.1.5</b> Solving SAT through randomization</a></li><li class="chapter" data-level="18.1.6" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html#bipartite-matching."><i class="fa fa-check"></i><b>18.1.6</b> Bipartite matching.</a></li></ul></li><li class="chapter" data-level="18.2" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html#exercises"><i class="fa fa-check"></i><b>18.2</b> Exercises</a></li><li class="chapter" data-level="18.3" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html#bibliographical-notes"><i class="fa fa-check"></i><b>18.3</b> Bibliographical notes</a></li><li class="chapter" data-level="18.4" data-path="lec_16_randomized_alg.html"><a href="lec_16_randomized_alg.html#acknowledgements"><i class="fa fa-check"></i><b>18.4</b> Acknowledgements</a></li></ul></li><li class="chapter" data-level="19" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html"><i class="fa fa-check"></i><b>19</b> Modeling randomized computation</a><ul><li class="chapter" data-level="19.1" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#modeling-randomized-computation"><i class="fa fa-check"></i><b>19.1</b> Modeling randomized computation</a><ul><li class="chapter" data-level="19.1.1" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#an-alternative-view-random-coins-as-an-extra-input"><i class="fa fa-check"></i><b>19.1.1</b> An alternative view: random coins as an extra input</a></li><li class="chapter" data-level="19.1.2" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#successamptwosided"><i class="fa fa-check"></i><b>19.1.2</b> Success amplification of two-sided error algorithms</a></li></ul></li><li class="chapter" data-level="19.2" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#mathbfbpp-and-mathbfnp-completeness"><i class="fa fa-check"></i><b>19.2</b> \mathbf{BPP} and \mathbf{NP} completeness</a></li><li class="chapter" data-level="19.3" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#the-power-of-randomization"><i class="fa fa-check"></i><b>19.3</b> The power of randomization</a><ul><li class="chapter" data-level="19.3.1" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#solving-mathbfbpp-in-exponential-time"><i class="fa fa-check"></i><b>19.3.1</b> Solving \mathbf{BPP} in exponential time</a></li><li class="chapter" data-level="19.3.2" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#simulating-randomized-algorithms-by-circuits"><i class="fa fa-check"></i><b>19.3.2</b> Simulating randomized algorithms by circuits</a></li></ul></li><li class="chapter" data-level="19.4" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#derandomization"><i class="fa fa-check"></i><b>19.4</b> Derandomization</a><ul><li class="chapter" data-level="19.4.1" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#pseudorandom-generators"><i class="fa fa-check"></i><b>19.4.1</b> Pseudorandom generators</a></li><li class="chapter" data-level="19.4.2" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#optimalprgconj"><i class="fa fa-check"></i><b>19.4.2</b> From existence to constructivity</a></li><li class="chapter" data-level="19.4.3" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#usefulness-of-pseudorandom-generators"><i class="fa fa-check"></i><b>19.4.3</b> Usefulness of pseudorandom generators</a></li></ul></li><li class="chapter" data-level="19.5" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#mathbfpmathbfnp-and-mathbfbpp-vs-mathbfp"><i class="fa fa-check"></i><b>19.5</b> \mathbf{P}=\mathbf{NP} and \mathbf{BPP} vs \mathbf{P}</a></li><li class="chapter" data-level="19.6" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#non-constructive-existence-of-pseudorandom-generators-advanced-optional"><i class="fa fa-check"></i><b>19.6</b> Non-constructive existence of pseudorandom generators (advanced, optional)</a></li><li class="chapter" data-level="19.7" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#exercises"><i class="fa fa-check"></i><b>19.7</b> Exercises</a></li><li class="chapter" data-level="19.8" data-path="lec_17_model_rand.html"><a href="lec_17_model_rand.html#modelrandbibnotes"><i class="fa fa-check"></i><b>19.8</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="20" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html"><i class="fa fa-check"></i><b>20</b> Cryptography</a><ul><li class="chapter" data-level="20.1" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#classical-cryptosystems"><i class="fa fa-check"></i><b>20.1</b> Classical cryptosystems</a></li><li class="chapter" data-level="20.2" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#defining-encryption"><i class="fa fa-check"></i><b>20.2</b> Defining encryption</a></li><li class="chapter" data-level="20.3" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#defining-security-of-encryption"><i class="fa fa-check"></i><b>20.3</b> Defining security of encryption</a></li><li class="chapter" data-level="20.4" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#perfect-secrecy"><i class="fa fa-check"></i><b>20.4</b> Perfect secrecy</a><ul><li class="chapter" data-level="20.4.1" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#example-perfect-secrecy-in-the-battlefield"><i class="fa fa-check"></i><b>20.4.1</b> Example: Perfect secrecy in the battlefield</a></li><li class="chapter" data-level="20.4.2" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#constructing-perfectly-secret-encryption"><i class="fa fa-check"></i><b>20.4.2</b> Constructing perfectly secret encryption</a></li></ul></li><li class="chapter" data-level="20.5" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#necessity-of-long-keys"><i class="fa fa-check"></i><b>20.5</b> Necessity of long keys</a></li><li class="chapter" data-level="20.6" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#computational-secrecy"><i class="fa fa-check"></i><b>20.6</b> Computational secrecy</a><ul><li class="chapter" data-level="20.6.1" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#stream-ciphers-or-the-derandomized-one-time-pad"><i class="fa fa-check"></i><b>20.6.1</b> Stream ciphers or the derandomized one-time pad</a></li></ul></li><li class="chapter" data-level="20.7" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#computational-secrecy-and-mathbfnp"><i class="fa fa-check"></i><b>20.7</b> Computational secrecy and \mathbf{NP}</a></li><li class="chapter" data-level="20.8" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#public-key-cryptography"><i class="fa fa-check"></i><b>20.8</b> Public key cryptography</a><ul><li class="chapter" data-level="20.8.1" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#defining-public-key-encryption"><i class="fa fa-check"></i><b>20.8.1</b> Defining public key encryption</a></li><li class="chapter" data-level="20.8.2" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#diffie-hellman-key-exchange"><i class="fa fa-check"></i><b>20.8.2</b> Diffie-Hellman key exchange</a></li></ul></li><li class="chapter" data-level="20.9" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#other-security-notions"><i class="fa fa-check"></i><b>20.9</b> Other security notions</a></li><li class="chapter" data-level="20.10" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#magic"><i class="fa fa-check"></i><b>20.10</b> Magic</a><ul><li class="chapter" data-level="20.10.1" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#zero-knowledge-proofs"><i class="fa fa-check"></i><b>20.10.1</b> Zero knowledge proofs</a></li><li class="chapter" data-level="20.10.2" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#fully-homomorphic-encryption"><i class="fa fa-check"></i><b>20.10.2</b> Fully homomorphic encryption</a></li><li class="chapter" data-level="20.10.3" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#multiparty-secure-computation"><i class="fa fa-check"></i><b>20.10.3</b> Multiparty secure computation</a></li></ul></li><li class="chapter" data-level="20.11" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#exercises"><i class="fa fa-check"></i><b>20.11</b> Exercises</a></li><li class="chapter" data-level="20.12" data-path="lec_19_cryptography.html"><a href="lec_19_cryptography.html#bibliographical-notes"><i class="fa fa-check"></i><b>20.12</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="21" data-path="lec_24_proofs.html"><a href="lec_24_proofs.html"><i class="fa fa-check"></i><b>21</b> Proofs and algorithms</a><ul><li class="chapter" data-level="21.1" data-path="lec_24_proofs.html"><a href="lec_24_proofs.html#exercises"><i class="fa fa-check"></i><b>21.1</b> Exercises</a></li><li class="chapter" data-level="21.2" data-path="lec_24_proofs.html"><a href="lec_24_proofs.html#bibliographical-notes"><i class="fa fa-check"></i><b>21.2</b> Bibliographical notes</a></li></ul></li><li class="chapter" data-level="22" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html"><i class="fa fa-check"></i><b>22</b> Quantum computing</a><ul><li class="chapter" data-level="22.1" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#the-double-slit-experiment"><i class="fa fa-check"></i><b>22.1</b> The double slit experiment</a></li><li class="chapter" data-level="22.2" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantum-amplitudes"><i class="fa fa-check"></i><b>22.2</b> Quantum amplitudes</a><ul><li class="chapter" data-level="22.2.1" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#linear-algebra-quick-review"><i class="fa fa-check"></i><b>22.2.1</b> Linear algebra quick review</a></li></ul></li><li class="chapter" data-level="22.3" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#bellineqsec"><i class="fa fa-check"></i><b>22.3</b> Bell’s Inequality</a></li><li class="chapter" data-level="22.4" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantum-weirdness"><i class="fa fa-check"></i><b>22.4</b> Quantum weirdness</a></li><li class="chapter" data-level="22.5" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantum-computing-and-computation---an-executive-summary."><i class="fa fa-check"></i><b>22.5</b> Quantum computing and computation - an executive summary.</a></li><li class="chapter" data-level="22.6" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantum-systems"><i class="fa fa-check"></i><b>22.6</b> Quantum systems</a><ul><li class="chapter" data-level="22.6.1" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantum-amplitudes-1"><i class="fa fa-check"></i><b>22.6.1</b> Quantum amplitudes</a></li><li class="chapter" data-level="22.6.2" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantum-systems-an-executive-summary"><i class="fa fa-check"></i><b>22.6.2</b> Quantum systems: an executive summary</a></li></ul></li><li class="chapter" data-level="22.7" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#analysis-of-bells-inequality-optional"><i class="fa fa-check"></i><b>22.7</b> Analysis of Bell’s Inequality (optional)</a></li><li class="chapter" data-level="22.8" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantum-computation"><i class="fa fa-check"></i><b>22.8</b> Quantum computation</a><ul><li class="chapter" data-level="22.8.1" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantum-circuits"><i class="fa fa-check"></i><b>22.8.1</b> Quantum circuits</a></li><li class="chapter" data-level="22.8.2" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#qnand-circ-programs-optional"><i class="fa fa-check"></i><b>22.8.2</b> QNAND-CIRC programs (optional)</a></li><li class="chapter" data-level="22.8.3" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#uniform-computation"><i class="fa fa-check"></i><b>22.8.3</b> Uniform computation</a></li></ul></li><li class="chapter" data-level="22.9" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#physically-realizing-quantum-computation"><i class="fa fa-check"></i><b>22.9</b> Physically realizing quantum computation</a></li><li class="chapter" data-level="22.10" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#shors-algorithm-hearing-the-shape-of-prime-factors"><i class="fa fa-check"></i><b>22.10</b> Shor’s Algorithm: Hearing the shape of prime factors</a><ul><li class="chapter" data-level="22.10.1" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#period-finding"><i class="fa fa-check"></i><b>22.10.1</b> Period finding</a></li><li class="chapter" data-level="22.10.2" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#shors-algorithm-a-birds-eye-view"><i class="fa fa-check"></i><b>22.10.2</b> Shor’s Algorithm: A bird’s eye view</a></li></ul></li><li class="chapter" data-level="22.11" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantum-fourier-transform-advanced-optional"><i class="fa fa-check"></i><b>22.11</b> Quantum Fourier Transform (advanced, optional)</a><ul><li class="chapter" data-level="22.11.1" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantum-fourier-transform-over-the-boolean-cube-simons-algorithm"><i class="fa fa-check"></i><b>22.11.1</b> Quantum Fourier Transform over the Boolean Cube: Simon’s Algorithm</a></li><li class="chapter" data-level="22.11.2" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#from-fourier-to-period-finding-simons-algorithm-advanced-optional"><i class="fa fa-check"></i><b>22.11.2</b> From Fourier to Period finding: Simon’s Algorithm (advanced, optional)</a></li><li class="chapter" data-level="22.11.3" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#from-simon-to-shor-advanced-optional"><i class="fa fa-check"></i><b>22.11.3</b> From Simon to Shor (advanced, optional)</a></li></ul></li><li class="chapter" data-level="22.12" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#exercises"><i class="fa fa-check"></i><b>22.12</b> Exercises</a></li><li class="chapter" data-level="22.13" data-path="lec_26_quantum_computing.html"><a href="lec_26_quantum_computing.html#quantumbibnotessec"><i class="fa fa-check"></i><b>22.13</b> Bibliographical notes</a></li></ul></li><li class="divider"></li>
</ul>
<!--bookdown:toc2:end-->
      </nav>
    </div>

    <div class="book-header" role="navigation">
      <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Probability theory 101</a>
      </h1>
    </div>

    <div class="book-body">
      <div class="body-inner">


        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<!--bookdown:toc:end-->
<!--bookdown:body:start-->

<div  class="section level2">

<!-- link to pdf version -->


<!-- start of header referring to comments -->
<div><p></p><p style="color:#871640;"><i class="fas fa-wrench"></i> See any bugs/typos/confusing explanations? <a href="https://github.com/boazbk/tcs/issues/new">Open a GitHub issue</a>. You can also <a href="#commentform">comment below</a> <i class="fas fa-wrench"></i></p></div>



<div><p style="color:#871640;">&#x2605; See also the <a id="pdflink" href='https://files.boazbarak.org/introtcs/lec_15_probability.pdf'><b>PDF version of this chapter</b></a> (better formatting/references) &#x2605;</p></div>

<!-- end of header referring to comments -->

<!--- start of actual content -->

<h1 id="probabilitychap" data-number="17">Probability Theory 101</h1>
<div id="section" class="objectives" name="Objectives">
<ul>
<li>Review the basic notion of probability theory that we will use.<br />
</li>
<li>Sample spaces, and in particular the space <span><span class="math inline">\(\{0,1\}^n\)</span></span><br />
</li>
<li>Events, probabilities of unions and intersections.<br />
</li>
<li>Random variables and their expectation, variance, and standard deviation.<br />
</li>
<li>Independence and correlation for both events and random variables.<br />
</li>
<li>Markov, Chebyshev and Chernoff tail bounds (bounding the probability that a random variable will deviate from its expectation).</li>
</ul>
</div>
<blockquote>
<p><em>“God doesn’t play dice with the universe”</em>, Albert Einstein</p>
</blockquote>
<blockquote>
<p><em>“Einstein was doubly wrong … not only does God definitely play dice, but He sometimes confuses us by throwing them where they can’t be seen.”</em>, Stephen Hawking</p>
</blockquote>
<blockquote>
<p><em>“‘The probability of winning a battle has no place in our theory because it does not belong to any [random experiment]. Probability cannot be applied to this problem any more than the physical concept of work can be applied to the ’work’ done by an actor reciting his part.”</em>, Richard Von Mises, 1928 (paraphrased)</p>
</blockquote>
<blockquote>
<p><em>“I am unable to see why ‘objectivity’ requires us to interpret every probability as a frequency in some random experiment; particularly when in most problems probabilities are frequencies only in an imaginary universe invented just for the purpose of allowing a frequency interpretation.”</em>, E.T. Jaynes, 1976</p>
</blockquote>
<p>Before we show how to use randomness in algorithms, let us do a quick review of some basic notions in probability theory. This is not meant to replace a course on probability theory, and if you have not seen this material before, I highly recommend you look at additional resources to get up to speed. Fortunately, we will not need many of the advanced notions of probability theory, but, as we will see, even the so-called “simple” setting of tossing <span><span class="math inline">\(n\)</span></span> coins can lead to very subtle and interesting issues.</p>
<h2 id="random-coins" data-number="17.1">Random coins</h2>
<p>The nature of randomness and probability is a topic of great philosophical, scientific and mathematical depth. Is there actual randomness in the world, or does it proceed in a deterministic clockwork fashion from some initial conditions set at the beginning of time? Does probability refer to our uncertainty of beliefs, or to the frequency of occurrences in repeated experiments? How can we define probability over infinite sets?</p>
<p>These are all important questions that have been studied and debated by scientists, mathematicians, statisticians and philosophers. Fortunately, we will not need to deal directly with these questions here. We will be mostly interested in the setting of tossing <span><span class="math inline">\(n\)</span></span> random, unbiased and independent coins. Below we define the basic probabilistic objects of <em>events</em> and <em>random variables</em> when restricted to this setting. These can be defined for much more general probabilistic experiments or <em>sample spaces</em>, and later on we will briefly discuss how this can be done. However, the <span><span class="math inline">\(n\)</span></span>-coin case is sufficient for almost everything we’ll need in this course.</p>
<p>If instead of “heads” and “tails” we encode the sides of each coin by “zero” and “one”, we can encode the result of tossing <span><span class="math inline">\(n\)</span></span> coins as a string in <span><span class="math inline">\(\{0,1\}^n\)</span></span>. Each particular outcome <span><span class="math inline">\(x\in \{0,1\}^n\)</span></span> is obtained with probability <span><span class="math inline">\(2^{-n}\)</span></span>. For example, if we toss three coins, then we obtain each of the 8 outcomes <span><span class="math inline">\(000,001,010,011,100,101,110,111\)</span></span> with probability <span><span class="math inline">\(2^{-3}=1/8\)</span></span> (see also <a href='#coinexperimentfig'>Figure 17.1</a>). We can describe the experiment of tossing <span><span class="math inline">\(n\)</span></span> coins as choosing a string <span><span class="math inline">\(x\)</span></span> uniformly at random from <span><span class="math inline">\(\{0,1\}^n\)</span></span>, and hence we’ll use the shorthand <span><span class="math inline">\(x\sim \{0,1\}^n\)</span></span> for <span><span class="math inline">\(x\)</span></span> that is chosen according to this experiment.</p>
<figure>
<img src="../figure/coinexperiment.png" alt="17.1: The probabilistic experiment of tossing three coins corresponds to making 2\times 2 \times 2 = 8 choices, each with equal probability. In this example, the blue set corresponds to the event A = \{ x\in \{0,1\}^3 \;|\; x_0 = 0 \} where the first coin toss is equal to 0, and the pink set corresponds to the event B = \{ x\in \{0,1\}^3 \;|\; x_1 = 1 \} where the second coin toss is equal to 1 (with their intersection having a purplish color). As we can see, each of these events contains 4 elements (out of 8 total) and so has probability 1/2. The intersection of A and B contains two elements, and so the probability that both of these events occur is 2/8 = 1/4." id="coinexperimentfig" class="margin" /><figcaption>17.1: The probabilistic experiment of tossing three coins corresponds to making <span><span class="math inline">\(2\times 2 \times 2 = 8\)</span></span> choices, each with equal probability. In this example, the blue set corresponds to the event <span><span class="math inline">\(A = \{ x\in \{0,1\}^3 \;|\; x_0 = 0 \}\)</span></span> where the first coin toss is equal to <span><span class="math inline">\(0\)</span></span>, and the pink set corresponds to the event <span><span class="math inline">\(B = \{ x\in \{0,1\}^3 \;|\; x_1 = 1 \}\)</span></span> where the second coin toss is equal to <span><span class="math inline">\(1\)</span></span> (with their intersection having a purplish color). As we can see, each of these events contains <span><span class="math inline">\(4\)</span></span> elements (out of <span><span class="math inline">\(8\)</span></span> total) and so has probability <span><span class="math inline">\(1/2\)</span></span>. The intersection of <span><span class="math inline">\(A\)</span></span> and <span><span class="math inline">\(B\)</span></span> contains two elements, and so the probability that both of these events occur is <span><span class="math inline">\(2/8 = 1/4\)</span></span>.</figcaption>
</figure>
<p>An <em>event</em> is simply a subset <span><span class="math inline">\(A\)</span></span> of <span><span class="math inline">\(\{0,1\}^n\)</span></span>. The <em>probability of <span><span class="math inline">\(A\)</span></span></em>, denoted by <span><span class="math inline">\(\Pr_{x\sim \{0,1\}^n}[A]\)</span></span> (or <span><span class="math inline">\(\Pr[A]\)</span></span> for short, when the sample space is understood from the context), is the probability that an <span><span class="math inline">\(x\)</span></span> chosen uniformly at random will be contained in <span><span class="math inline">\(A\)</span></span>. Note that this is the same as <span><span class="math inline">\(|A|/2^n\)</span></span> (where <span><span class="math inline">\(|A|\)</span></span> as usual denotes the number of elements in the set <span><span class="math inline">\(A\)</span></span>). For example, the probability that <span><span class="math inline">\(x\)</span></span> has an even number of ones is <span><span class="math inline">\(\Pr[A]\)</span></span> where <span><span class="math inline">\(A=\{ x : \sum_{i=0}^{n-1} x_i \;= 0 \mod 2 \}\)</span></span>. In the case <span><span class="math inline">\(n=3\)</span></span>, <span><span class="math inline">\(A=\{ 000,011,101,110 \}\)</span></span>, and hence <span><span class="math inline">\(\Pr[A]=\tfrac{4}{8}=\tfrac{1}{2}\)</span></span> (see <a href='#eventhreecoinsfig'>Figure 17.2</a>). It turns out this is true for every <span><span class="math inline">\(n\)</span></span>:</p>
<figure>
<img src="../figure/even3coins.png" alt="17.2: The event that if we toss three coins x_0,x_1,x_2 \in \{0,1\} then the sum of the x_i’s is even has probability 1/2 since it corresponds to exactly 4 out of the 8 possible strings of length 3." id="eventhreecoinsfig" class="margin" /><figcaption>17.2: The event that if we toss three coins <span><span class="math inline">\(x_0,x_1,x_2 \in \{0,1\}\)</span></span> then the sum of the <span><span class="math inline">\(x_i\)</span></span>’s is even has probability <span><span class="math inline">\(1/2\)</span></span> since it corresponds to exactly <span><span class="math inline">\(4\)</span></span> out of the <span><span class="math inline">\(8\)</span></span> possible strings of length <span><span class="math inline">\(3\)</span></span>.</figcaption>
</figure>
<div id="evenprob" class="lemma" name="Lemma 17.1">
<p>For every <span><span class="math inline">\(n&gt;0\)</span></span>, <span>
<div class='myequationbox'><span class="math display">\[\Pr_{x\sim \{0,1\}^n}[ \text{$\sum_{i=0}^{n-1} x_i$ is even }] = 1/2\]</span></div></span></p>
</div>
<div id="section-1" class="pause" name="Pause">
<p>To test your intuition on probability, try to stop here and prove the lemma on your own.</p>
</div>
<div class="proof" data-ref="evenprob" name="Proof 17.1">
<p>We prove the lemma by induction on <span><span class="math inline">\(n\)</span></span>. For the case <span><span class="math inline">\(n=1\)</span></span> it is clear since <span><span class="math inline">\(x=0\)</span></span> is even and <span><span class="math inline">\(x=1\)</span></span> is odd, and hence the probability that <span><span class="math inline">\(x\in \{0,1\}\)</span></span> is even is <span><span class="math inline">\(1/2\)</span></span>. Let <span><span class="math inline">\(n&gt;1\)</span></span>. We assume by induction that the lemma is true for <span><span class="math inline">\(n-1\)</span></span> and we will prove it for <span><span class="math inline">\(n\)</span></span>. We split the set <span><span class="math inline">\(\{0,1\}^n\)</span></span> into four disjoint sets <span><span class="math inline">\(E_0,E_1,O_0,O_1\)</span></span>, where for <span><span class="math inline">\(b\in \{0,1\}\)</span></span>, <span><span class="math inline">\(E_b\)</span></span> is defined as the set of <span><span class="math inline">\(x\in \{0,1\}^n\)</span></span> such that <span><span class="math inline">\(x_0\cdots x_{n-2}\)</span></span> has even number of ones and <span><span class="math inline">\(x_{n-1}=b\)</span></span> and similarly <span><span class="math inline">\(O_b\)</span></span> is the set of <span><span class="math inline">\(x\in \{0,1\}^n\)</span></span> such that <span><span class="math inline">\(x_0 \cdots x_{n-2}\)</span></span> has odd number of ones and <span><span class="math inline">\(x_{n-1}=b\)</span></span>. Since <span><span class="math inline">\(E_0\)</span></span> is obtained by simply extending <span><span class="math inline">\(n-1\)</span></span>-length string with even number of ones by the digit <span><span class="math inline">\(0\)</span></span>, the size of <span><span class="math inline">\(E_0\)</span></span> is simply the number of such <span><span class="math inline">\(n-1\)</span></span>-length strings which by the induction hypothesis is <span><span class="math inline">\(2^{n-1}/2 = 2^{n-2}\)</span></span>. The same reasoning applies for <span><span class="math inline">\(E_1\)</span></span>, <span><span class="math inline">\(O_0\)</span></span>, and <span><span class="math inline">\(O_1\)</span></span>. Hence each one of the four sets <span><span class="math inline">\(E_0,E_1,O_0,O_1\)</span></span> is of size <span><span class="math inline">\(2^{n-2}\)</span></span>. Since <span><span class="math inline">\(x\in \{0,1\}^n\)</span></span> has an even number of ones if and only if <span><span class="math inline">\(x \in E_0 \cup O_1\)</span></span> (i.e., either the first <span><span class="math inline">\(n-1\)</span></span> coordinates sum up to an even number and the final coordinate is <span><span class="math inline">\(0\)</span></span> or the first <span><span class="math inline">\(n-1\)</span></span> coordinates sum up to an odd number and the final coordinate is <span><span class="math inline">\(1\)</span></span>), we get that the probability that <span><span class="math inline">\(x\)</span></span> satisfies this property is <span>
<div class='myequationbox'><span class="math display">\[
\tfrac{|E_0\cup O_1|}{2^n} = \frac{2^{n-2}+2^{n-2}}{2^n} = \frac{1}{2} \;,
\]</span></div></span> using the fact that <span><span class="math inline">\(E_0\)</span></span> and <span><span class="math inline">\(O_1\)</span></span> are disjoint and hence <span><span class="math inline">\(|E_0 \cup O_1| = |E_0|+|O_1|\)</span></span>.</p>
</div>
<p>We can also use the <em>intersection</em> (<span><span class="math inline">\(\cap\)</span></span>) and <em>union</em> (<span><span class="math inline">\(\cup\)</span></span>) operators to talk about the probability of both event <span><span class="math inline">\(A\)</span></span> <em>and</em> event <span><span class="math inline">\(B\)</span></span> happening, or the probability of event <span><span class="math inline">\(A\)</span></span> <em>or</em> event <span><span class="math inline">\(B\)</span></span> happening. For example, the probability <span><span class="math inline">\(p\)</span></span> that <span><span class="math inline">\(x\)</span></span> has an <em>even</em> number of ones <em>and</em> <span><span class="math inline">\(x_0=1\)</span></span> is the same as <span><span class="math inline">\(\Pr[A\cap B]\)</span></span> where <span><span class="math inline">\(A=\{ x\in \{0,1\}^n : \sum_{i=0}^{n-1} x_i =0 \mod 2 \}\)</span></span> and <span><span class="math inline">\(B=\{ x\in \{0,1\}^n : x_0 = 1 \}\)</span></span>. This probability is equal to <span><span class="math inline">\(1/4\)</span></span> for <span><span class="math inline">\(n &gt; 1\)</span></span>. (It is a great exercise for you to pause here and verify that you understand why this is the case.)</p>
<p>Because intersection corresponds to considering the logical AND of the conditions that two events happen, while union corresponds to considering the logical OR, we will sometimes use the <span><span class="math inline">\(\wedge\)</span></span> and <span><span class="math inline">\(\vee\)</span></span> operators instead of <span><span class="math inline">\(\cap\)</span></span> and <span><span class="math inline">\(\cup\)</span></span>, and so write this probability <span><span class="math inline">\(p=\Pr[A \cap B]\)</span></span> defined above also as <span>
<div class='myequationbox'><span class="math display">\[
\Pr_{x\sim \{0,1\}^n} \left[ \sum_i x_i =0 \mod 2 \; \wedge \; x_0 = 1 \right] \;.
\]</span></div></span></p>
<p>If <span><span class="math inline">\(A \subseteq \{0,1\}^n\)</span></span> is an event, then <span><span class="math inline">\(\overline{A} = \{0,1\}^n \setminus A\)</span></span> corresponds to the event that <span><span class="math inline">\(A\)</span></span> does <em>not</em> happen. Since <span><span class="math inline">\(|\overline{A}|=2^n-|A|\)</span></span>, we get that <span>
<div class='myequationbox'><span class="math display">\[\Pr[\overline{A}] = \tfrac{|\overline{A}|}{2^n} = \tfrac{2^n-|A|}{2^n}=1-\tfrac{|A|}{2^n} = 1- \Pr[A]
\]</span></div></span> This makes sense: since <span><span class="math inline">\(A\)</span></span> happens if and only if <span><span class="math inline">\(\overline{A}\)</span></span> does <em>not</em> happen, the probability of <span><span class="math inline">\(\overline{A}\)</span></span> should be one minus the probability of <span><span class="math inline">\(A\)</span></span>.</p>
<div id="samplespace" class="remark" title="Remember the sample space" name="Remark 17.2 (Remember the sample space) ">
<p>While the above definition might seem very simple and almost trivial, the human mind seems not to have evolved for probabilistic reasoning, and it is surprising how often people can get even the simplest settings of probability wrong. One way to make sure you don’t get confused when trying to calculate probability statements is to always ask yourself the following two questions: <strong>(1)</strong> Do I understand what is the <strong>sample space</strong> that this probability is taken over?, and <strong>(2)</strong> Do I understand what is the definition of the <strong>event</strong> that we are analyzing?.</p>
<p>For example, suppose that I were to randomize seating in my course, and then it turned out that students sitting in row 7 performed better on the final: how surprising should we find this? If we started out with the hypothesis that there is something special about the number 7 and chose it ahead of time, then the event that we are discussing is the event <span><span class="math inline">\(A\)</span></span> that students sitting in number 7 had better performance on the final, and we might find it surprising. However, if we first looked at the results and then chose the row whose average performance is best, then the event we are discussing is the event <span><span class="math inline">\(B\)</span></span> that there exists <em>some</em> row where the performance is higher than the overall average. <span><span class="math inline">\(B\)</span></span> is a superset of <span><span class="math inline">\(A\)</span></span>, and its probability (even if there is no correlation between sitting and performance) can be quite significant.</p>
</div>
<h3 id="random-variables" data-number="17.1.1">Random variables</h3>
<p><em>Events</em> correspond to Yes/No questions, but often we want to analyze finer questions. For example, if we make a bet at the roulette wheel, we don’t want to just analyze whether we won or lost, but also <em>how much</em> we’ve gained. A (real valued) <em>random variable</em> is simply a way to associate a number with the result of a probabilistic experiment. Formally, a random variable is a function <span><span class="math inline">\(X:\{0,1\}^n \rightarrow \R\)</span></span> that maps every outcome <span><span class="math inline">\(x\in \{0,1\}^n\)</span></span> to an element <span><span class="math inline">\(X(x) \in \R\)</span></span>. For example, the function <span><span class="math inline">\(\ensuremath{\mathit{SUM}}:\{0,1\}^n \rightarrow \R\)</span></span> that maps <span><span class="math inline">\(x\)</span></span> to the sum of its coordinates (i.e., to <span><span class="math inline">\(\sum_{i=0}^{n-1} x_i\)</span></span>) is a random variable.</p>
<p>The <em>expectation</em> of a random variable <span><span class="math inline">\(X\)</span></span>, denoted by <span><span class="math inline">\(\E[X]\)</span></span>, is the average value that that this number takes, taken over all draws from the probabilistic experiment. In other words, the expectation of <span><span class="math inline">\(X\)</span></span> is defined as follows: <span>
<div class='myequationbox'><span class="math display">\[
\E[X] = \sum_{x\in \{0,1\}^n} 2^{-n}X(x) \;.
\]</span></div></span></p>
<p>If <span><span class="math inline">\(X\)</span></span> and <span><span class="math inline">\(Y\)</span></span> are random variables, then we can define <span><span class="math inline">\(X+Y\)</span></span> as simply the random variable that maps a point <span><span class="math inline">\(x\in \{0,1\}^n\)</span></span> to <span><span class="math inline">\(X(x)+Y(x)\)</span></span>. One basic and very useful property of the expectation is that it is <em>linear</em>:</p>
<div id="linearityexp" class="lemma" title="Linearity of expectation" name="Lemma 17.3 (Linearity of expectation) ">
<p><span>
<div class='myequationbox'><span class="math display">\[ \E[ X+Y ] = \E[X] + \E[Y] \]</span></div></span></p>
</div>
<div id="section-2" class="proof" data-ref="linearityexp" name="Proof">
<p><span>
<div class='myequationbox'><span class="math display">\[
\begin{gathered}
\E [X+Y] = \sum_{x\in \{0,1\}^n}2^{-n}\left(X(x)+Y(x)\right) =  \\
\sum_{x\in \{0,1\}^b} 2^{-n}X(x) + \sum_{x\in \{0,1\}^b} 2^{-n}Y(x) = \\
\E[X] + \E[Y]
\end{gathered}
\]</span></div></span></p>
</div>
<p>Similarly, <span><span class="math inline">\(\E[kX] = k\E[X]\)</span></span> for every <span><span class="math inline">\(k \in \R\)</span></span>.</p>
<div id="expectationofsum" class="solvedexercise" title="Expectation of sum" name="Solvedexercise 17.1 (Expectation of sum) ">
<p>Let <span><span class="math inline">\(X:\{0,1\}^n \rightarrow \R\)</span></span> be the random variable that maps <span><span class="math inline">\(x\in \{0,1\}^n\)</span></span> to <span><span class="math inline">\(x_0 + x_1 + \ldots + x_{n-1}\)</span></span>. Prove that <span><span class="math inline">\(\E[X] = n/2\)</span></span>.</p>
</div>
<div class="solution" data-ref="expectationofsum" name="Solution 17.1.1">
<p>We can solve this using the linearity of expectation. We can define random variables <span><span class="math inline">\(X_0,X_1,\ldots,X_{n-1}\)</span></span> such that <span><span class="math inline">\(X_i(x)= x_i\)</span></span>. Since each <span><span class="math inline">\(x_i\)</span></span> equals <span><span class="math inline">\(1\)</span></span> with probability <span><span class="math inline">\(1/2\)</span></span> and <span><span class="math inline">\(0\)</span></span> with probability <span><span class="math inline">\(1/2\)</span></span>, <span><span class="math inline">\(\E[X_i]=1/2\)</span></span>. Since <span><span class="math inline">\(X = \sum_{i=0}^{n-1} X_i\)</span></span>, by the linearity of expectation <span>
<div class='myequationbox'><span class="math display">\[
\E[X] = \E[X_0] + \E[X_1] + \cdots + \E[X_{n-1}] = \tfrac{n}{2} \;.
\]</span></div></span></p>
</div>
<div id="section-3" class="pause" name="Pause">
<p>If you have not seen discrete probability before, please go over this argument again until you are sure you follow it; it is a prototypical simple example of the type of reasoning we will employ again and again in this course.</p>
</div>
<p>If <span><span class="math inline">\(A\)</span></span> is an event, then <span><span class="math inline">\(1_A\)</span></span> is the random variable such that <span><span class="math inline">\(1_A(x)\)</span></span> equals <span><span class="math inline">\(1\)</span></span> if <span><span class="math inline">\(x\in A\)</span></span>, and <span><span class="math inline">\(1_A(x)=0\)</span></span> otherwise. Note that <span><span class="math inline">\(\Pr[A] = \E[1_A]\)</span></span> (can you see why?). Using this and the linearity of expectation, we can show one of the most useful bounds in probability theory:</p>
<div id="unionbound" class="lemma" title="Union bound" name="Lemma 17.4 (Union bound) ">
<p>For every two events <span><span class="math inline">\(A,B\)</span></span>, <span><span class="math inline">\(\Pr[ A \cup B] \leq \Pr[A]+\Pr[B]\)</span></span></p>
</div>
<div id="section-4" class="pause" name="Pause">
<p>Before looking at the proof, try to see why the union bound makes intuitive sense. We can also prove it directly from the definition of probabilities and the cardinality of sets, together with the equation <span><span class="math inline">\(|A \cup B| \leq |A|+|B|\)</span></span>. Can you see why the latter equation is true? (See also <a href='#unionboundfig'>Figure 17.3</a>.)</p>
</div>
<div id="section-5" class="proof" data-ref="unionbound" name="Proof">
<p>For every <span><span class="math inline">\(x\)</span></span>, the variable <span><span class="math inline">\(1_{A\cup B}(x) \leq 1_A(x)+1_B(x)\)</span></span>. Hence, <span><span class="math inline">\(\Pr[A\cup B] = \E[ 1_{A \cup B} ] \leq \E[1_A+1_B] = \E[1_A]+\E[1_B] = \Pr[A]+\Pr[B]\)</span></span>.</p>
</div>
<p>The way we often use this in theoretical computer science is to argue that, for example, if there is a list of 100 bad events that can happen, and each one of them happens with probability at most <span><span class="math inline">\(1/10000\)</span></span>, then with probability at least <span><span class="math inline">\(1-100/10000 = 0.99\)</span></span>, no bad event happens.</p>
<figure>
<img src="../figure/unionbound.png" alt="17.3: The union bound tells us that the probability of A or B happening is at most the sum of the individual probabilities. We can see it by noting that for every two sets |A\cup B| \leq |A|+|B| (with equality only if A and B have no intersection)." id="unionboundfig" class="margin" /><figcaption>17.3: The <em>union bound</em> tells us that the probability of <span><span class="math inline">\(A\)</span></span> or <span><span class="math inline">\(B\)</span></span> happening is at most the sum of the individual probabilities. We can see it by noting that for every two sets <span><span class="math inline">\(|A\cup B| \leq |A|+|B|\)</span></span> (with equality only if <span><span class="math inline">\(A\)</span></span> and <span><span class="math inline">\(B\)</span></span> have no intersection).</figcaption>
</figure>
<h3 id="distributions-over-strings" data-number="17.1.2">Distributions over strings</h3>
<p>While most of the time we think of random variables as having as output a <em>real number</em>, we sometimes consider random variables whose output is a <em>string</em>. That is, we can think of a map <span><span class="math inline">\(Y:\{0,1\}^n \rightarrow \{0,1\}^*\)</span></span> and consider the “random variable” <span><span class="math inline">\(Y\)</span></span> such that for every <span><span class="math inline">\(y\in \{0,1\}^*\)</span></span>, the probability that <span><span class="math inline">\(Y\)</span></span> outputs <span><span class="math inline">\(y\)</span></span> is equal to <span><span class="math inline">\(\tfrac{1}{2^n}\left| \{ x \in \{0,1\}^n \;|\; Y(x)=y \}\right|\)</span></span>. To avoid confusion, we will typically refer to such string-valued random variables as <em>distributions</em> over strings. So, a <em>distribution</em> <span><span class="math inline">\(Y\)</span></span> over strings <span><span class="math inline">\(\{0,1\}^*\)</span></span> can be thought of as a finite collection of strings <span><span class="math inline">\(y_0,\ldots,y_{M-1} \in \{0,1\}^*\)</span></span> and probabilities <span><span class="math inline">\(p_0,\ldots,p_{M-1}\)</span></span> (which are non-negative numbers summing up to one), so that <span><span class="math inline">\(\Pr[ Y = y_i ] = p_i\)</span></span>.</p>
<p>Two distributions <span><span class="math inline">\(Y\)</span></span> and <span><span class="math inline">\(Y&#39;\)</span></span> are <em>identical</em> if they assign the same probability to every string. For example, consider the following two functions <span><span class="math inline">\(Y,Y&#39;:\{0,1\}^2 \rightarrow \{0,1\}^2\)</span></span>. For every <span><span class="math inline">\(x \in \{0,1\}^2\)</span></span>, we define <span><span class="math inline">\(Y(x)=x\)</span></span> and <span><span class="math inline">\(Y&#39;(x)=x_0(x_0\oplus x_1)\)</span></span> where <span><span class="math inline">\(\oplus\)</span></span> is the XOR operations. Although these are two different functions, they induce the same distribution over <span><span class="math inline">\(\{0,1\}^2\)</span></span> when invoked on a uniform input. The distribution <span><span class="math inline">\(Y(x)\)</span></span> for <span><span class="math inline">\(x\sim \{0,1\}^2\)</span></span> is of course the uniform distribution over <span><span class="math inline">\(\{0,1\}^2\)</span></span>. On the other hand <span><span class="math inline">\(Y&#39;\)</span></span> is simply the map <span><span class="math inline">\(00 \mapsto 00\)</span></span>, <span><span class="math inline">\(01 \mapsto 01\)</span></span>, <span><span class="math inline">\(10 \mapsto 11\)</span></span>, <span><span class="math inline">\(11 \mapsto 10\)</span></span> which is a permutation over the map <span><span class="math inline">\(F:\{0,1\}^2 \rightarrow \{0,1\}^2\)</span></span> defined as <span><span class="math inline">\(F(x_0x_1)=x_0x_1\)</span></span> and the map <span><span class="math inline">\(G:\{0,1\}^2 \rightarrow \{0,1\}^2\)</span></span> defined as <span><span class="math inline">\(G(x_0x_1)=x_0(x_0 \oplus x_1)\)</span></span></p>
<h3 id="more-general-sample-spaces." data-number="17.1.3">More general sample spaces.</h3>
<p>While in this chapter we assume that the underlying probabilistic experiment corresponds to tossing <span><span class="math inline">\(n\)</span></span> independent coins, everything we say easily generalizes to sampling <span><span class="math inline">\(x\)</span></span> from a more general finite or countable set <span><span class="math inline">\(S\)</span></span> (and not-so-easily generalizes to uncountable sets <span><span class="math inline">\(S\)</span></span> as well). A <em>probability distribution</em> over a finite set <span><span class="math inline">\(S\)</span></span> is simply a function <span><span class="math inline">\(\mu : S \rightarrow [0,1]\)</span></span> such that <span><span class="math inline">\(\sum_{x\in S}\mu(s)=1\)</span></span>. We think of this as the experiment where we obtain every <span><span class="math inline">\(x\in S\)</span></span> with probability <span><span class="math inline">\(\mu(s)\)</span></span>, and sometimes denote this as <span><span class="math inline">\(x\sim \mu\)</span></span>. An <em>event</em> <span><span class="math inline">\(A\)</span></span> is a subset of <span><span class="math inline">\(S\)</span></span>, and the probability of <span><span class="math inline">\(A\)</span></span>, which we denote by <span><span class="math inline">\(\Pr_\mu[A]\)</span></span>, is <span><span class="math inline">\(\sum_{x\in A} \mu(x)\)</span></span>. A <em>random variable</em> is a function <span><span class="math inline">\(X:S \rightarrow \R\)</span></span>, where the probability that <span><span class="math inline">\(X=y\)</span></span> is equal to <span><span class="math inline">\(\sum_{x\in S \text{ s.t. } X(x)=y} \mu(x)\)</span></span>.</p>
<h2 id="correlations-and-independence" data-number="17.2">Correlations and independence</h2>
<p>One of the most delicate but important concepts in probability is the notion of <em>independence</em> (and the opposing notion of <em>correlations</em>). Subtle correlations are often behind surprises and errors in probability and statistical analysis, and several mistaken predictions have been blamed on miscalculating the correlations between, say, housing prices in Florida and Arizona, or voter preferences in Ohio and Michigan. See also Joe Blitzstein’s aptly named talk <a href="https://youtu.be/dzFf3r1yph8">“Conditioning is the Soul of Statistics”</a>. (Another thorny issue is of course the difference between <em>correlation</em> and <em>causation</em>. Luckily, this is another point we don’t need to worry about in our clean setting of tossing <span><span class="math inline">\(n\)</span></span> coins.)</p>
<p>Two events <span><span class="math inline">\(A\)</span></span> and <span><span class="math inline">\(B\)</span></span> are <em>independent</em> if the fact that <span><span class="math inline">\(A\)</span></span> happens makes <span><span class="math inline">\(B\)</span></span> neither more nor less likely to happen. For example, if we think of the experiment of tossing <span><span class="math inline">\(3\)</span></span> random coins <span><span class="math inline">\(x\in \{0,1\}^3\)</span></span>, and we let <span><span class="math inline">\(A\)</span></span> be the event that <span><span class="math inline">\(x_0=1\)</span></span> and <span><span class="math inline">\(B\)</span></span> the event that <span><span class="math inline">\(x_0 + x_1 + x_2 \geq 2\)</span></span>, then if <span><span class="math inline">\(A\)</span></span> happens it is more likely that <span><span class="math inline">\(B\)</span></span> happens, and hence these events are <em>not</em> independent. On the other hand, if we let <span><span class="math inline">\(C\)</span></span> be the event that <span><span class="math inline">\(x_1=1\)</span></span>, then because the second coin toss is not affected by the result of the first one, the events <span><span class="math inline">\(A\)</span></span> and <span><span class="math inline">\(C\)</span></span> are independent.</p>
<p>The formal definition is that events <span><span class="math inline">\(A\)</span></span> and <span><span class="math inline">\(B\)</span></span> are <em>independent</em> if <span><span class="math inline">\(\Pr[A \cap B]=\Pr[A] \cdot \Pr[B]\)</span></span>. If <span><span class="math inline">\(\Pr[A \cap B] &gt; \Pr[A]\cdot \Pr[B]\)</span></span> then we say that <span><span class="math inline">\(A\)</span></span> and <span><span class="math inline">\(B\)</span></span> are <em>positively correlated</em>, while if <span><span class="math inline">\(\Pr[ A \cap B] &lt; \Pr[A] \cdot \Pr[B]\)</span></span> then we say that <span><span class="math inline">\(A\)</span></span> and <span><span class="math inline">\(B\)</span></span> are <em>negatively correlated</em> (see <a href='#coinexperimentfig'>Figure 17.1</a>).</p>
<figure>
<img src="../figure/independence.png" alt="17.4: Two events A and B are independent if \Pr[A \cap B]=\Pr[A]\cdot \Pr[B]. In the two figures above, the empty x\times x square is the sample space, and A and B are two events in this sample space. In the left figure, A and B are independent, while in the right figure they are negatively correlated, since B is less likely to occur if we condition on A (and vice versa). Mathematically, one can see this by noticing that in the left figure the areas of A and B respectively are a\cdot x and b\cdot x, and so their probabilities are \tfrac{a\cdot x}{x^2}=\tfrac{a}{x} and \tfrac{b\cdot x}{x^2}=\tfrac{b}{x} respectively, while the area of A \cap B is a\cdot b which corresponds to the probability \tfrac{a\cdot b}{x^2}. In the right figure, the area of the triangle B is \tfrac{b\cdot x}{2} which corresponds to a probability of \tfrac{b}{2x}, but the area of A \cap B is \tfrac{b&#39; \cdot a}{2} for some b&#39;&lt;b. This means that the probability of A \cap B is \tfrac{b&#39;\cdot a}{2x^2} &lt; \tfrac{b}{2x} \cdot \tfrac{a}{x}, or in other words \Pr[A \cap B ] &lt; \Pr[A] \cdot \Pr[B]." id="independencefig" class="margin" /><figcaption>17.4: Two events <span><span class="math inline">\(A\)</span></span> and <span><span class="math inline">\(B\)</span></span> are <em>independent</em> if <span><span class="math inline">\(\Pr[A \cap B]=\Pr[A]\cdot \Pr[B]\)</span></span>. In the two figures above, the empty <span><span class="math inline">\(x\times x\)</span></span> square is the sample space, and <span><span class="math inline">\(A\)</span></span> and <span><span class="math inline">\(B\)</span></span> are two events in this sample space. In the left figure, <span><span class="math inline">\(A\)</span></span> and <span><span class="math inline">\(B\)</span></span> are independent, while in the right figure they are negatively correlated, since <span><span class="math inline">\(B\)</span></span> is less likely to occur if we condition on <span><span class="math inline">\(A\)</span></span> (and vice versa). Mathematically, one can see this by noticing that in the left figure the areas of <span><span class="math inline">\(A\)</span></span> and <span><span class="math inline">\(B\)</span></span> respectively are <span><span class="math inline">\(a\cdot x\)</span></span> and <span><span class="math inline">\(b\cdot x\)</span></span>, and so their probabilities are <span><span class="math inline">\(\tfrac{a\cdot x}{x^2}=\tfrac{a}{x}\)</span></span> and <span><span class="math inline">\(\tfrac{b\cdot x}{x^2}=\tfrac{b}{x}\)</span></span> respectively, while the area of <span><span class="math inline">\(A \cap B\)</span></span> is <span><span class="math inline">\(a\cdot b\)</span></span> which corresponds to the probability <span><span class="math inline">\(\tfrac{a\cdot b}{x^2}\)</span></span>. In the right figure, the area of the triangle <span><span class="math inline">\(B\)</span></span> is <span><span class="math inline">\(\tfrac{b\cdot x}{2}\)</span></span> which corresponds to a probability of <span><span class="math inline">\(\tfrac{b}{2x}\)</span></span>, but the area of <span><span class="math inline">\(A \cap B\)</span></span> is <span><span class="math inline">\(\tfrac{b&#39; \cdot a}{2}\)</span></span> for some <span><span class="math inline">\(b&#39;&lt;b\)</span></span>. This means that the probability of <span><span class="math inline">\(A \cap B\)</span></span> is <span><span class="math inline">\(\tfrac{b&#39;\cdot a}{2x^2} &lt; \tfrac{b}{2x} \cdot \tfrac{a}{x}\)</span></span>, or in other words <span><span class="math inline">\(\Pr[A \cap B ] &lt; \Pr[A] \cdot \Pr[B]\)</span></span>.</figcaption>
</figure>
<p>If we consider the above examples on the experiment of choosing <span><span class="math inline">\(x\in \{0,1\}^3\)</span></span> then we can see that</p>
<p><span>
<div class='myequationbox'><span class="math display">\[
\begin{aligned}
\Pr[x_0=1] &amp;= \tfrac{1}{2} \\
\Pr[x_0+x_1+x_2 \geq 2] = \Pr[\{ 011,101,110,111 \}] &amp;= \tfrac{4}{8} = \tfrac{1}{2}
\end{aligned}
\]</span></div></span></p>
<p>but</p>
<p><span>
<div class='myequationbox'><span class="math display">\[
\Pr[x_0 =1 \; \wedge \; x_0+x_1+x_2 \geq 2 ] = \Pr[ \{101,110,111 \} ] = \tfrac{3}{8} &gt; \tfrac{1}{2} \cdot \tfrac{1}{2}
\]</span></div></span></p>
<p>and hence, as we already observed, the events <span><span class="math inline">\(\{ x_0 = 1 \}\)</span></span> and <span><span class="math inline">\(\{ x_0+x_1+x_2 \geq 2 \}\)</span></span> are not independent and in fact are positively correlated. On the other hand, <span><span class="math inline">\(\Pr[ x_0 = 1 \wedge x_1 = 1 ] = \Pr[ \{110,111 \}] = \tfrac{2}{8} = \tfrac{1}{2} \cdot \tfrac{1}{2}\)</span></span> and hence the events <span><span class="math inline">\(\{x_0 = 1 \}\)</span></span> and <span><span class="math inline">\(\{ x_1 = 1 \}\)</span></span> are indeed independent.</p>
<div id="disjoint" class="remark" title="Disjointness vs independence" name="Remark 17.5 (Disjointness vs independence) ">
<p>People sometimes confuse the notion of <em>disjointness</em> and <em>independence</em>, but these are actually quite different. Two events <span><span class="math inline">\(A\)</span></span> and <span><span class="math inline">\(B\)</span></span> are <em>disjoint</em> if <span><span class="math inline">\(A \cap B = \emptyset\)</span></span>, which means that if <span><span class="math inline">\(A\)</span></span> happens then <span><span class="math inline">\(B\)</span></span> definitely does not happen. They are <em>independent</em> if <span><span class="math inline">\(\Pr[A \cap B]=\Pr[A]\Pr[B]\)</span></span> which means that knowing that <span><span class="math inline">\(A\)</span></span> happens gives us no information about whether <span><span class="math inline">\(B\)</span></span> happened or not. If <span><span class="math inline">\(A\)</span></span> and <span><span class="math inline">\(B\)</span></span> have nonzero probability, then being disjoint implies that they are <em>not</em> independent, since in particular it means that they are negatively correlated.</p>
</div>
<p><strong>Conditional probability:</strong> If <span><span class="math inline">\(A\)</span></span> and <span><span class="math inline">\(B\)</span></span> are events, and <span><span class="math inline">\(A\)</span></span> happens with nonzero probability then we define the probability that <span><span class="math inline">\(B\)</span></span> happens <em>conditioned on <span><span class="math inline">\(A\)</span></span></em> to be <span><span class="math inline">\(\Pr[B|A] = \Pr[A \cap B]/\Pr[A]\)</span></span>. This corresponds to calculating the probability that <span><span class="math inline">\(B\)</span></span> happens if we already know that <span><span class="math inline">\(A\)</span></span> happened. Note that <span><span class="math inline">\(A\)</span></span> and <span><span class="math inline">\(B\)</span></span> are independent if and only if <span><span class="math inline">\(\Pr[B|A]=\Pr[B]\)</span></span>.</p>
<p><strong>More than two events:</strong> We can generalize this definition to more than two events. We say that events <span><span class="math inline">\(A_1,\ldots,A_k\)</span></span> are <em>mutually independent</em> if knowing that any set of them occurred or didn’t occur does not change the probability that an event outside the set occurs. Formally, the condition is that for every subset <span><span class="math inline">\(I \subseteq [k]\)</span></span>, <span>
<div class='myequationbox'><span class="math display">\[
\Pr[ \wedge_{i\in I} A_i] =\prod_{i\in I} \Pr[A_i].
\]</span></div></span></p>
<p>For example, if <span><span class="math inline">\(x\sim \{0,1\}^3\)</span></span>, then the events <span><span class="math inline">\(\{ x_0=1 \}\)</span></span>, <span><span class="math inline">\(\{ x_1 = 1\}\)</span></span> and <span><span class="math inline">\(\{x_2 = 1 \}\)</span></span> are mutually independent. On the other hand, the events <span><span class="math inline">\(\{x_0 = 1 \}\)</span></span>, <span><span class="math inline">\(\{x_1 = 1\}\)</span></span> and <span><span class="math inline">\(\{ x_0 + x_1 = 0 \mod 2 \}\)</span></span> are <em>not</em> mutually independent, even though every pair of these events is independent (can you see why? see also <a href='#independencecoinsfig'>Figure 17.5</a>).</p>
<figure>
<img src="../figure/independencecoins.png" alt="17.5: Consider the sample space \{0,1\}^n and the events A,B,C,D,E corresponding to A: x_0=1, B: x_1=1, C: x_0+x_1+x_2 \geq 2, D: x_0+x_1+x_2 = 0 mod 2 and D: x_0+x_1 = 0 mod 2. We can see that A and B are independent, C is positively correlated with A and positively correlated with B, the three events A,B,D are mutually independent, and while every pair out of A,B,E is independent, the three events A,B,E are not mutually independent since their intersection has probability \tfrac{2}{8}=\tfrac{1}{4} instead of \tfrac{1}{2}\cdot \tfrac{1}{2} \cdot \tfrac{1}{2} = \tfrac{1}{8}." id="independencecoinsfig" class="margin" /><figcaption>17.5: Consider the sample space <span><span class="math inline">\(\{0,1\}^n\)</span></span> and the events <span><span class="math inline">\(A,B,C,D,E\)</span></span> corresponding to <span><span class="math inline">\(A\)</span></span>: <span><span class="math inline">\(x_0=1\)</span></span>, <span><span class="math inline">\(B\)</span></span>: <span><span class="math inline">\(x_1=1\)</span></span>, <span><span class="math inline">\(C\)</span></span>: <span><span class="math inline">\(x_0+x_1+x_2 \geq 2\)</span></span>, <span><span class="math inline">\(D\)</span></span>: <span><span class="math inline">\(x_0+x_1+x_2 = 0 mod 2\)</span></span> and <span><span class="math inline">\(D\)</span></span>: <span><span class="math inline">\(x_0+x_1 = 0 mod 2\)</span></span>. We can see that <span><span class="math inline">\(A\)</span></span> and <span><span class="math inline">\(B\)</span></span> are independent, <span><span class="math inline">\(C\)</span></span> is positively correlated with <span><span class="math inline">\(A\)</span></span> and positively correlated with <span><span class="math inline">\(B\)</span></span>, the three events <span><span class="math inline">\(A,B,D\)</span></span> are mutually independent, and while every pair out of <span><span class="math inline">\(A,B,E\)</span></span> is independent, the three events <span><span class="math inline">\(A,B,E\)</span></span> are not mutually independent since their intersection has probability <span><span class="math inline">\(\tfrac{2}{8}=\tfrac{1}{4}\)</span></span> instead of <span><span class="math inline">\(\tfrac{1}{2}\cdot \tfrac{1}{2} \cdot \tfrac{1}{2} = \tfrac{1}{8}\)</span></span>.</figcaption>
</figure>
<h3 id="independent-random-variables" data-number="17.2.1">Independent random variables</h3>
<p>We say that two random variables <span><span class="math inline">\(X:\{0,1\}^n \rightarrow \R\)</span></span> and <span><span class="math inline">\(Y:\{0,1\}^n \rightarrow \R\)</span></span> are independent if for every <span><span class="math inline">\(u,v \in \R\)</span></span>, the events <span><span class="math inline">\(\{ X=u \}\)</span></span> and <span><span class="math inline">\(\{ Y=v \}\)</span></span> are independent. (We use <span><span class="math inline">\(\{ X=u \}\)</span></span> as shorthand for <span><span class="math inline">\(\{ x \;|\; X(x)=u \}\)</span></span>.) In other words, <span><span class="math inline">\(X\)</span></span> and <span><span class="math inline">\(Y\)</span></span> are independent if <span><span class="math inline">\(\Pr[ X=u \wedge Y=v]=\Pr[X=u]\Pr[Y=v]\)</span></span> for every <span><span class="math inline">\(u,v \in \R\)</span></span>. For example, if two random variables depend on the result of tossing different coins then they are independent:</p>
<div id="indcoins" class="lemma" name="Lemma 17.6">
<p>Suppose that <span><span class="math inline">\(S=\{ s_0,\ldots, s_{k-1} \}\)</span></span> and <span><span class="math inline">\(T=\{ t_0 ,\ldots, t_{m-1} \}\)</span></span> are disjoint subsets of <span><span class="math inline">\(\{0,\ldots,n-1\}\)</span></span> and let <span><span class="math inline">\(X,Y:\{0,1\}^n \rightarrow \R\)</span></span> be random variables such that <span><span class="math inline">\(X=F(x_{s_0},\ldots,x_{s_{k-1}})\)</span></span> and <span><span class="math inline">\(Y=G(x_{t_0},\ldots,x_{t_{m-1}})\)</span></span> for some functions <span><span class="math inline">\(F: \{0,1\}^k \rightarrow \R\)</span></span> and <span><span class="math inline">\(G: \{0,1\}^m \rightarrow \R\)</span></span>. Then <span><span class="math inline">\(X\)</span></span> and <span><span class="math inline">\(Y\)</span></span> are independent.</p>
</div>
<div id="section-6" class="pause" name="Pause">
<p>The notation in the lemma’s statement is a bit cumbersome, but at the end of the day, it simply says that if <span><span class="math inline">\(X\)</span></span> and <span><span class="math inline">\(Y\)</span></span> are random variables that depend on two disjoint sets <span><span class="math inline">\(S\)</span></span> and <span><span class="math inline">\(T\)</span></span> of coins (for example, <span><span class="math inline">\(X\)</span></span> might be the sum of the first <span><span class="math inline">\(n/2\)</span></span> coins, and <span><span class="math inline">\(Y\)</span></span> might be the largest consecutive stretch of zeroes in the second <span><span class="math inline">\(n/2\)</span></span> coins), then they are independent.</p>
</div>
<div id="section-7" class="proof" data-ref="indcoins" name="Proof">
<p>Let <span><span class="math inline">\(a,b\in \R\)</span></span>, and let <span><span class="math inline">\(A = \{ x \in \{0,1\}^k : F(x)=a \}\)</span></span> and <span><span class="math inline">\(B=\{ x\in \{0,1\}^m : F(x)=b \}\)</span></span>. Since <span><span class="math inline">\(S\)</span></span> and <span><span class="math inline">\(T\)</span></span> are disjoint, we can reorder the indices so that <span><span class="math inline">\(S = \{0,\ldots,k-1\}\)</span></span> and <span><span class="math inline">\(T=\{k,\ldots,k+m-1\}\)</span></span> without affecting any of the probabilities. Hence we can write <span><span class="math inline">\(\Pr[X=a \wedge X=b] = |C|/2^n\)</span></span> where <span><span class="math inline">\(C= \{ x_0,\ldots,x_{n-1} : (x_0,\ldots,x_{k-1}) \in A \wedge (x_k,\ldots,x_{k+m-1}) \in B \}\)</span></span>. Another way to write this using string concatenation is that <span><span class="math inline">\(C = \{ xyz : x\in A, y\in B, z\in \{0,1\}^{n-k-m} \}\)</span></span>, and hence <span><span class="math inline">\(|C|=|A||B|2^{n-k-m}\)</span></span>, which means that <span>
<div class='myequationbox'><span class="math display">\[
\tfrac{|C|}{2^n} = \tfrac{|A|}{2^k}\tfrac{|B|}{2^m}\tfrac{2^{n-k-m}}{2^{n-k-m}}=\Pr[X=a]\Pr[Y=b] .
\]</span></div></span></p>
</div>
<p>Note that if <span><span class="math inline">\(X\)</span></span> and <span><span class="math inline">\(Y\)</span></span> are independent random variables then (if we let <span><span class="math inline">\(S_X,S_Y\)</span></span> denote all the numbers that have positive probability of being the output of <span><span class="math inline">\(X\)</span></span> and <span><span class="math inline">\(Y\)</span></span>, respectively) it holds that: <span>
<div class='myequationbox'><span class="math display">\[
\begin{gathered}
\E[ \ensuremath{\mathit{XY}} ] = \sum_{a \in S_X,b \in S_Y} {\textstyle\Pr[X=a \wedge Y=b]}\cdot ab \; =^{(1)} \; \sum_{a \in S_X,b \in S_Y} {\textstyle \Pr[X=a]\Pr[Y=b]}\cdot ab =^{(2)} \\
\left(\sum_{a \in S_X} {\textstyle \Pr[X=a]}\cdot a\right)\left(\sum_{b \in S_Y} {\textstyle \Pr[Y=b]}b\right) =^{(3)} \\
\E[X] \E[Y]
\end{gathered}
\]</span></div></span> where the first equality (<span><span class="math inline">\(=^{(1)}\)</span></span>) follows from the independence of <span><span class="math inline">\(X\)</span></span> and <span><span class="math inline">\(Y\)</span></span>, the second equality (<span><span class="math inline">\(=^{(2)}\)</span></span>) follows by “opening the parentheses” of the righthand side, and the third inequality (<span><span class="math inline">\(=^{(3)}\)</span></span>) follows from the definition of expectation. (This is not an “if and only if”; see <a href='#noindnocorex'>Exercise 17.3</a>.)</p>
<p>Another useful fact is that if <span><span class="math inline">\(X\)</span></span> and <span><span class="math inline">\(Y\)</span></span> are independent random variables, then so are <span><span class="math inline">\(F(X)\)</span></span> and <span><span class="math inline">\(G(Y)\)</span></span> for all functions <span><span class="math inline">\(F,G:\R \rightarrow R\)</span></span>. This is intuitively true since learning <span><span class="math inline">\(F(X)\)</span></span> can only provide us with less information than does learning <span><span class="math inline">\(X\)</span></span> itself. Hence, if learning <span><span class="math inline">\(X\)</span></span> does not teach us anything about <span><span class="math inline">\(Y\)</span></span> (and so also about <span><span class="math inline">\(F(Y)\)</span></span>) then neither will learning <span><span class="math inline">\(F(X)\)</span></span>. Indeed, to prove this we can write for every <span><span class="math inline">\(a,b \in \R\)</span></span>:</p>
<p><span>
<div class='myequationbox'><span class="math display">\[
\begin{gathered}
\Pr[ F(X)=a \wedge G(Y)=b ] = \sum_{x \text{ s.t.} F(x)=a, y \text{ s.t. } G(y)=b} \Pr[ X=x \wedge Y=y ] = \\
\sum_{x \text{ s.t.} F(x)=a, y \text{ s.t. } G(y)=b} \Pr[ X=x ] \Pr[  Y=y ]  = \\
\left( \sum_{x \text{ s.t.} F(x)=a } \Pr[X=x ] \right) \cdot \left( \sum_{y \text{ s.t.} G(y)=b } \Pr[Y=y ] \right) = \\
\Pr[ F(X)=a] \Pr[G(Y)=b] .
\end{gathered}
\]</span></div></span></p>
<h3 id="collections-of-independent-random-variables." data-number="17.2.2">Collections of independent random variables.</h3>
<p>We can extend the notions of independence to more than two random variables: we say that the random variables <span><span class="math inline">\(X_0,\ldots,X_{n-1}\)</span></span> are <em>mutually independent</em> if for every <span><span class="math inline">\(a_0,\ldots,a_{n-1} \in \R\)</span></span>, <span>
<div class='myequationbox'><span class="math display">\[
\Pr\left[X_0=a_0 \wedge \cdots \wedge X_{n-1}=a_{n-1}\right]=\Pr[X_0=a_0]\cdots \Pr[X_{n-1}=a_{n-1}] .
\]</span></div></span> And similarly, we have that</p>
<div id="expprod" class="lemma" title="Expectation of product of independent random variables" name="Lemma 17.7 (Expectation of product of independent random variables) ">
<p>If <span><span class="math inline">\(X_0,\ldots,X_{n-1}\)</span></span> are mutually independent then <span>
<div class='myequationbox'><span class="math display">\[
\E[ \prod_{i=0}^{n-1} X_i ] = \prod_{i=0}^{n-1} \E[X_i] .
\]</span></div></span></p>
</div>
<div id="indeplem" class="lemma" title="Functions preserve independence" name="Lemma 17.8 (Functions preserve independence) ">
<p>If <span><span class="math inline">\(X_0,\ldots,X_{n-1}\)</span></span> are mutually independent, and <span><span class="math inline">\(Y_0,\ldots,Y_{n-1}\)</span></span> are defined as <span><span class="math inline">\(Y_i = F_i(X_i)\)</span></span> for some functions <span><span class="math inline">\(F_0,\ldots,F_{n-1}:\R \rightarrow \R\)</span></span>, then <span><span class="math inline">\(Y_0,\ldots,Y_{n-1}\)</span></span> are mutually independent as well.</p>
</div>
<div id="section-8" class="pause" name="Pause">
<p>We leave proving <a href='#expprod'>Lemma 17.7</a> and <a href='#indeplem'>Lemma 17.8</a> as <a href='#expprodex'>Exercise 17.5</a> <a href='#indeplemex'>Exercise 17.6</a>. It is good idea for you stop now and do these exercises to make sure you are comfortable with the notion of independence, as we will use it heavily later on in this course.</p>
</div>
<h2 id="concentration-and-tail-bounds" data-number="17.3">Concentration and tail bounds</h2>
<p>The name “expectation” is somewhat misleading. For example, suppose that you and I place a bet on the outcome of 10 coin tosses, where if they all come out to be <span><span class="math inline">\(1\)</span></span>’s then I pay you 100,000 dollars and otherwise you pay me 10 dollars. If we let <span><span class="math inline">\(X:\{0,1\}^{10} \rightarrow \R\)</span></span> be the random variable denoting your gain, then we see that</p>
<p><span>
<div class='myequationbox'><span class="math display">\[
\E[X] = 2^{-10}\cdot 100000 - (1-2^{-10})10 \sim 90 .
\]</span></div></span></p>
<p>But we don’t really “expect” the result of this experiment to be for you to gain 90 dollars. Rather, 99.9% of the time you will pay me 10 dollars, and you will hit the jackpot 0.01% of the times.</p>
<p>However, if we repeat this experiment again and again (with fresh and hence <em>independent</em> coins), then in the long run we do expect your average earning to be close to 90 dollars, which is the reason why casinos can make money in a predictable way even though every individual bet is random. For example, if we toss <span><span class="math inline">\(n\)</span></span> independent and unbiased coins, then as <span><span class="math inline">\(n\)</span></span> grows, the number of coins that come up ones will be more and more <em>concentrated</em> around <span><span class="math inline">\(n/2\)</span></span> according to the famous “bell curve” (see <a href='#bellfig'>Figure 17.6</a>).</p>
<figure>
<img src="../figure/binomial.png" alt="17.6: The probabilities that we obtain a particular sum when we toss n=10,20,100,1000 coins converge quickly to the Gaussian/normal distribution." id="bellfig" class="margin" /><figcaption>17.6: The probabilities that we obtain a particular sum when we toss <span><span class="math inline">\(n=10,20,100,1000\)</span></span> coins converge quickly to the Gaussian/normal distribution.</figcaption>
</figure>
<p>Much of probability theory is concerned with so called <em>concentration</em> or <em>tail</em> bounds, which are upper bounds on the probability that a random variable <span><span class="math inline">\(X\)</span></span> deviates too much from its expectation. The first and simplest one of them is Markov’s inequality:</p>
<div id="markovthm" class="theorem" title="Markov&#39;s inequality" name="Theorem 17.9 (Markov&#39;s inequality) ">
<p>If <span><span class="math inline">\(X\)</span></span> is a non-negative random variable then for every <span><span class="math inline">\(k&gt;1\)</span></span>, <span><span class="math inline">\(\Pr[ X \geq k \E[X] ] \leq 1/k\)</span></span>.</p>
</div>
<div id="section-9" class="pause" name="Pause">
<p>Markov’s Inequality is actually a very natural statement (see also <a href='#markovfig'>Figure 17.7</a>). For example, if you know that the average (not the median!) household income in the US is 70,000 dollars, then in particular you can deduce that at most 25 percent of households make more than 280,000 dollars, since otherwise, even if the remaining 75 percent had zero income, the top 25 percent alone would cause the average income to be larger than 70,000 dollars. From this example you can already see that in many situations, Markov’s inequality will not be <em>tight</em> and the probability of deviating from expectation will be much smaller: see the Chebyshev and Chernoff inequalities below.</p>
</div>
<div id="section-10" class="proof" data-ref="markovthm" name="Proof">
<p>Let <span><span class="math inline">\(\mu = \E[X]\)</span></span> and define <span><span class="math inline">\(Y=1_{X \geq k \mu}\)</span></span>. That is, <span><span class="math inline">\(Y(x)=1\)</span></span> if <span><span class="math inline">\(X(x) \geq k \mu\)</span></span> and <span><span class="math inline">\(Y(x)=0\)</span></span> otherwise. Note that by definition, for every <span><span class="math inline">\(x\)</span></span>, <span><span class="math inline">\(Y(x) \leq X/(k\mu)\)</span></span>. We need to show <span><span class="math inline">\(\E[Y] \leq 1/k\)</span></span>. But this follows since <span><span class="math inline">\(\E[Y] \leq \E[X/k(\mu)] = \E[X]/(k\mu) = \mu/(k\mu)=1/k\)</span></span>.</p>
</div>
<figure>
<img src="../figure/markovineq.png" alt="17.7: Markov’s Inequality tells us that a non-negative random variable X cannot be much larger than its expectation, with high probability. For example, if the expectation of X is \mu, then the probability that X&gt;4\mu must be at most 1/4, as otherwise just the contribution from this part of the sample space will be too large." id="markovfig" class="margin" /><figcaption>17.7: Markov’s Inequality tells us that a non-negative random variable <span><span class="math inline">\(X\)</span></span> cannot be much larger than its expectation, with high probability. For example, if the expectation of <span><span class="math inline">\(X\)</span></span> is <span><span class="math inline">\(\mu\)</span></span>, then the probability that <span><span class="math inline">\(X&gt;4\mu\)</span></span> must be at most <span><span class="math inline">\(1/4\)</span></span>, as otherwise just the contribution from this part of the sample space will be too large.</figcaption>
</figure>
<p><strong>The averaging principle.</strong> While the expectation of a random variable <span><span class="math inline">\(X\)</span></span> is hardly always the “typical value”, we can show that <span><span class="math inline">\(X\)</span></span> is guaranteed to achieve a value that is at least its expectation with positive probability. For example, if the average grade in an exam is <span><span class="math inline">\(87\)</span></span> points, at least one student got a grade <span><span class="math inline">\(87\)</span></span> or more on the exam. This is known as the <em>averaging principle</em>, and despite its simplicity it is surprisingly useful.</p>
<div id="averagingprinciplerem" class="lemma" name="Lemma 17.10">
<p>Let <span><span class="math inline">\(X\)</span></span> be a random variable, then <span><span class="math inline">\(\Pr[ X \geq \E[X] ] &gt;0\)</span></span>.</p>
</div>
<div class="proof" data-ref="averagingprinciplerem" name="Proof 17.3">
<p>Suppose towards the sake of contradiction that <span><span class="math inline">\(\Pr[ X &lt; \E[X] ] =1\)</span></span>. Then the random variable <span><span class="math inline">\(Y = \E[X]-X\)</span></span> is always positive. By linearity of expectation <span><span class="math inline">\(\E[Y] = \E[X] - \E[X]=0\)</span></span>. Yet by Markov, a non-negative random variable <span><span class="math inline">\(Y\)</span></span> with <span><span class="math inline">\(\E[Y]=0\)</span></span> must equal <span><span class="math inline">\(0\)</span></span> with probability <span><span class="math inline">\(1\)</span></span>, since the probability that <span><span class="math inline">\(Y&gt; k\cdot 0 = 0\)</span></span> is at most <span><span class="math inline">\(1/k\)</span></span> for every <span><span class="math inline">\(k&gt;1\)</span></span>. Hence we get a contradiction to the assumption that <span><span class="math inline">\(Y\)</span></span> is always positive.</p>
</div>
<h3 id="chebyshevs-inequality" data-number="17.3.1">Chebyshev’s Inequality</h3>
<p>Markov’s inequality says that a (non-negative) random variable <span><span class="math inline">\(X\)</span></span> can’t go too crazy and be, say, a million times its expectation, with significant probability. But ideally we would like to say that with high probability, <span><span class="math inline">\(X\)</span></span> should be very close to its expectation, e.g., in the range <span><span class="math inline">\([0.99 \mu, 1.01 \mu]\)</span></span> where <span><span class="math inline">\(\mu = \E[X]\)</span></span>. In such a case we say that <span><span class="math inline">\(X\)</span></span> is <em>concentrated</em>, and hence its expectation (i.e., mean) will be close to its <em>median</em> and other ways of measuring <span><span class="math inline">\(X\)</span></span>’s “typical value”. <em>Chebyshev’s inequality</em> can be thought of as saying that <span><span class="math inline">\(X\)</span></span> is concentrated if it has a small <em>standard deviation</em>.</p>
<p>A standard way to measure the deviation of a random variable from its expectation is by using its <em>standard deviation</em>. For a random variable <span><span class="math inline">\(X\)</span></span>, we define the <em>variance</em> of <span><span class="math inline">\(X\)</span></span> as <span><span class="math inline">\(\mathrm{Var}[X] = \E[(X-\mu)^2]\)</span></span> where <span><span class="math inline">\(\mu = \E[X]\)</span></span>; i.e., the variance is the average squared distance of <span><span class="math inline">\(X\)</span></span> from its expectation. The <em>standard deviation</em> of <span><span class="math inline">\(X\)</span></span> is defined as <span><span class="math inline">\(\sigma[X] = \sqrt{\mathrm{Var}[X]}\)</span></span>. (This is well-defined since the variance, being an average of a square, is always a non-negative number.)</p>
<p>Using Chebyshev’s inequality, we can control the probability that a random variable is too many standard deviations away from its expectation.</p>
<div id="chebychevthm" class="theorem" title="Chebyshev&#39;s inequality" name="Theorem 17.11 (Chebyshev&#39;s inequality) ">
<p>Suppose that <span><span class="math inline">\(\mu=\E[X]\)</span></span> and <span><span class="math inline">\(\sigma^2 = \mathrm{Var}[X]\)</span></span>. Then for every <span><span class="math inline">\(k&gt;0\)</span></span>, <span><span class="math inline">\(\Pr[ |X-\mu | \geq k \sigma ] \leq 1/k^2\)</span></span>.</p>
</div>
<div id="section-11" class="proof" data-ref="chebychevthm" name="Proof">
<p>The proof follows from Markov’s inequality. We define the random variable <span><span class="math inline">\(Y = (X-\mu)^2\)</span></span>. Then <span><span class="math inline">\(\E[Y] = \mathrm{Var}[X] = \sigma^2\)</span></span>, and hence by Markov the probability that <span><span class="math inline">\(Y &gt; k^2\sigma^2\)</span></span> is at most <span><span class="math inline">\(1/k^2\)</span></span>. But clearly <span><span class="math inline">\((X-\mu)^2 \geq k^2\sigma^2\)</span></span> if and only if <span><span class="math inline">\(|X-\mu| \geq k\sigma\)</span></span>.</p>
</div>
<p>One example of how to use Chebyshev’s inequality is the setting when <span><span class="math inline">\(X = X_1 + \cdots + X_n\)</span></span> where <span><span class="math inline">\(X_i\)</span></span>’s are <em>independent and identically distributed</em> (i.i.d for short) variables with values in <span><span class="math inline">\([0,1]\)</span></span> where each has expectation <span><span class="math inline">\(1/2\)</span></span>. Since <span><span class="math inline">\(\E[X] = \sum_i \E[X_i] = n/2\)</span></span>, we would like to say that <span><span class="math inline">\(X\)</span></span> is very likely to be in, say, the interval <span><span class="math inline">\([0.499n,0.501n]\)</span></span>. Using Markov’s inequality directly will not help us, since it will only tell us that <span><span class="math inline">\(X\)</span></span> is very likely to be at most <span><span class="math inline">\(100n\)</span></span> (which we already knew, since it always lies between <span><span class="math inline">\(0\)</span></span> and <span><span class="math inline">\(n\)</span></span>). However, since <span><span class="math inline">\(X_1,\ldots,X_n\)</span></span> are independent, <span>
<div class='myequationbox'><span class="math display">\[
\mathrm{Var}[X_1+\cdots +X_n] = \mathrm{Var}[X_1]+\cdots + \mathrm{Var}[X_n]  \;\;(17.18)\;.
\]</span><a id='varianceeq'></a></div></span> (We leave showing this to the reader as <a href='#varianceex'>Exercise 17.7</a>.)</p>
<p>For every random variable <span><span class="math inline">\(X_i\)</span></span> in <span><span class="math inline">\([0,1]\)</span></span>, <span><span class="math inline">\(\mathrm{Var}[X_i] \leq 1\)</span></span> (if the variable is always in <span><span class="math inline">\([0,1]\)</span></span>, it can’t be more than <span><span class="math inline">\(1\)</span></span> away from its expectation), and hence <a href='#varianceeq'>Equation 17.18</a> implies that <span><span class="math inline">\(\mathrm{Var}[X]\leq n\)</span></span> and hence <span><span class="math inline">\(\sigma[X] \leq \sqrt{n}\)</span></span>. For large <span><span class="math inline">\(n\)</span></span>, <span><span class="math inline">\(\sqrt{n} \ll 0.001n\)</span></span>, and in particular if <span><span class="math inline">\(\sqrt{n} \leq 0.001n/k\)</span></span>, we can use Chebyshev’s inequality to bound the probability that <span><span class="math inline">\(X\)</span></span> is not in <span><span class="math inline">\([0.499n,0.501n]\)</span></span> by <span><span class="math inline">\(1/k^2\)</span></span>.</p>
<h3 id="the-chernoff-bound" data-number="17.3.2">The Chernoff bound</h3>
<p>Chebyshev’s inequality already shows a connection between independence and concentration, but in many cases we can hope for a quantitatively much stronger result. If, as in the example above, <span><span class="math inline">\(X= X_1+\ldots+X_n\)</span></span> where the <span><span class="math inline">\(X_i\)</span></span>’s are bounded i.i.d random variables of mean <span><span class="math inline">\(1/2\)</span></span>, then as <span><span class="math inline">\(n\)</span></span> grows, the distribution of <span><span class="math inline">\(X\)</span></span> would be roughly the <em>normal</em> or <em>Gaussian</em> distribution<span><span class="math inline">\(-\)</span></span> that is, distributed according to the <em>bell curve</em> (see <a href='#bellfig'>Figure 17.6</a> and <a href='#empiricalbellfig'>Figure 17.8</a>). This distribution has the property of being <em>very</em> concentrated in the sense that the probability of deviating <span><span class="math inline">\(k\)</span></span> standard deviations from the mean is not merely <span><span class="math inline">\(1/k^2\)</span></span> as is guaranteed by Chebyshev, but rather is roughly <span><span class="math inline">\(e^{-k^2}\)</span></span>. Specifically, for a normal random variable <span><span class="math inline">\(X\)</span></span> of expectation <span><span class="math inline">\(\mu\)</span></span> and standard deviation <span><span class="math inline">\(\sigma\)</span></span>, the probability that <span><span class="math inline">\(|X-\mu| \geq k\sigma\)</span></span> is at most <span><span class="math inline">\(2e^{-k^2/2}\)</span></span>. That is, we have an <em>exponential decay</em> of the probability of deviation.</p>
<figure>
<img src="../figure/sixsigma.jpg" alt="17.8: In the normal distribution or the Bell curve, the probability of deviating k standard deviations from the expectation shrinks exponentially in k^2, and specifically with probability at least 1-2e^{-k^2/2}, a random variable X of expectation \mu and standard deviation \sigma satisfies \mu -k\sigma \leq X \leq \mu+k\sigma. This figure gives more precise bounds for k=1,2,3,4,5,6. (Image credit:Imran Baghirov)" id="empiricalbellfig" class="margin" /><figcaption>17.8: In the <em>normal distribution</em> or the Bell curve, the probability of deviating <span><span class="math inline">\(k\)</span></span> standard deviations from the expectation shrinks <em>exponentially</em> in <span><span class="math inline">\(k^2\)</span></span>, and specifically with probability at least <span><span class="math inline">\(1-2e^{-k^2/2}\)</span></span>, a random variable <span><span class="math inline">\(X\)</span></span> of expectation <span><span class="math inline">\(\mu\)</span></span> and standard deviation <span><span class="math inline">\(\sigma\)</span></span> satisfies <span><span class="math inline">\(\mu -k\sigma \leq X \leq \mu+k\sigma\)</span></span>. This figure gives more precise bounds for <span><span class="math inline">\(k=1,2,3,4,5,6\)</span></span>. (Image credit:Imran Baghirov)</figcaption>
</figure>
<p>The following extremely useful theorem shows that such exponential decay occurs every time we have a sum of independent and bounded variables. This theorem is known under many names in different communities, though it is mostly called the <a href="https://en.wikipedia.org/wiki/Chernoff_bound">Chernoff bound</a> in the computer science literature:</p>
<div id="chernoffthm" class="theorem" title="Chernoff/Hoeffding bound" name="Theorem 17.12 (Chernoff/Hoeffding bound) ">
<p>If <span><span class="math inline">\(X_1,\ldots,X_n\)</span></span> are i.i.d random variables such that <span><span class="math inline">\(X_i \in [0,1]\)</span></span> and <span><span class="math inline">\(\E[X_i]=p\)</span></span> for every <span><span class="math inline">\(i\)</span></span>, then for every <span><span class="math inline">\(\epsilon &gt;0\)</span></span> <span>
<div class='myequationbox'><span class="math display">\[
\Pr[ \left| \sum_{i=0}^{n-1} X_i - pn \right| &gt; \epsilon n ] \leq 2\cdot e^{-2\epsilon^2 n} .
\]</span></div></span></p>
</div>
<p>We omit the proof, which appears in many texts, and uses Markov’s inequality on i.i.d random variables <span><span class="math inline">\(Y_0,\ldots,Y_n\)</span></span> that are of the form <span><span class="math inline">\(Y_i = e^{\lambda X_i}\)</span></span> for some carefully chosen parameter <span><span class="math inline">\(\lambda\)</span></span>. See <a href='#chernoffstirlingex'>Exercise 17.10</a> for a proof of the simple (but highly useful and representative) case where each <span><span class="math inline">\(X_i\)</span></span> is <span><span class="math inline">\(\{0,1\}\)</span></span> valued and <span><span class="math inline">\(p=1/2\)</span></span>. (See also <a href='#poorchernoff'>Exercise 17.11</a> for a generalization.)</p>
<div class="recap" name="Recap 17.3.2">
<ul>
<li>A basic probabilistic experiment corresponds to tossing <span><span class="math inline">\(n\)</span></span> coins or choosing <span><span class="math inline">\(x\)</span></span> uniformly at random from <span><span class="math inline">\(\{0,1\}^n\)</span></span>.</li>
<li><em>Random variables</em> assign a real number to every result of a coin toss. The <em>expectation</em> of a random variable <span><span class="math inline">\(X\)</span></span> is its average value.</li>
<li>There are several <em>concentration</em> results, also known as <em>tail bounds</em> showing that under certain conditions, random variables deviate significantly from their expectation only with small probability.</li>
</ul>
</div>
<h2 id="exercises" data-number="17.4">Exercises</h2>
<div id="section-12" class="exercise" name="Exercise">
<p>Suppose that we toss three independent fair coins <span><span class="math inline">\(a,b,c \in \{0,1\}\)</span></span>. What is the probability that the XOR of <span><span class="math inline">\(a\)</span></span>,<span><span class="math inline">\(b\)</span></span>, and <span><span class="math inline">\(c\)</span></span> is equal to <span><span class="math inline">\(1\)</span></span>? What is the probability that the AND of these three values is equal to <span><span class="math inline">\(1\)</span></span>? Are these two events independent?</p>
</div>
<div id="section-13" class="exercise" name="Exercise">
<p>Give an example of random variables <span><span class="math inline">\(X,Y: \{0,1\}^3 \rightarrow \R\)</span></span> such that <span><span class="math inline">\(\E[\ensuremath{\mathit{XY}}] \neq \E[X]\E[Y]\)</span></span>.</p>
</div>
<div id="noindnocorex" class="exercise" name="Exercise 17.3">
<p>Give an example of random variables <span><span class="math inline">\(X,Y: \{0,1\}^3 \rightarrow \R\)</span></span> such that <span><span class="math inline">\(X\)</span></span> and <span><span class="math inline">\(Y\)</span></span> are <em>not</em> independent but <span><span class="math inline">\(\E[\ensuremath{\mathit{XY}}] =\E[X]\E[Y]\)</span></span>.</p>
</div>
<div id="majorityex" class="exercise" name="Exercise 17.4">
<p>Let <span><span class="math inline">\(n\)</span></span> be an odd number, and let <span><span class="math inline">\(X:\{0,1\}^n \rightarrow \R\)</span></span> be the random variable defined as follows: for every <span><span class="math inline">\(x\in \{0,1\}^n\)</span></span>, <span><span class="math inline">\(X(x)=1\)</span></span> if <span><span class="math inline">\(\sum_{i=0}x_i &gt; n/2\)</span></span> and <span><span class="math inline">\(X(x)=0\)</span></span> otherwise. Prove that <span><span class="math inline">\(\E[X] = 1/2\)</span></span>.</p>
</div>
<div id="expprodex" class="exercise" title="Product of expectations" name="Exercise 17.5 (Product of expectations) ">
<p>Prove <a href='#expprod'>Lemma 17.7</a></p>
</div>
<div id="indeplemex" class="exercise" title="Transformations preserve independence" name="Exercise 17.6 (Transformations preserve independence) ">
<p>Prove <a href='#indeplem'>Lemma 17.8</a></p>
</div>
<div id="varianceex" class="exercise" title="Variance of independent random variables" name="Exercise 17.7 (Variance of independent random variables) ">
<p>Prove that if <span><span class="math inline">\(X_0,\ldots,X_{n-1}\)</span></span> are independent random variables then <span><span class="math inline">\(\mathrm{Var}[X_0+\cdots+X_{n-1}]=\sum_{i=0}^{n-1} \mathrm{Var}[X_i]\)</span></span>.</p>
</div>
<div id="entropyex" class="exercise" title="Entropy (challenge)" name="Exercise 17.8 (Entropy (challenge)) ">
<p>Recall the definition of a distribution <span><span class="math inline">\(\mu\)</span></span> over some finite set <span><span class="math inline">\(S\)</span></span>. Shannon defined the <em>entropy</em> of a distribution <span><span class="math inline">\(\mu\)</span></span>, denoted by <span><span class="math inline">\(H(\mu)\)</span></span>, to be <span><span class="math inline">\(\sum_{x\in S} \mu(x)\log(1/\mu(x))\)</span></span>. The idea is that if <span><span class="math inline">\(\mu\)</span></span> is a distribution of entropy <span><span class="math inline">\(k\)</span></span>, then encoding members of <span><span class="math inline">\(\mu\)</span></span> will require <span><span class="math inline">\(k\)</span></span> bits, in an amortized sense. In this exercise we justify this definition. Let <span><span class="math inline">\(\mu\)</span></span> be such that <span><span class="math inline">\(H(\mu)=k\)</span></span>.<br />
1. Prove that for every one to one function <span><span class="math inline">\(F:S \rightarrow \{0,1\}^*\)</span></span>, <span><span class="math inline">\(\E_{x \sim \mu} |F(x)| \geq k\)</span></span>.<br />
2. Prove that for every <span><span class="math inline">\(\epsilon\)</span></span>, there is some <span><span class="math inline">\(n\)</span></span> and a one-to-one function <span><span class="math inline">\(F:S^n \rightarrow \{0,1\}^*\)</span></span>, such that <span><span class="math inline">\(\E_{x\sim \mu^n} |F(x)| \leq n(k+\epsilon)\)</span></span>, where <span><span class="math inline">\(x \sim \mu\)</span></span> denotes the experiments of choosing <span><span class="math inline">\(x_0,\ldots,x_{n-1}\)</span></span> each independently from <span><span class="math inline">\(S\)</span></span> using the distribution <span><span class="math inline">\(\mu\)</span></span>.</p>
</div>
<div id="entropybinomex" class="exercise" title="Entropy approximation to binomial" name="Exercise 17.9 (Entropy approximation to binomial) ">
<p>Let <span><span class="math inline">\(H(p) = p \log(1/p)+(1-p)\log(1/(1-p))\)</span></span>.<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup> Prove that for every <span><span class="math inline">\(p \in (0,1)\)</span></span> and <span><span class="math inline">\(\epsilon&gt;0\)</span></span>, if <span><span class="math inline">\(n\)</span></span> is large enough then<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup> <span>
<div class='myequationbox'><span class="math display">\[
2^{(H(p)-\epsilon)n }\binom{n}{pn} \leq 2^{(H(p)+\epsilon)n}
\]</span></div></span> where <span><span class="math inline">\(\binom{n}{k}\)</span></span> is the binomial coefficient <span><span class="math inline">\(\tfrac{n!}{k!(n-k)!}\)</span></span> which is equal to the number of <span><span class="math inline">\(k\)</span></span>-size subsets of <span><span class="math inline">\(\{0,\ldots,n-1\}\)</span></span>.</p>
</div>
<div id="chernoffstirlingex" class="exercise" title="Chernoff using Stirling" name="Exercise 17.10 (Chernoff using Stirling) ">
<ol type="1">
<li>Prove that <span><span class="math inline">\(\Pr_{x\sim \{0,1\}^n}[ \sum x_i = k ] = \binom{n}{k}2^{-n}\)</span></span>.<br />
</li>
<li>Use this and <a href='#entropybinomex'>Exercise 17.9</a> to prove the Chernoff bound for the case that <span><span class="math inline">\(X_0,\ldots,X_n\)</span></span> are i.i.d. random variables over <span><span class="math inline">\(\{0,1\}\)</span></span> each equaling <span><span class="math inline">\(0\)</span></span> and <span><span class="math inline">\(1\)</span></span> with probability <span><span class="math inline">\(1/2\)</span></span>.</li>
</ol>
</div>
<div id="poorchernoff" class="exercise" title="Poor man&#39;s Chernoff" name="Exercise 17.11 (Poor man&#39;s Chernoff) ">
<p>Let <span><span class="math inline">\(X_0,\ldots,X_n\)</span></span> be i.i.d random variables with <span><span class="math inline">\(\E X_i = p\)</span></span> and <span><span class="math inline">\(\Pr [ 0 \leq X_i \leq 1 ]=1\)</span></span>. Define <span><span class="math inline">\(Y_i = X_i - p\)</span></span>.<br />
1. Prove that for every <span><span class="math inline">\(j_1,\ldots,j_n \in \N\)</span></span>, if there exists one <span><span class="math inline">\(i\)</span></span> such that <span><span class="math inline">\(j_i\)</span></span> is odd then <span><span class="math inline">\(\E [\prod_{i=0}^{n-1} Y_i^{j_i}] = 0\)</span></span>.<br />
2. Prove that for every <span><span class="math inline">\(k\)</span></span>, <span><span class="math inline">\(\E[ (\sum_{i=0}^{n-1} Y_i)^k ] \leq (10kn)^{k/2}\)</span></span>.<sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup><br />
3. Prove that for every <span><span class="math inline">\(\epsilon&gt;0\)</span></span>, <span><span class="math inline">\(\Pr[ |\sum_i Y_i| \geq \epsilon n ] \geq 2^{-\epsilon^2 n / (10000\log 1/\epsilon)}\)</span></span>.<sup id="fnref:4"><a href="#fn:4" rel="footnote">4</a></sup></p>
</div>
<div id="coindistex" class="exercise" title="Simulating distributions using coins" name="Exercise 18.3 (Simulating distributions using coins) ">
<p>Our model for probability involves tossing <span><span class="math inline">\(n\)</span></span> coins, but sometimes algorithms require sampling from other distributions, such as selecting a uniform number in <span><span class="math inline">\(\{0,\ldots,M-1\}\)</span></span> for some <span><span class="math inline">\(M\)</span></span>. Fortunately, we can simulate this with an exponentially small probability of error: prove that for every <span><span class="math inline">\(M\)</span></span>, if <span><span class="math inline">\(n&gt;k\lceil \log M \rceil\)</span></span>, then there is a function <span><span class="math inline">\(F:\{0,1\}^n \rightarrow \{0,\ldots,M-1\} \cup \{ \bot \}\)</span></span> such that <strong>(1)</strong> The probability that <span><span class="math inline">\(F(x)=\bot\)</span></span> is at most <span><span class="math inline">\(2^{-k}\)</span></span> and <strong>(2)</strong> the distribution of <span><span class="math inline">\(F(x)\)</span></span> conditioned on <span><span class="math inline">\(F(x) \neq \bot\)</span></span> is equal to the uniform distribution over <span><span class="math inline">\(\{0,\ldots,M-1\}\)</span></span>.<sup id="fnref:5"><a href="#fn:5" rel="footnote">5</a></sup></p>
</div>
<div id="samplingex" class="exercise" title="Sampling" name="Exercise 17.13 (Sampling) ">
<p>Suppose that a country has 300,000,000 citizens, 52 percent of which prefer the color “green” and 48 percent of which prefer the color “orange”. Suppose we sample <span><span class="math inline">\(n\)</span></span> random citizens and ask them their favorite color (assume they will answer truthfully). What is the smallest value <span><span class="math inline">\(n\)</span></span> among the following choices so that the probability that the majority of the sample answers “green” is at most <span><span class="math inline">\(0.05\)</span></span>?</p>
<ol type="a">
<li><p>1,000</p></li>
<li><p>10,000</p></li>
<li><p>100,000</p></li>
<li><p>1,000,000</p></li>
</ol>
</div>
<div id="exid" class="exercise" name="Exercise 17.14">
<p>Would the answer to <a href='#samplingex'>Exercise 17.13</a> change if the country had 300,000,000,000 citizens?</p>
</div>
<div id="exidtwo" class="exercise" title="Sampling (2)" name="Exercise 17.15 (Sampling (2)) ">
<p>Under the same assumptions as <a href='#samplingex'>Exercise 17.13</a>, what is the smallest value <span><span class="math inline">\(n\)</span></span> among the following choices so that the probability that the majority of the sample answers “green” is at most <span><span class="math inline">\(2^{-100}\)</span></span>?</p>
<ol type="a">
<li><p>1,000</p></li>
<li><p>10,000</p></li>
<li><p>100,000</p></li>
<li><p>1,000,000</p></li>
<li><p>It is impossible to get such low probability since there are fewer than <span><span class="math inline">\(2^{100}\)</span></span> citizens.</p></li>
</ol>
</div>
<h2 id="bibliographical-notes" data-number="17.5">Bibliographical notes</h2>
<p>There are many sources for more information on discrete probability, including the texts referenced in <a href='lec_00_1_math_background.html#notesmathchap'>Section 1.9</a>. One particularly recommended source for probability is [Harvard’s <a href="https://projects.iq.harvard.edu/stat110/home">STAT 110</a> class, whose lectures are available on <a href="https://projects.iq.harvard.edu/stat110/youtube">youtube</a> and whose book is available <a href="http://probabilitybook.net">online</a>.</p>
<p>The version of the Chernoff bound that we stated in <a href='#chernoffthm'>Theorem 17.12</a> is sometimes known as <a href="https://en.wikipedia.org/wiki/Hoeffding%27s_inequality">Hoeffding’s Inequality</a>. Other variants of the Chernoff bound are known as well, but all of them are equally good for the applications of this book.</p>
<div id="footnotediv" class="footnotes">
<ol>
<li class="footnote" id="fn:1"><p>
<div>
<p>While you don’t need this to solve this exercise, this is the function that maps <span><span class="math inline">\(p\)</span></span> to the entropy (as defined in <a href='#entropyex'>Exercise 17.8</a>) of the <span><span class="math inline">\(p\)</span></span>-biased coin distribution over <span><span class="math inline">\(\{0,1\}\)</span></span>, which is the function <span><span class="math inline">\(\mu:\{0,1\}\rightarrow [0,1]\)</span></span> s.y. <span><span class="math inline">\(\mu(0)=1-p\)</span></span> and <span><span class="math inline">\(\mu(1)=p\)</span></span>.</p>
</div>
<a href="#fnref:1" title="return to article"> ↩</a><p></li>
<li class="footnote" id="fn:2"><p>
<div>
<p><strong>Hint:</strong> Use Stirling’s formula for approximating the factorial function.</p>
</div>
<a href="#fnref:2" title="return to article"> ↩</a><p></li>
<li class="footnote" id="fn:3"><p>
<div>
<p><strong>Hint:</strong> Bound the number of tuples <span><span class="math inline">\(j_0,\ldots,j_{n-1}\)</span></span> such that every <span><span class="math inline">\(j_i\)</span></span> is even and <span><span class="math inline">\(\sum j_i = k\)</span></span>.</p>
</div>
<a href="#fnref:3" title="return to article"> ↩</a><p></li>
<li class="footnote" id="fn:4"><p>
<div>
<p><strong>Hint:</strong> Set <span><span class="math inline">\(k=2\lceil \epsilon^2 n /1000 \rceil\)</span></span> and then show that if the event <span><span class="math inline">\(|\sum Y_i | \geq \epsilon n\)</span></span> happens then the random variable <span><span class="math inline">\((\sum Y_i)^k\)</span></span> is a factor of <span><span class="math inline">\(\epsilon^{-k}\)</span></span> larger than its expectation.</p>
</div>
<a href="#fnref:4" title="return to article"> ↩</a><p></li>
<li class="footnote" id="fn:5"><p>
<div>
<p><strong>Hint:</strong> Think of <span><span class="math inline">\(x\in \{0,1\}^n\)</span></span> as choosing <span><span class="math inline">\(k\)</span></span> numbers <span><span class="math inline">\(y_1,\ldots,y_k \in \{0,\ldots, 2^{\lceil \log M \rceil}-1 \}\)</span></span>. Output the first such number that is in <span><span class="math inline">\(\{0,\ldots,M-1\}\)</span></span>. </p>
</div>
<a href="#fnref:5" title="return to article"> ↩</a><p></li>
</ol>
</div>
<!--bookdown:body:end-->


<!-- end of  actual content -->

<!-- start of comments -->


<a name="commentform"></a>
<h2 id="comments" class="nocount">Comments</h2>

<p>Comments are posted on the <a href="https://github.com/boazbk/tcs/issues">GitHub repository</a> using the <a href="https://utteranc.es">utteranc.es</a> app.
A GitHub login is required to comment.
If you don't want to authorize the app to post on your behalf, you can also comment directly on the <a href="https://github.com/boazbk/tcs/issues?q=Defining Computation+in%3Atitle">GitHub issue for this page</a>.


<p>


<script src="https://utteranc.es/client.js" 
repo="boazbk/tcs" 
issue-term="title" 
label="comments"
theme="github-light" 
crossorigin="anonymous" async>
  </script>


<!-- end of comments -->

<p>Compiled on 12/02/2019 21:39:35</p>

<p>Copyright 2019, Boaz Barak.


<a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/"><img alt="Creative Commons License"
    style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png" /></a><br />This work is
licensed under a <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/">Creative Commons
  Attribution-NonCommercial-NoDerivatives 4.0 International License</a>.

<p>Produced using <a href="https://pandoc.org/">pandoc</a> and <a href="http://scorreia.com/software/panflute/">panflute</a> with templates derived from <a href="https://www.gitbook.com/">gitbook</a> and <a href="https://bookdown.org/">bookdown</a>.</p>



</div>


            </section>

          </div>
        </div>
      </div>
<!--bookdown:link_prev-->
<!--bookdown:link_next-->



    </div>
  </div>
<!--bookdown:config-->
<script src="js/app.min.js"></script>
<script src="js/lunr.js"></script>
<script src="js/plugin-search.js"></script>
<script src="js/plugin-sharing.js"></script>
<script src="js/plugin-fontsettings.js"></script>
<script src="js/fullscreen.js"></script>
<script src="js/plugin-bookdown.js"></script>
<script src="js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"history": {
"link": null,
"text": null
},
"download": ["https://files.boazbarak.org/introtcs/lec_15_probability.pdf"],
"toc": {
"collapse": "section"
}
});
});
</script>


</body>

</html>
